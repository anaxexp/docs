{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started AnaxExp was designed to help developers deploy and manage application across their servers in the most simple fashion. Before you deploy your first application make sure you've connected at least one server and optionally a git repository. Or you can use our demo server and git server (all applications deployed to our demo server will be automatically destroyed after 12 hours). Key concepts: Application Instances Infrastructure: single-server and cluster Stacks How to: Deploy your first app Connect your server Deploy code and CI/CD setup Open-source projects for local development: Docker4Drupal WordPress4Docker Docker4PHP","title":"Home"},{"location":"#getting-started","text":"AnaxExp was designed to help developers deploy and manage application across their servers in the most simple fashion. Before you deploy your first application make sure you've connected at least one server and optionally a git repository. Or you can use our demo server and git server (all applications deployed to our demo server will be automatically destroyed after 12 hours). Key concepts: Application Instances Infrastructure: single-server and cluster Stacks How to: Deploy your first app Connect your server Deploy code and CI/CD setup Open-source projects for local development: Docker4Drupal WordPress4Docker Docker4PHP","title":"Getting started"},{"location":"billing/","text":"Billing and Payment Overview AnaxExp provides subscriptions on per application instance basis. You can find available pricing plans on our pricing plans page . The price specified for 1 month (30 days). Free Trial We offer 1 week free trials for paid subscriptions. To start a free trial you should add a bank card. During activation of your trial you'll have to choose a pricing plan with a billing period (monthly or annual). Once the trial expired your card will be automatically charged for the first month with the amount of the selected pricing plan. If you cancel your subscription before the end of the trial your card won't be charged. Bank Cards We don't store your bank cards information. We use PCI compliant third-party service Stripe to store cards information and process all the payments. You can add only one card. If you want to change the attached card, just edit its information. If you want to delete attached card permanently please contact our support team . Billing At the beginning of every billing month we charge you based on the number of instances (minimum 10) in your organization regardless of their status. If the number of instances changed during the month it will be accounted at the next billing period \u2013 we will either charge or refund the appropriate amount based on the instance-hours. AnaxExp uses recurring payment for all the subscriptions. The subscriptions automatically renewed every billing period (month or year), the card attached to the organization charged automatically. After the card has been charged the invoice will be sent to the email address of the owner user who edited the card details last. If you cancel your subscription before the end of the current billing period your card won't be charged. Invoices We send an invoice after every charge to an email address specified in the billing email field ( Billing Settings ) or to an email of the user who added the bank card. Additionally, you can find all invoices and download them in PDF from Billing Invoices tab. Billing address and VAT You can specify all required billing details from Billings Settings page. The details will appear on your invoices. Permissions All owners of the organization can manage any cards and subscriptions inside of the organization. Discount We offer discounted price per instance if paid 1-year upfront starting with a certain number of instances","title":"Billing"},{"location":"billing/#billing-and-payment","text":"","title":"Billing and Payment"},{"location":"billing/#overview","text":"AnaxExp provides subscriptions on per application instance basis. You can find available pricing plans on our pricing plans page . The price specified for 1 month (30 days).","title":"Overview"},{"location":"billing/#free-trial","text":"We offer 1 week free trials for paid subscriptions. To start a free trial you should add a bank card. During activation of your trial you'll have to choose a pricing plan with a billing period (monthly or annual). Once the trial expired your card will be automatically charged for the first month with the amount of the selected pricing plan. If you cancel your subscription before the end of the trial your card won't be charged.","title":"Free Trial"},{"location":"billing/#bank-cards","text":"We don't store your bank cards information. We use PCI compliant third-party service Stripe to store cards information and process all the payments. You can add only one card. If you want to change the attached card, just edit its information. If you want to delete attached card permanently please contact our support team .","title":"Bank Cards"},{"location":"billing/#billing","text":"At the beginning of every billing month we charge you based on the number of instances (minimum 10) in your organization regardless of their status. If the number of instances changed during the month it will be accounted at the next billing period \u2013 we will either charge or refund the appropriate amount based on the instance-hours. AnaxExp uses recurring payment for all the subscriptions. The subscriptions automatically renewed every billing period (month or year), the card attached to the organization charged automatically. After the card has been charged the invoice will be sent to the email address of the owner user who edited the card details last. If you cancel your subscription before the end of the current billing period your card won't be charged.","title":"Billing"},{"location":"billing/#invoices","text":"We send an invoice after every charge to an email address specified in the billing email field ( Billing Settings ) or to an email of the user who added the bank card. Additionally, you can find all invoices and download them in PDF from Billing Invoices tab.","title":"Invoices"},{"location":"billing/#billing-address-and-vat","text":"You can specify all required billing details from Billings Settings page. The details will appear on your invoices.","title":"Billing address and VAT"},{"location":"billing/#permissions","text":"All owners of the organization can manage any cards and subscriptions inside of the organization.","title":"Permissions"},{"location":"billing/#discount","text":"We offer discounted price per instance if paid 1-year upfront starting with a certain number of instances","title":"Discount"},{"location":"dev/","text":"Developers knowledge base API keys We automatically generate an API key when you register, it can be found under Account API Keys AnaxExp API We provide API to perform most common tasks such as deployment of a new applications and instances, code deployments, etc. To use API you would need an access token you can find on your profile page in the dashboard. Version 3 (beta) API reference SDK: * PHP * Go Version 2 (deprecated) API reference Coming soon AnaxExp CLI GitHub project: anaxexp/anaxexp-cli Docker image: anaxexp/anaxexp-cli","title":"Development"},{"location":"dev/#developers-knowledge-base","text":"","title":"Developers knowledge base"},{"location":"dev/#api-keys","text":"We automatically generate an API key when you register, it can be found under Account API Keys","title":"API keys"},{"location":"dev/#anaxexp-api","text":"We provide API to perform most common tasks such as deployment of a new applications and instances, code deployments, etc. To use API you would need an access token you can find on your profile page in the dashboard.","title":"AnaxExp API"},{"location":"dev/#version-3-beta","text":"API reference SDK: * PHP * Go","title":"Version 3 (beta)"},{"location":"dev/#version-2-deprecated","text":"API reference Coming soon","title":"Version 2 (deprecated)"},{"location":"dev/#anaxexp-cli","text":"GitHub project: anaxexp/anaxexp-cli Docker image: anaxexp/anaxexp-cli","title":"AnaxExp CLI"},{"location":"faq/","text":"FAQ How to access applications data from host? Please see this article How to upgrade my application? If you're running a vanilla application via AnaxExp's managed stack and this application provides an option to upgrade it via UI (like WordPress or Matomo) this option will not work because it requires a full writing permission on the entire codebase which we avoid for security reasons. The way you upgrade to a new version is by upgrading your application stack that contains this update (see stack changelog). If the latest version of stack does not yet have the latest version please contact us and let us know what update is missing. We usually take a more conservative position regarding updates (not counting security updates) for stability sake. How AnaxExp's availability affects my applications? AnaxExp's availability does not directly affect availability of your server(s) or applications. Although if AnaxExp becomes unavailable some tasks such as auto deployment won't be processed and auto backups will be postponed. You can always track our current status at http://status.anaxexp.com/ Does AnaxExp move applications data outside my region? Cases when your data can be moved outside of your region: When you use AnaxExp Storage for backups mirroring. The location of this storage is United States. You should use your storage with the same location instead (e.g. AWS S3 bucket) When you import archives from our dashboard we temporary store this data in our storage for 24 hours for further transfer. This storage never listed publicly, all URLs have huge random hash values and never published outside the dashboard What happens if my server IP changes? For single server infrastructure we automatically detect new external IP. Upon the detection we update the connected server IP on AnaxExp and DNS records of technical domains (DNS propagation may take a few minutes). So normally, you don't need to do anything additionally. Infrastructure version 5.5.4 does not have this function, please contact us and we'll update your IP manually or upgrade your infrastructure How do I put application to maintenance mode? You can put your app instance to maintenance from Instance Stack Settings How can I access my DigitalOcean server by SSH See this article Can I access my application codebase by SSH Yes, if your stack has sshd container, if not you can still access any container via shell, see this article for details Do you support drush aliases for Drupal apps Yes, see this article to learn how to use them How can I upgrade my WordPress applications See this article How can I build my frontend application with Node (npm)? You should use CI/CD deployment How can I purchase a server? AnaxExp is not a hosting provider. We believe that there are plenty of reliable providers on the market already. Instead we provide a way to connect your own server from any cloud. Why does team plan starts from $50? The team plan starts with 10 app instances minimum. And here is why \u2013 when people use AnaxExp in production they expect a certain level of service which we can't provide for less than $50/mo. On the other hand we don't want to provide poor service. Quality over quantity. I have a lot of applications and looking for ways to save costs If you have many low-traffic applications we recommend consider the following: Do not deploy too many apps per one server because the overhead growth relatively to # of containers is not linear. ~200-300 containers is a recommended maximum Use dedicated (or bare-metal) server instead of VPS. It will be cheaper (but less reliable) if you know how many resources you need. You will get a guaranteed stable CPU performance unlike on VPS (cloud providers usually oversell) How can I delete my account? You can delete your account from Account delete page What's the origin of AnaxExp name? AnaxExp pronounced as w\u0254\u02d0dbi. AnaxExp is a name of a water spirit in a slavic mythology ( https://en.wikipedia.org/wiki/Vodyanoy ). Vodyanoy or AnaxExp in Upper Sorbian language. What's the location of AnaxExp team? AnaxExp is a US corporation with a distributed team. Currently we provide support mostly in EU hours (CET).","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-to-access-applications-data-from-host","text":"Please see this article","title":"How to access applications data from host?"},{"location":"faq/#how-to-upgrade-my-application","text":"If you're running a vanilla application via AnaxExp's managed stack and this application provides an option to upgrade it via UI (like WordPress or Matomo) this option will not work because it requires a full writing permission on the entire codebase which we avoid for security reasons. The way you upgrade to a new version is by upgrading your application stack that contains this update (see stack changelog). If the latest version of stack does not yet have the latest version please contact us and let us know what update is missing. We usually take a more conservative position regarding updates (not counting security updates) for stability sake.","title":"How to upgrade my application?"},{"location":"faq/#how-anaxexps-availability-affects-my-applications","text":"AnaxExp's availability does not directly affect availability of your server(s) or applications. Although if AnaxExp becomes unavailable some tasks such as auto deployment won't be processed and auto backups will be postponed. You can always track our current status at http://status.anaxexp.com/","title":"How AnaxExp's availability affects my applications?"},{"location":"faq/#does-anaxexp-move-applications-data-outside-my-region","text":"Cases when your data can be moved outside of your region: When you use AnaxExp Storage for backups mirroring. The location of this storage is United States. You should use your storage with the same location instead (e.g. AWS S3 bucket) When you import archives from our dashboard we temporary store this data in our storage for 24 hours for further transfer. This storage never listed publicly, all URLs have huge random hash values and never published outside the dashboard","title":"Does AnaxExp move applications data outside my region?"},{"location":"faq/#what-happens-if-my-server-ip-changes","text":"For single server infrastructure we automatically detect new external IP. Upon the detection we update the connected server IP on AnaxExp and DNS records of technical domains (DNS propagation may take a few minutes). So normally, you don't need to do anything additionally. Infrastructure version 5.5.4 does not have this function, please contact us and we'll update your IP manually or upgrade your infrastructure","title":"What happens if my server IP changes?"},{"location":"faq/#how-do-i-put-application-to-maintenance-mode","text":"You can put your app instance to maintenance from Instance Stack Settings","title":"How do I put application to maintenance mode?"},{"location":"faq/#how-can-i-access-my-digitalocean-server-by-ssh","text":"See this article","title":"How can I access my DigitalOcean server by SSH"},{"location":"faq/#can-i-access-my-application-codebase-by-ssh","text":"Yes, if your stack has sshd container, if not you can still access any container via shell, see this article for details","title":"Can I access my application codebase by SSH"},{"location":"faq/#do-you-support-drush-aliases-for-drupal-apps","text":"Yes, see this article to learn how to use them","title":"Do you support drush aliases for Drupal apps"},{"location":"faq/#how-can-i-upgrade-my-wordpress-applications","text":"See this article","title":"How can I upgrade my WordPress applications"},{"location":"faq/#how-can-i-build-my-frontend-application-with-node-npm","text":"You should use CI/CD deployment","title":"How can I build my frontend application with Node (npm)?"},{"location":"faq/#how-can-i-purchase-a-server","text":"AnaxExp is not a hosting provider. We believe that there are plenty of reliable providers on the market already. Instead we provide a way to connect your own server from any cloud.","title":"How can I purchase a server?"},{"location":"faq/#why-does-team-plan-starts-from-50","text":"The team plan starts with 10 app instances minimum. And here is why \u2013 when people use AnaxExp in production they expect a certain level of service which we can't provide for less than $50/mo. On the other hand we don't want to provide poor service. Quality over quantity.","title":"Why does team plan starts from $50?"},{"location":"faq/#i-have-a-lot-of-applications-and-looking-for-ways-to-save-costs","text":"If you have many low-traffic applications we recommend consider the following: Do not deploy too many apps per one server because the overhead growth relatively to # of containers is not linear. ~200-300 containers is a recommended maximum Use dedicated (or bare-metal) server instead of VPS. It will be cheaper (but less reliable) if you know how many resources you need. You will get a guaranteed stable CPU performance unlike on VPS (cloud providers usually oversell)","title":"I have a lot of applications and looking for ways to save costs"},{"location":"faq/#how-can-i-delete-my-account","text":"You can delete your account from Account delete page","title":"How can I delete my account?"},{"location":"faq/#whats-the-origin-of-anaxexp-name","text":"AnaxExp pronounced as w\u0254\u02d0dbi. AnaxExp is a name of a water spirit in a slavic mythology ( https://en.wikipedia.org/wiki/Vodyanoy ). Vodyanoy or AnaxExp in Upper Sorbian language.","title":"What's the origin of AnaxExp name?"},{"location":"faq/#whats-the-location-of-anaxexp-team","text":"AnaxExp is a US corporation with a distributed team. Currently we provide support mostly in EU hours (CET).","title":"What's the location of AnaxExp team?"},{"location":"roles/","text":"Permissions AnaxExp provides a set of default roles with following permissions: Action Owner Administrator Team leader Developer Unprivileged View apps \u2713 \u2713 \u2713 \u2713 \u2713 View servers \u2713 \u2713 \u2713 \u2713 Manage dev instances \u2713 \u2713 \u2713 \u2713 View actions history \u2713 \u2713 \u2713 View team members \u2713 \u2713 \u2713 Add new servers \u2713 \u2713 \u2713 Manage servers \u2713 \u2713 \u2713 Manage apps' settings \u2713 \u2713 \u2713 Manage stage instances \u2713 \u2713 \u2713 Manage prod instances \u2713 \u2713 \u2713 Create new apps \u2713 \u2713 Delete servers \u2713 \u2713 Delete apps \u2713 \u2713 Manage team \u2713 \u2713 Manage organization \u2713 \u2713 Delete stage instance \u2713 \u2713 Delete prod instance \u2713 You can assign roles both inside of an organization and apps. If you assign a more privileged role to a user inside of the organization he loses his less-privileged roles in apps.","title":"User roles"},{"location":"roles/#permissions","text":"AnaxExp provides a set of default roles with following permissions: Action Owner Administrator Team leader Developer Unprivileged View apps \u2713 \u2713 \u2713 \u2713 \u2713 View servers \u2713 \u2713 \u2713 \u2713 Manage dev instances \u2713 \u2713 \u2713 \u2713 View actions history \u2713 \u2713 \u2713 View team members \u2713 \u2713 \u2713 Add new servers \u2713 \u2713 \u2713 Manage servers \u2713 \u2713 \u2713 Manage apps' settings \u2713 \u2713 \u2713 Manage stage instances \u2713 \u2713 \u2713 Manage prod instances \u2713 \u2713 \u2713 Create new apps \u2713 \u2713 Delete servers \u2713 \u2713 Delete apps \u2713 \u2713 Manage team \u2713 \u2713 Manage organization \u2713 \u2713 Delete stage instance \u2713 \u2713 Delete prod instance \u2713 You can assign roles both inside of an organization and apps. If you assign a more privileged role to a user inside of the organization he loses his less-privileged roles in apps.","title":"Permissions"},{"location":"status/","text":"AnaxExp Platform Status You can always see the actual status of our platform on our status page . The possible problems with our platform do not affect the work of your server or applications.","title":"Status"},{"location":"status/#anaxexp-platform-status","text":"You can always see the actual status of our platform on our status page . The possible problems with our platform do not affect the work of your server or applications.","title":"AnaxExp Platform Status"},{"location":"support/","text":"Getting Support Support response time We provide best effort support (with exception of premium support customers), this means we do not guarantee that we will respond shortly to your request despite how urgent it is. We usually respond in EU business hours and try to give initial responses within one business day. Cannot see live chat widget? You may not see live chat support widget if you have tracking protection enabled in your browser You can contact us in one of these ways: Contact us using live chat from Dashboard (widget in the right bottom corner) Send us an email to support [at] anaxexp.com Use our contact form Get community support on Slack Submit an issue on GitHub in a relevant repository Have a question about local environment? We do not provide support for local environment via live chat (unless you have a premium support package) but you can ask the community on Slack or submit an issue on GitHub","title":"Support"},{"location":"support/#getting-support","text":"Support response time We provide best effort support (with exception of premium support customers), this means we do not guarantee that we will respond shortly to your request despite how urgent it is. We usually respond in EU business hours and try to give initial responses within one business day. Cannot see live chat widget? You may not see live chat support widget if you have tracking protection enabled in your browser You can contact us in one of these ways: Contact us using live chat from Dashboard (widget in the right bottom corner) Send us an email to support [at] anaxexp.com Use our contact form Get community support on Slack Submit an issue on GitHub in a relevant repository Have a question about local environment? We do not provide support for local environment via live chat (unless you have a premium support package) but you can ask the community on Slack or submit an issue on GitHub","title":"Getting Support"},{"location":"troubleshooting/","text":"Troubleshooting Cannot connect by SSH There are few known reasons for that: You must have at least one SSH key added to your profile Your ssh key not added to your SSH agent, try executing ssh-add /path/to/private/key Try specify which key to use ssh user@hostname -i /path/to/private/key ` Emails delivery from my application fails If you're using a server from a public cloud there's 90% chance that its IP is already compromised and blacklisted by major mail services, hence your emails won't be delivered or will land in the spam folder. If your stack has mail transfer agent OpenSMTPD we recommend integrating it with a 3 rd party email service (relay mode): AWS Simple Email Service SendGrid Any other SMTP server, see OpenSMTPD stack documentation Host identification has changed If you see the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the RSA key sent by the remote host is SHA256:XXXXXXXXXXXXX/XXXX. Please contact your system administrator. Add correct host key in /Users/xxx/.ssh/known_hosts to get rid of this message. Offending RSA key in /Users/xxx/.ssh/known_hosts:xx RSA host key for [node-xxxxx.wod.by]:xxxx has changed and you have requested strict checking. Host key verification failed. This means that the container you're trying connect to was recreated and RSA key has changed. To avoid this kind of errors you can disable strict host key checking for *.wod.by host by adding the following lines to ~/.ssh/config file: Host *.wod.by StrictHostKeyChecking no UserKnownHostsFile=/dev/null My server status is unreachable This problem could be caused by the lack of memory on your server. Make sure you have enough memory: $ free -m If you don't have enough memory, you can use Linux Swap. Make sure you're using swap by executing: $ sudo swapon -s If not, follow this guide to add swap (Ubuntu). If you still have the problem please contact AnaxExp support team. Cannot connect server There are few known reasons for that: Make sure your server satisfies requirements and recommendations AnaxExp's script must be run as a root A slow speed of Read/Write operations on a disk. See the list of recommended hosting providers Docker requirements are not met ufw enabled Inbound / outbound external firewall Non-compatible virtualization (virtuozza) Application deployment or other tasks fail This error means that one of the deployment steps exceeded its timeout. There are few known reasons for that: There's something wrong on our side, see http://status.anaxexp.com/ Something wrong with your server, make sure you have enough free disk space Check your CPU load average by running top Check you have enough free RAM by running free -h Check your system log for extra errors journalctl -f You've reached containers limit per server (300), contact our support to increase the limit A slow speed of Read/Write operations on a disk Huge ping to your server due to global network issues Application gives \"File not found\" error This error means that the HTTP server could not find the entrypoint (in case of PHP-based stacks it's usually index.php ) in a container. This might happen for a few reasons: You have your entrypoint (e.g. index.php ) in a subdirectory of your git root and you did not specify it during the initial deployment of new application Your codebase is missing, could be that you've selected a wrong branch during deployment/build Cannot update WordPress core or its plugins/themes See this article Infrastructure 5.x known issues \u2639\ufe0f Sometimes we can't get logs of a task with the error Container not found . Task may have been completed but we consider it as failed Sometimes we can't get the size of a backup archive so we don't show it in the dashboard If you update a rolling-update container and it fails we will not be able to detect the failure. Despite the actual failure the deployment will be considered successful because the older version of the container is still intact We can not handle errors of containers that failed to start, so the task will hang until it expires by timeout. Here's how you can manually check your deployment state in such cases: Access your server via SSH as root Run the following command (replace [INSTANCE UUID] ) kubectl get po -n [ INSTANCE UUID ] You will see statuses of pods (containers) of your application instance. You can get logs of the specific pod (container) either by running (if container is creating or running) kubectl logs [ POD NAME ] -n [ INSTANCE UUID ] or (if container is not currently running or in the error state) kubectl describe po [ POD NAME ] -n [ INSTANCE UUID ] Infrastructure 6.x All the known issues will be resolved in Infrastructure 6.x","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#cannot-connect-by-ssh","text":"There are few known reasons for that: You must have at least one SSH key added to your profile Your ssh key not added to your SSH agent, try executing ssh-add /path/to/private/key Try specify which key to use ssh user@hostname -i /path/to/private/key `","title":"Cannot connect by SSH"},{"location":"troubleshooting/#emails-delivery-from-my-application-fails","text":"If you're using a server from a public cloud there's 90% chance that its IP is already compromised and blacklisted by major mail services, hence your emails won't be delivered or will land in the spam folder. If your stack has mail transfer agent OpenSMTPD we recommend integrating it with a 3 rd party email service (relay mode): AWS Simple Email Service SendGrid Any other SMTP server, see OpenSMTPD stack documentation","title":"Emails delivery from my application fails"},{"location":"troubleshooting/#host-identification-has-changed","text":"If you see the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the RSA key sent by the remote host is SHA256:XXXXXXXXXXXXX/XXXX. Please contact your system administrator. Add correct host key in /Users/xxx/.ssh/known_hosts to get rid of this message. Offending RSA key in /Users/xxx/.ssh/known_hosts:xx RSA host key for [node-xxxxx.wod.by]:xxxx has changed and you have requested strict checking. Host key verification failed. This means that the container you're trying connect to was recreated and RSA key has changed. To avoid this kind of errors you can disable strict host key checking for *.wod.by host by adding the following lines to ~/.ssh/config file: Host *.wod.by StrictHostKeyChecking no UserKnownHostsFile=/dev/null","title":"Host identification has changed"},{"location":"troubleshooting/#my-server-status-is-unreachable","text":"This problem could be caused by the lack of memory on your server. Make sure you have enough memory: $ free -m If you don't have enough memory, you can use Linux Swap. Make sure you're using swap by executing: $ sudo swapon -s If not, follow this guide to add swap (Ubuntu). If you still have the problem please contact AnaxExp support team.","title":"My server status is unreachable"},{"location":"troubleshooting/#cannot-connect-server","text":"There are few known reasons for that: Make sure your server satisfies requirements and recommendations AnaxExp's script must be run as a root A slow speed of Read/Write operations on a disk. See the list of recommended hosting providers Docker requirements are not met ufw enabled Inbound / outbound external firewall Non-compatible virtualization (virtuozza)","title":"Cannot connect server"},{"location":"troubleshooting/#application-deployment-or-other-tasks-fail","text":"This error means that one of the deployment steps exceeded its timeout. There are few known reasons for that: There's something wrong on our side, see http://status.anaxexp.com/ Something wrong with your server, make sure you have enough free disk space Check your CPU load average by running top Check you have enough free RAM by running free -h Check your system log for extra errors journalctl -f You've reached containers limit per server (300), contact our support to increase the limit A slow speed of Read/Write operations on a disk Huge ping to your server due to global network issues","title":"Application deployment or other tasks fail"},{"location":"troubleshooting/#application-gives-file-not-found-error","text":"This error means that the HTTP server could not find the entrypoint (in case of PHP-based stacks it's usually index.php ) in a container. This might happen for a few reasons: You have your entrypoint (e.g. index.php ) in a subdirectory of your git root and you did not specify it during the initial deployment of new application Your codebase is missing, could be that you've selected a wrong branch during deployment/build","title":"Application gives \"File not found\" error"},{"location":"troubleshooting/#cannot-update-wordpress-core-or-its-pluginsthemes","text":"See this article","title":"Cannot update WordPress core or its plugins/themes"},{"location":"troubleshooting/#infrastructure-5x-known-issues","text":"Sometimes we can't get logs of a task with the error Container not found . Task may have been completed but we consider it as failed Sometimes we can't get the size of a backup archive so we don't show it in the dashboard If you update a rolling-update container and it fails we will not be able to detect the failure. Despite the actual failure the deployment will be considered successful because the older version of the container is still intact We can not handle errors of containers that failed to start, so the task will hang until it expires by timeout. Here's how you can manually check your deployment state in such cases: Access your server via SSH as root Run the following command (replace [INSTANCE UUID] ) kubectl get po -n [ INSTANCE UUID ] You will see statuses of pods (containers) of your application instance. You can get logs of the specific pod (container) either by running (if container is creating or running) kubectl logs [ POD NAME ] -n [ INSTANCE UUID ] or (if container is not currently running or in the error state) kubectl describe po [ POD NAME ] -n [ INSTANCE UUID ] Infrastructure 6.x All the known issues will be resolved in Infrastructure 6.x","title":"Infrastructure 5.x known issues \u2639\ufe0f"},{"location":"apps/backups/","text":"Backups Some stacks provide backups orchestration for services such as database. You can run backups manually for applications based on such stacks from the dashboard. Backups of an application instance will be stored on the same server where the instance is deployed. Auto backups You can enable auto backups for your applications by specifying backup depth (backups older than this # of days will be cleaned up automatically) and time in UTC when to start the backup process. To avoid extra load on your server, backups run successively per server. Mirroring AnaxExp provides backup mirroring to different storages. You can either use your own storage (currently only AWS S3 supported) or AnaxExp Storage. AnaxExp Storage The simplest way to setup backup mirroring is to use simply select AnaxExp Storage as a provider. The default location is US east region if not specified differently. Backups older than a year (365 days) since the moment of upload will be deleted automatically. AWS S3 Specify the following AWS credentials to set up mirroring to your S3 bucket: AWS Access Key Id AWS Secret Access Key Bucket name Bucket region \u200b How backup process affect my server performance We limit all backup-related tasks by CPU and RAM including mirroring to decrease the impact on your server performance. Additionally, we recommend to set auto backups start time to a night period when your application have the lowest traffic.","title":"Backups"},{"location":"apps/backups/#backups","text":"Some stacks provide backups orchestration for services such as database. You can run backups manually for applications based on such stacks from the dashboard. Backups of an application instance will be stored on the same server where the instance is deployed.","title":"Backups"},{"location":"apps/backups/#auto-backups","text":"You can enable auto backups for your applications by specifying backup depth (backups older than this # of days will be cleaned up automatically) and time in UTC when to start the backup process. To avoid extra load on your server, backups run successively per server.","title":"Auto backups"},{"location":"apps/backups/#mirroring","text":"AnaxExp provides backup mirroring to different storages. You can either use your own storage (currently only AWS S3 supported) or AnaxExp Storage.","title":"Mirroring"},{"location":"apps/backups/#anaxexp-storage","text":"The simplest way to setup backup mirroring is to use simply select AnaxExp Storage as a provider. The default location is US east region if not specified differently. Backups older than a year (365 days) since the moment of upload will be deleted automatically.","title":"AnaxExp Storage"},{"location":"apps/backups/#aws-s3","text":"Specify the following AWS credentials to set up mirroring to your S3 bucket: AWS Access Key Id AWS Secret Access Key Bucket name Bucket region \u200b","title":"AWS S3"},{"location":"apps/backups/#how-backup-process-affect-my-server-performance","text":"We limit all backup-related tasks by CPU and RAM including mirroring to decrease the impact on your server performance. Additionally, we recommend to set auto backups start time to a night period when your application have the lowest traffic.","title":"How backup process affect my server performance"},{"location":"apps/deploy/","text":"Code deployment Direct git deployments Only for Drupal and WP Direct git deployment is available only for Drupal and WordPress stacks and their forks You can connect your git repository to AnaxExp and use it as a codebase source for your applications. On code deployment we will perform pull from the target branch and run post-deployment scripts (if enabled). Additionally, you can run deployment automatically every time you push code to a git branch (all instances using this branch will be deployed): Configure git hooks for your git repository Navigate to Instance Deployment Settings and check Automatic deploy option For details instructions how to connect a repository and configure hooks see the following articles: GitHub BitBucket GitLab Custom git provider CI/CD Sometimes direct git integration may not be enough for a few reasons: Build stage is a must have for repositories with dependencies (e.g. npm, composer) You need to run tests Direct git deployment cannot be used for custom stacks and cluster deployments With CI/CD you have build artifacts like docker images that you can download locally With CI/CD you can rollback build (feature TBA) AnaxExp CI Coming soon We plan to replace direct git integration with our built-in CI system Via third-party CI You can set up CI/CD workflow for your application by integrating AnaxExp with third-party CI tools. Build can be performed on any CI tools with AnaxExp CLI. Big picture: Deploy an app based on a managed stack that supports CI deployments or custom stack with at least one service having deployment.type=ci Get AnaxExp CLI tool or its docker image Initialize the build by providing your AnaxExp API key and UUID of your app instance Build images with your codebase. Images will be based on the images from your stack Push ( release ) images to a private docker registry we provide you (or any other registry) Deploy the build (a set of images) to your app instance AnaxExp CLI VM-based builds over docker-in-docker If your CI tool can run builds both in Docker and Virtual Machine (docker daemon must be available) we recommend using the latter because it's faster You can install the latest stable AnaxExp CLI (Linux amd64) tool during the build like this: wget -qO- https://api.anaxexp.com/api/v1/get/cli | sh If you want to install it locally for other systems such as macOS or Windows, or install a specific version follow the instructions at https://github.com/anaxexp/anaxexp-cli Or you can use anaxexp/anaxexp-cli docker image if your CI supports only docker-based builds Init anaxexp ci init [ INSTANCE UUID ] This command will gather build information about your instance such as services (images) that can be built and private docker registry credentials. All builds must start with the init. To perform this step you must have your AnaxExp API key exported as $ANAXEXP_API_KEY or provided via --api-key . Make sure the key is secured and not exposed to public. Build Services available during the build If you're building a managed stack, the list of services eligible for the build is hardcoded and you can find it in a stack documentation . If you're building a custom stack, all services that have deployment.type=ci will be available During the build stage you can prepare your codebase for the build by running anaxexp ci run which is basically a wrapper of docker run . You can either specify a docker image that runs a command: anaxexp ci run -i anaxexp/node:8 -- yarn install anaxexp ci run -i anaxexp/node:9 -p path/to/frontend -- yarn install Or specify a service name from your stack, the image of your current stack version will be used: anaxexp ci run -s backend -- composer install --prefer-dist -n -o --no-dev You can use bind mounts on libraries cache directories to utilize caching capabilities of your CI tool ( .travis.yml example): cache: directories: - /home/travis/.composer/cache script: - anaxexp ci run -v $HOME/.composer:$HOME/.composer -s php -- composer install Once the codebase is ready you can run the build via anaxexp ci build which is a wrapper of docker build . By default the build command builds a new image based on the image of a service you specified, and copies codebase (contents of the current directory, same as --from \\. ) to service's image default working directory: # Build all ci services images anaxexp ci build # Same thing anaxexp ci build --from \\. # Build php service image anaxexp ci build php # Build all images of services starting with node- anaxexp ci build node-* # Build php service image with the contents from ./build directory anaxexp ci build php --from ./build # Build node service image with the contents from ./build directory to /usr/src/app directory inside node image anaxexp ci build node --from ./build --to /usr/src/app Or you can build from your own Dockerfile : anaxexp ci build --dockerfile /path/to/my/Dockerfile If you're using custom Dockerfile make sure it starts with the following lines to make sure it will be based on the image from your stack: ARG ANAXEXP_BASE_IMAGE FROM ${ANAXEXP_BASE_IMAGE} By default we build images with the name (tag) of a private docker registry we provide. But you can use your own registry: anaxexp ci build -t my-private-docker-hub/repository Release Docker registry AnaxExp provides a private docker registry registry.anaxexp.com which used by default. You can use custom docker registry during the build but if it's a private one make sure to add the appropriate docker registry integration so servers where you deploy instances can access your images. How to download images? Once you deployed your first build you can find images' URLs on Instance Stack page. You can get those images locally by running docker login registry.anaxexp.com and entering your AnaxExp user's email/password. Once images are built, you can push them to a docker registry: # Push all images to the default docker registry anaxexp ci release # Push images of specific services anaxexp ci release php node # Push to a custom docker registry anaxexp ci release -t my-private-docker-hub/repository # Additionally push with the tag of the current git branch name anaxexp ci release -t my-private-docker-hub/repository -b Deploy # Deploy all services from the default docker registry anaxexp ci deploy # Deploy specific services anaxexp ci deploy php crond # Deploy all services from a custom docker registry anaxexp ci deploy -t my-private-docker-hub/repository Examples You can find build examples for different CI services such as CircleCI, TravisCI, BitBucket pipelines and custom shell scripts at https://github.com/anaxexp/anaxexp-ci","title":"Deployment"},{"location":"apps/deploy/#code-deployment","text":"","title":"Code deployment"},{"location":"apps/deploy/#direct-git-deployments","text":"Only for Drupal and WP Direct git deployment is available only for Drupal and WordPress stacks and their forks You can connect your git repository to AnaxExp and use it as a codebase source for your applications. On code deployment we will perform pull from the target branch and run post-deployment scripts (if enabled). Additionally, you can run deployment automatically every time you push code to a git branch (all instances using this branch will be deployed): Configure git hooks for your git repository Navigate to Instance Deployment Settings and check Automatic deploy option For details instructions how to connect a repository and configure hooks see the following articles: GitHub BitBucket GitLab Custom git provider","title":"Direct git deployments"},{"location":"apps/deploy/#cicd","text":"Sometimes direct git integration may not be enough for a few reasons: Build stage is a must have for repositories with dependencies (e.g. npm, composer) You need to run tests Direct git deployment cannot be used for custom stacks and cluster deployments With CI/CD you have build artifacts like docker images that you can download locally With CI/CD you can rollback build (feature TBA)","title":"CI/CD"},{"location":"apps/deploy/#anaxexp-ci","text":"Coming soon We plan to replace direct git integration with our built-in CI system","title":"AnaxExp CI"},{"location":"apps/deploy/#via-third-party-ci","text":"You can set up CI/CD workflow for your application by integrating AnaxExp with third-party CI tools. Build can be performed on any CI tools with AnaxExp CLI. Big picture: Deploy an app based on a managed stack that supports CI deployments or custom stack with at least one service having deployment.type=ci Get AnaxExp CLI tool or its docker image Initialize the build by providing your AnaxExp API key and UUID of your app instance Build images with your codebase. Images will be based on the images from your stack Push ( release ) images to a private docker registry we provide you (or any other registry) Deploy the build (a set of images) to your app instance","title":"Via third-party CI"},{"location":"apps/deploy/#anaxexp-cli","text":"VM-based builds over docker-in-docker If your CI tool can run builds both in Docker and Virtual Machine (docker daemon must be available) we recommend using the latter because it's faster You can install the latest stable AnaxExp CLI (Linux amd64) tool during the build like this: wget -qO- https://api.anaxexp.com/api/v1/get/cli | sh If you want to install it locally for other systems such as macOS or Windows, or install a specific version follow the instructions at https://github.com/anaxexp/anaxexp-cli Or you can use anaxexp/anaxexp-cli docker image if your CI supports only docker-based builds","title":"AnaxExp CLI"},{"location":"apps/deploy/#init","text":"anaxexp ci init [ INSTANCE UUID ] This command will gather build information about your instance such as services (images) that can be built and private docker registry credentials. All builds must start with the init. To perform this step you must have your AnaxExp API key exported as $ANAXEXP_API_KEY or provided via --api-key . Make sure the key is secured and not exposed to public.","title":"Init"},{"location":"apps/deploy/#build","text":"Services available during the build If you're building a managed stack, the list of services eligible for the build is hardcoded and you can find it in a stack documentation . If you're building a custom stack, all services that have deployment.type=ci will be available During the build stage you can prepare your codebase for the build by running anaxexp ci run which is basically a wrapper of docker run . You can either specify a docker image that runs a command: anaxexp ci run -i anaxexp/node:8 -- yarn install anaxexp ci run -i anaxexp/node:9 -p path/to/frontend -- yarn install Or specify a service name from your stack, the image of your current stack version will be used: anaxexp ci run -s backend -- composer install --prefer-dist -n -o --no-dev You can use bind mounts on libraries cache directories to utilize caching capabilities of your CI tool ( .travis.yml example): cache: directories: - /home/travis/.composer/cache script: - anaxexp ci run -v $HOME/.composer:$HOME/.composer -s php -- composer install Once the codebase is ready you can run the build via anaxexp ci build which is a wrapper of docker build . By default the build command builds a new image based on the image of a service you specified, and copies codebase (contents of the current directory, same as --from \\. ) to service's image default working directory: # Build all ci services images anaxexp ci build # Same thing anaxexp ci build --from \\. # Build php service image anaxexp ci build php # Build all images of services starting with node- anaxexp ci build node-* # Build php service image with the contents from ./build directory anaxexp ci build php --from ./build # Build node service image with the contents from ./build directory to /usr/src/app directory inside node image anaxexp ci build node --from ./build --to /usr/src/app Or you can build from your own Dockerfile : anaxexp ci build --dockerfile /path/to/my/Dockerfile If you're using custom Dockerfile make sure it starts with the following lines to make sure it will be based on the image from your stack: ARG ANAXEXP_BASE_IMAGE FROM ${ANAXEXP_BASE_IMAGE} By default we build images with the name (tag) of a private docker registry we provide. But you can use your own registry: anaxexp ci build -t my-private-docker-hub/repository","title":"Build"},{"location":"apps/deploy/#release","text":"Docker registry AnaxExp provides a private docker registry registry.anaxexp.com which used by default. You can use custom docker registry during the build but if it's a private one make sure to add the appropriate docker registry integration so servers where you deploy instances can access your images. How to download images? Once you deployed your first build you can find images' URLs on Instance Stack page. You can get those images locally by running docker login registry.anaxexp.com and entering your AnaxExp user's email/password. Once images are built, you can push them to a docker registry: # Push all images to the default docker registry anaxexp ci release # Push images of specific services anaxexp ci release php node # Push to a custom docker registry anaxexp ci release -t my-private-docker-hub/repository # Additionally push with the tag of the current git branch name anaxexp ci release -t my-private-docker-hub/repository -b","title":"Release"},{"location":"apps/deploy/#deploy","text":"# Deploy all services from the default docker registry anaxexp ci deploy # Deploy specific services anaxexp ci deploy php crond # Deploy all services from a custom docker registry anaxexp ci deploy -t my-private-docker-hub/repository","title":"Deploy"},{"location":"apps/deploy/#examples","text":"You can find build examples for different CI services such as CircleCI, TravisCI, BitBucket pipelines and custom shell scripts at https://github.com/anaxexp/anaxexp-ci","title":"Examples"},{"location":"apps/domains/","text":"Domains Technical .wod.by domain name AnaxExp provides a short *.wod.by domain name to every application, it depends on the name of the application and the name of the organization. It looks like this: [instance].[app name].[organization name].wod.by you can find your technical domain from Instance Domains page . You can restrict access to technical domains by: Disabling domain Setting basic auth for this domain Attaching custom domain name You can attach your custom domain name by following these steps: Go to Domains Add on your instance page Add your domain Add an A record targeted to your server IP from your DNS provider's control panel You're all set. Please note that DNS records propagation may take up to 24 hours You can attach wildcard domains, e.g. *.example.com WWW redirects If you want your www subdomain to automatically redirect to non-www domain or vise-versa, you can do it from a domain edit page by selecting one of the redirect actions. Please note, you must add both www and non-www domains manually before configuring WWW redirect actions. Basic auth You can enable basic auth from Instance Domains Basic auth. Only single basic auth can be set up per instance but you can choose which domains you want to apply it to. Indexation by Search Robots All technical *.wod.by domains are not indexed by search engines (header X-Robots-Tag). Additionally, you can optionally prevent indexation for your custom domains on domain edit/add pages. HTTPs Let's Encrypt Certificates You can enable HTTPS for your custom domains by requesting a free SSL certificate from Let's Encrypt . Navigate to Domains tab of your instance page and click on Get certificate in the list of domains (currently not available for *.wod.by domains). Choose a provider \"Let's encrypt\" and submit the form. Before requesting the certificate make sure that your domain already attached to the server where the instance is currently deployed. Additionally you can enable redirect for all HTTP requests to HTTPS from a domain edit page. Let's Encrypt certificates renewal We automatically renew all certificates acquired via Let's Encrypt. Let's Encrypt limitations Let's Encrypt have some rate limits and may not be trusted by old browsers Custom SSL Certificates It's not currently possible to upload certificates from the dashboard. Please contact us if you need to set up a custom SSL certificate. HSTS Since infrastructure version = 5.5.3 you can configure HSTS per domain via three provided options: Disable Enable for this domain (default) Enable for this domain and all subdomains Max age is 31536000 and cannot be changed.","title":"Domains"},{"location":"apps/domains/#domains","text":"","title":"Domains"},{"location":"apps/domains/#technical-wodby-domain-name","text":"AnaxExp provides a short *.wod.by domain name to every application, it depends on the name of the application and the name of the organization. It looks like this: [instance].[app name].[organization name].wod.by you can find your technical domain from Instance Domains page . You can restrict access to technical domains by: Disabling domain Setting basic auth for this domain","title":"Technical .wod.by domain name"},{"location":"apps/domains/#attaching-custom-domain-name","text":"You can attach your custom domain name by following these steps: Go to Domains Add on your instance page Add your domain Add an A record targeted to your server IP from your DNS provider's control panel You're all set. Please note that DNS records propagation may take up to 24 hours You can attach wildcard domains, e.g. *.example.com","title":"Attaching custom domain name"},{"location":"apps/domains/#www-redirects","text":"If you want your www subdomain to automatically redirect to non-www domain or vise-versa, you can do it from a domain edit page by selecting one of the redirect actions. Please note, you must add both www and non-www domains manually before configuring WWW redirect actions.","title":"WWW redirects"},{"location":"apps/domains/#basic-auth","text":"You can enable basic auth from Instance Domains Basic auth. Only single basic auth can be set up per instance but you can choose which domains you want to apply it to.","title":"Basic auth"},{"location":"apps/domains/#indexation-by-search-robots","text":"All technical *.wod.by domains are not indexed by search engines (header X-Robots-Tag). Additionally, you can optionally prevent indexation for your custom domains on domain edit/add pages.","title":"Indexation by Search Robots"},{"location":"apps/domains/#https","text":"","title":"HTTPs"},{"location":"apps/domains/#lets-encrypt-certificates","text":"You can enable HTTPS for your custom domains by requesting a free SSL certificate from Let's Encrypt . Navigate to Domains tab of your instance page and click on Get certificate in the list of domains (currently not available for *.wod.by domains). Choose a provider \"Let's encrypt\" and submit the form. Before requesting the certificate make sure that your domain already attached to the server where the instance is currently deployed. Additionally you can enable redirect for all HTTP requests to HTTPS from a domain edit page. Let's Encrypt certificates renewal We automatically renew all certificates acquired via Let's Encrypt. Let's Encrypt limitations Let's Encrypt have some rate limits and may not be trusted by old browsers","title":"Let's Encrypt Certificates"},{"location":"apps/domains/#custom-ssl-certificates","text":"It's not currently possible to upload certificates from the dashboard. Please contact us if you need to set up a custom SSL certificate.","title":"Custom SSL Certificates"},{"location":"apps/domains/#hsts","text":"Since infrastructure version = 5.5.3 you can configure HSTS per domain via three provided options: Disable Enable for this domain (default) Enable for this domain and all subdomains Max age is 31536000 and cannot be changed.","title":"HSTS"},{"location":"apps/instances/","text":"Instances All apps have instances Every application deployed via AnaxExp has at least one instance. In other words, when we say app what we really mean is application instance . What is instance? \u200b Application instance is a single isolated environment of your application (dev, staging, prod, feature, etc). Every app can have an unlimited number of instances but at least one. By default we deploy development (or just dev) instance of your application. But you can deploy as many instances as you want. You can also deploy instances of the same applications across different servers/clusters. \u200b You can remove or add a new instance from the Instances page. To get there navigate to the instance page and click on a cogwheel in the header. Instances' data As for infrastructure 5.x when you delete an instance we do not delete its backups and files. See this article to learn how to clean them up. Instance type There are 3 types of instances provided by default: dev, staging and production. The difference between instance types depends on a stack (see your stack documentation) but usually they differ in error reporting level, e.g. show all errors on dev instances and none on production. Some stacks may have no difference in configuration for different instance types, in this case, we recommend using production instance type in case changes affecting this behaviour will be introduced in future. \u200b How prod environments are different: Health checks enabled for all containers Less errors show up","title":"Instances"},{"location":"apps/instances/#instances","text":"All apps have instances Every application deployed via AnaxExp has at least one instance. In other words, when we say app what we really mean is application instance .","title":"Instances"},{"location":"apps/instances/#what-is-instance","text":"\u200b Application instance is a single isolated environment of your application (dev, staging, prod, feature, etc). Every app can have an unlimited number of instances but at least one. By default we deploy development (or just dev) instance of your application. But you can deploy as many instances as you want. You can also deploy instances of the same applications across different servers/clusters. \u200b You can remove or add a new instance from the Instances page. To get there navigate to the instance page and click on a cogwheel in the header. Instances' data As for infrastructure 5.x when you delete an instance we do not delete its backups and files. See this article to learn how to clean them up.","title":"What is instance?"},{"location":"apps/instances/#instance-type","text":"There are 3 types of instances provided by default: dev, staging and production. The difference between instance types depends on a stack (see your stack documentation) but usually they differ in error reporting level, e.g. show all errors on dev instances and none on production. Some stacks may have no difference in configuration for different instance types, in this case, we recommend using production instance type in case changes affecting this behaviour will be introduced in future. \u200b How prod environments are different: Health checks enabled for all containers Less errors show up","title":"Instance type"},{"location":"apps/logs/","text":"Logging Most of the containers used in stack managed by AnaxExp built in a way so all software running in containers stream their logs to the output of container and handled by Docker and Kubernetes. Logs are not persistent, if you restart a container (e.g. happens when you configure a service and redeploy stack) you will lose it. There 2 ways to get your applications logs: Log streaming from dashboard Go to Instance Logs , choose a service (container) and click Stream. Last N log messages will be fetched, plus all new messages will be shown in real-time. \u200b \u200b \u200b CLI with kubectl Go to Instance Stack Service and copy Show logs command . Connect to the host server as root and run the command. Adjust the value of tail param to specify how many messages to fetch. For more params see kubectl logs reference . Some software may additionally store their application logs in files inside of a container In some cases you can stream application's logs (e.g. watchdog in Drupal stack) to an additional syslog container via modules/libraries like Monolog.","title":"Logging"},{"location":"apps/logs/#logging","text":"Most of the containers used in stack managed by AnaxExp built in a way so all software running in containers stream their logs to the output of container and handled by Docker and Kubernetes. Logs are not persistent, if you restart a container (e.g. happens when you configure a service and redeploy stack) you will lose it. There 2 ways to get your applications logs:","title":"Logging"},{"location":"apps/logs/#log-streaming-from-dashboard","text":"Go to Instance Logs , choose a service (container) and click Stream. Last N log messages will be fetched, plus all new messages will be shown in real-time. \u200b \u200b \u200b","title":"Log streaming from dashboard"},{"location":"apps/logs/#cli-with-kubectl","text":"Go to Instance Stack Service and copy Show logs command . Connect to the host server as root and run the command. Adjust the value of tail param to specify how many messages to fetch. For more params see kubectl logs reference . Some software may additionally store their application logs in files inside of a container In some cases you can stream application's logs (e.g. watchdog in Drupal stack) to an additional syslog container via modules/libraries like Monolog.","title":"CLI with kubectl"},{"location":"apps/new/","text":"Deploying new application Step 1 \u2013 choose stack and destination server/cluster \u200b \u200b Select one of your connected servers or Demo server by AnaxExp where you want to deploy your applications. Demo server All applications deployed to Demo server will be automatically cleaned in 12 hours. Choose a stack for deployment. Some stacks may provide optional services (e.g. redis as cache storage) that you can either enable or disable in your stack, and multiple implementation for a service (e.g. Nginx or Apache as an HTTP server). You can always enable/disable a service after the deployment. Step 2 \u2013 configure your application instance \u200b \u200b \u200b Enter the name of your application and select the instance type (Dev by default). Your application hostname will be automatically generated based on the name, you can optionally adjust it. You can change all these settings later after initial deployment. Step 3 \u2013 stack configuration (optional) Some stacks like Drupal and WordPress provide additional configuration on step 3 such as a code deployment workflow and import. \u200b","title":"New app"},{"location":"apps/new/#deploying-new-application","text":"","title":"Deploying new application"},{"location":"apps/new/#step-1-choose-stack-and-destination-servercluster","text":"\u200b \u200b Select one of your connected servers or Demo server by AnaxExp where you want to deploy your applications. Demo server All applications deployed to Demo server will be automatically cleaned in 12 hours. Choose a stack for deployment. Some stacks may provide optional services (e.g. redis as cache storage) that you can either enable or disable in your stack, and multiple implementation for a service (e.g. Nginx or Apache as an HTTP server). You can always enable/disable a service after the deployment.","title":"Step 1 \u2013 choose stack and destination server/cluster"},{"location":"apps/new/#step-2-configure-your-application-instance","text":"\u200b \u200b \u200b Enter the name of your application and select the instance type (Dev by default). Your application hostname will be automatically generated based on the name, you can optionally adjust it. You can change all these settings later after initial deployment.","title":"Step 2 \u2013 configure your application instance"},{"location":"apps/new/#step-3-stack-configuration-optional","text":"Some stacks like Drupal and WordPress provide additional configuration on step 3 such as a code deployment workflow and import. \u200b","title":"Step 3 \u2013 stack configuration (optional)"},{"location":"apps/post-deployment-scripts/","text":"Post-deployment scripts Only for PHP Currently this option available only for PHP stacks Intro AnaxExp provides you a way to run your scripts after each deployment. You can defined post-deployment scripts in anaxexp.yml file via pipelines (example for command stage ): pipeline: - name: Drupal 8 clear cache on dev type: command command: drush cr directory: $HTTP_ROOT only_if: test $ANAXEXP_ENVIRONMENT_TYPE = dev Root directory In the example above $HTTP_ROOT used as a directory instead of $APP_ROOT because drupal root is in a subdirectory (composer-based project) Or like this: pipeline: - name: Run my custom script type: command command: ./my-script.sh directory: $APP_ROOT The pipeline is an automated manifestation of your deployment process. In other words, it's just a set of post-deployment actions to execute Available environment variables See Environment Variables article . Cleanup pipeline anaxexp.yml can have one cleanup block; cleanup is another pipeline which needs to be executed after a pipeline has either failed or passed. In the cleanup block, we can add command or shell script stages. The below example create a log file in pipeline and then cleanup the log file in the cleaup steps. pipeline: - name: start pipeline command: echo \u201cpipeline\u201d log/log.txt cleanup: - name: cleanup command: rm log/* Pipeline stages Stage in pipeline has three elements, name, type and configurations. configuration elements are optional. The elements of configurations depend on the type. For example command_stage type has command configuration, which specify the shell command run in the stage. The following is the table on the type and the parameters. Command stage Command stage executes one command. Users specify Command stage adding command in type. The following is the parameter of Command stage. Configuration Optional meaning command false shell command run in the stage only_if true run specified command on when the condition written in only_if is satisfied directory true the directory where anaxexp runs the specified command Parallel stages You can set child stages and run these stages in parallel like this. pipeline: - name: parallel stages parallel: - name: parallel command 1 type: command command: parallel command 1 - name: parallel command 2 type: command command: parallel command 2 - name: parallel command 3 type: command command: parallel command 3 In the above setting, parallel command 1, parallel command 2 and parallel command 3 are executed in parallel. Reusing the results from stages AnaxExp stores the results of preceding stages. The stages can make use of the results of finished stages using the three special variables ( __OUT , __ERR , __COMBINED and __RESULT ) in anaxexp.yml configuration files. __OUT - output flushed to standard output __ERR - output flushed to standard error __COMBINED - combined output of stdout and stderr __RESULT - execution result (true or false) The three variables are maps whose keys are stage names and the value are results of the stages. For example, we want the standard output result of the stage named \"stage1\", we write __OUT[\"stage1\"]. The following is a sample configuration with a special value. pipeline: - name: stage_1 command: echo hello world - name: stage_2 command: echo __OUT[ stage_1 ] AnaxExp with the above configuration outputs \"hello world\" twice, since the second stage (stage_2) flushes the standard output result of the first stage (stage_1). Wait for running stages until the conditions are satisfied The stage starts immediately after the previous stage finish, but some stages need to wait for some action such as port is ready or file are created. wait_for feature supports the actions which need to be ready before the stages begin. wait_for is defined as a property of stage. pipeline: - name: launch solr command: bin/solr start - name: post data to solr index command: bin/post -d ~/tmp/foobar.js wait_for: host=localhost port=8983 state=ready The wait_for property takes the key value pairs. Key has several variations. The value depends on the key type. The following table shows the supported key value pairs and the description. Key Value (value type) Description delay second (float) Seconds to wait after the previous stage finish port port number (int) Port number file file name (string) File to be created in the previous stages host host (string) IP address or host name state state of the other key (string) Four types of states are supported. The possible value is dependent to the other Keys. State values There are several state values and possible state values are depend on the other key. There are seveal state values and possible state values are depend on the other key. State value Description present / ready Specified port is ready or file is created. absent / unready port is not active or file does not exist Creating your own pipeline stage You can create your own pipeline stage and then use them in your pipeline. The the example below where we define two simple pipelines hello and goodbye . namespace: mypackage stages: - def: name: hello command: echo May I help you majesty! - def: name: goodbye command: echo Goobye majesty. To import stages to pipeline configuration file, we use require block and add the list of file names into the block. For example, the following example import the stages defined in conf/mystage.yml require: - conf/mystages.yml pipeline: - call: mypackage::hello - call: mypackage::goodbye In the above setting, the stages ( mypackage::hello and mypackage::goodbye ) which are defined in mystage.yml are specified. The files specified in pipeline configuration file need to have two blocks namespace and stages . In namespace, we add the package name, the package name is need to avoid collisions of the stage names in multiple required files. The stages block contains the list of stage definitions. We can define the stages same as the stages in pipeline configurations. Initializing git submodules pipeline : - name : Ignore SSH host key check for github and bitbucket type : command command : printf Host bitbucket.org github.com\\n\\tStrictHostKeyChecking no\\n ~/.ssh/config - name : Git sub-modules setup type : command command : git submodule update --init --checkout directory : $ANAXEXP_APP_ROOT Logs You can find output logs of executed post-deployment scripts under Instance Tasks","title":"Post-deployment scripts"},{"location":"apps/post-deployment-scripts/#post-deployment-scripts","text":"Only for PHP Currently this option available only for PHP stacks","title":"Post-deployment scripts"},{"location":"apps/post-deployment-scripts/#intro","text":"AnaxExp provides you a way to run your scripts after each deployment. You can defined post-deployment scripts in anaxexp.yml file via pipelines (example for command stage ): pipeline: - name: Drupal 8 clear cache on dev type: command command: drush cr directory: $HTTP_ROOT only_if: test $ANAXEXP_ENVIRONMENT_TYPE = dev Root directory In the example above $HTTP_ROOT used as a directory instead of $APP_ROOT because drupal root is in a subdirectory (composer-based project) Or like this: pipeline: - name: Run my custom script type: command command: ./my-script.sh directory: $APP_ROOT The pipeline is an automated manifestation of your deployment process. In other words, it's just a set of post-deployment actions to execute","title":"Intro"},{"location":"apps/post-deployment-scripts/#available-environment-variables","text":"See Environment Variables article .","title":"Available environment variables"},{"location":"apps/post-deployment-scripts/#cleanup-pipeline","text":"anaxexp.yml can have one cleanup block; cleanup is another pipeline which needs to be executed after a pipeline has either failed or passed. In the cleanup block, we can add command or shell script stages. The below example create a log file in pipeline and then cleanup the log file in the cleaup steps. pipeline: - name: start pipeline command: echo \u201cpipeline\u201d log/log.txt cleanup: - name: cleanup command: rm log/*","title":"Cleanup pipeline"},{"location":"apps/post-deployment-scripts/#pipeline-stages","text":"Stage in pipeline has three elements, name, type and configurations. configuration elements are optional. The elements of configurations depend on the type. For example command_stage type has command configuration, which specify the shell command run in the stage. The following is the table on the type and the parameters.","title":"Pipeline stages"},{"location":"apps/post-deployment-scripts/#command-stage","text":"Command stage executes one command. Users specify Command stage adding command in type. The following is the parameter of Command stage. Configuration Optional meaning command false shell command run in the stage only_if true run specified command on when the condition written in only_if is satisfied directory true the directory where anaxexp runs the specified command","title":"Command stage"},{"location":"apps/post-deployment-scripts/#parallel-stages","text":"You can set child stages and run these stages in parallel like this. pipeline: - name: parallel stages parallel: - name: parallel command 1 type: command command: parallel command 1 - name: parallel command 2 type: command command: parallel command 2 - name: parallel command 3 type: command command: parallel command 3 In the above setting, parallel command 1, parallel command 2 and parallel command 3 are executed in parallel.","title":"Parallel stages"},{"location":"apps/post-deployment-scripts/#reusing-the-results-from-stages","text":"AnaxExp stores the results of preceding stages. The stages can make use of the results of finished stages using the three special variables ( __OUT , __ERR , __COMBINED and __RESULT ) in anaxexp.yml configuration files. __OUT - output flushed to standard output __ERR - output flushed to standard error __COMBINED - combined output of stdout and stderr __RESULT - execution result (true or false) The three variables are maps whose keys are stage names and the value are results of the stages. For example, we want the standard output result of the stage named \"stage1\", we write __OUT[\"stage1\"]. The following is a sample configuration with a special value. pipeline: - name: stage_1 command: echo hello world - name: stage_2 command: echo __OUT[ stage_1 ] AnaxExp with the above configuration outputs \"hello world\" twice, since the second stage (stage_2) flushes the standard output result of the first stage (stage_1).","title":"Reusing the results from stages"},{"location":"apps/post-deployment-scripts/#wait-for-running-stages-until-the-conditions-are-satisfied","text":"The stage starts immediately after the previous stage finish, but some stages need to wait for some action such as port is ready or file are created. wait_for feature supports the actions which need to be ready before the stages begin. wait_for is defined as a property of stage. pipeline: - name: launch solr command: bin/solr start - name: post data to solr index command: bin/post -d ~/tmp/foobar.js wait_for: host=localhost port=8983 state=ready The wait_for property takes the key value pairs. Key has several variations. The value depends on the key type. The following table shows the supported key value pairs and the description. Key Value (value type) Description delay second (float) Seconds to wait after the previous stage finish port port number (int) Port number file file name (string) File to be created in the previous stages host host (string) IP address or host name state state of the other key (string) Four types of states are supported. The possible value is dependent to the other Keys.","title":"Wait for running stages until the conditions are satisfied"},{"location":"apps/post-deployment-scripts/#state-values","text":"There are several state values and possible state values are depend on the other key. There are seveal state values and possible state values are depend on the other key. State value Description present / ready Specified port is ready or file is created. absent / unready port is not active or file does not exist","title":"State values"},{"location":"apps/post-deployment-scripts/#creating-your-own-pipeline-stage","text":"You can create your own pipeline stage and then use them in your pipeline. The the example below where we define two simple pipelines hello and goodbye . namespace: mypackage stages: - def: name: hello command: echo May I help you majesty! - def: name: goodbye command: echo Goobye majesty. To import stages to pipeline configuration file, we use require block and add the list of file names into the block. For example, the following example import the stages defined in conf/mystage.yml require: - conf/mystages.yml pipeline: - call: mypackage::hello - call: mypackage::goodbye In the above setting, the stages ( mypackage::hello and mypackage::goodbye ) which are defined in mystage.yml are specified. The files specified in pipeline configuration file need to have two blocks namespace and stages . In namespace, we add the package name, the package name is need to avoid collisions of the stage names in multiple required files. The stages block contains the list of stage definitions. We can define the stages same as the stages in pipeline configurations.","title":"Creating your own pipeline stage"},{"location":"apps/post-deployment-scripts/#initializing-git-submodules","text":"pipeline : - name : Ignore SSH host key check for github and bitbucket type : command command : printf Host bitbucket.org github.com\\n\\tStrictHostKeyChecking no\\n ~/.ssh/config - name : Git sub-modules setup type : command command : git submodule update --init --checkout directory : $ANAXEXP_APP_ROOT","title":"Initializing git submodules"},{"location":"apps/post-deployment-scripts/#logs","text":"You can find output logs of executed post-deployment scripts under Instance Tasks","title":"Logs"},{"location":"cluster/","text":"Managed Kubernetes cluster Overview In addition to our single server infrastructure we offer a Kubernetes cluster solution. Currently, we offer cluster setup only for public clouds (AWS, GCP, Azure). Only specific stacks / services provided by AnaxExp are suitable for cluster deployment. Setup and maintenance of the cluster is significantly more complex and expensive than a single-server setup. Normally we recommend cluster setups only if you need one of the following: Scalability If you're not sure whether a single server is enough to handle your load, this probably means you don't need a cluster. There are two types of scalability for different services of your stack: Stateless services. These services don't care how many replicas are running and on which node (have no state). Can be flexibly scheduled across a pool of auto-scaled nodes. Stateless services can be scaled manually (# of replicas) or automatically (based on resources consumption). Examples: HTTP servers, PHP-FPM. Stateful services. Run on specific nodes (have state) and require additional individual configuration for scalability (read replicas, sharding). Examples: any DBMS, Elasticsearch. Usually public clouds provide stateful services that can be scaled without manual configuration (e.g. AWS RDS, ElastiCache). High availability HA setup guarantees that your application will recover if one of infrastructure services goes down. Services can be restored rapidly but not instantaneously. Suitable for applications that must be restored quickly and can withstand a short interruption should a failure occur. Stateless services configured for auto scheduling across a pool of nodes. New nodes in a pool will spun up automatically when requested. Stateful services configured for high availability (master-master, master-slave setup) or replaced by public cloud managed services with built-in high availability (e.g. AWS RDS instead of Galera Cluster). High availability usually implies that the cluster deployed across 2 or more availability zones. Fault tolerance A fault tolerant environment has no service interruption but a significantly higher cost. We run additional services replicas in multiple availability zones (inside the same region) for redundancy. The chance that two (or more) availability zones simultaneously goes down is extremely low but for extra high availability, infrastructure can be deployed across multiple regions. Supported Public Clouds AWS EKS GCP GKE Azure AKS \u200b Request cluster Please fill out the application http://go.wod.by/cluster-application \u200b","title":"Kubernetes cluster"},{"location":"cluster/#managed-kubernetes-cluster","text":"","title":"Managed Kubernetes cluster"},{"location":"cluster/#overview","text":"In addition to our single server infrastructure we offer a Kubernetes cluster solution. Currently, we offer cluster setup only for public clouds (AWS, GCP, Azure). Only specific stacks / services provided by AnaxExp are suitable for cluster deployment. Setup and maintenance of the cluster is significantly more complex and expensive than a single-server setup. Normally we recommend cluster setups only if you need one of the following:","title":"Overview"},{"location":"cluster/#scalability","text":"If you're not sure whether a single server is enough to handle your load, this probably means you don't need a cluster. There are two types of scalability for different services of your stack: Stateless services. These services don't care how many replicas are running and on which node (have no state). Can be flexibly scheduled across a pool of auto-scaled nodes. Stateless services can be scaled manually (# of replicas) or automatically (based on resources consumption). Examples: HTTP servers, PHP-FPM. Stateful services. Run on specific nodes (have state) and require additional individual configuration for scalability (read replicas, sharding). Examples: any DBMS, Elasticsearch. Usually public clouds provide stateful services that can be scaled without manual configuration (e.g. AWS RDS, ElastiCache).","title":"Scalability"},{"location":"cluster/#high-availability","text":"HA setup guarantees that your application will recover if one of infrastructure services goes down. Services can be restored rapidly but not instantaneously. Suitable for applications that must be restored quickly and can withstand a short interruption should a failure occur. Stateless services configured for auto scheduling across a pool of nodes. New nodes in a pool will spun up automatically when requested. Stateful services configured for high availability (master-master, master-slave setup) or replaced by public cloud managed services with built-in high availability (e.g. AWS RDS instead of Galera Cluster). High availability usually implies that the cluster deployed across 2 or more availability zones.","title":"High availability"},{"location":"cluster/#fault-tolerance","text":"A fault tolerant environment has no service interruption but a significantly higher cost. We run additional services replicas in multiple availability zones (inside the same region) for redundancy. The chance that two (or more) availability zones simultaneously goes down is extremely low but for extra high availability, infrastructure can be deployed across multiple regions.","title":"Fault tolerance"},{"location":"cluster/#supported-public-clouds","text":"AWS EKS GCP GKE Azure AKS \u200b","title":"Supported Public Clouds"},{"location":"cluster/#request-cluster","text":"Please fill out the application http://go.wod.by/cluster-application \u200b","title":"Request cluster"},{"location":"infrastructure/","text":"Single-server infrastructure Overview When you connect a server to AnaxExp we deploy container-based infrastructure to this server. This will further allow you to your applications based on stacks. AnaxExp is not hosting provider AnaxExp is not a hosting provider. We believe that there are plenty of reliable providers on the market already. You can connect your own servers from any hosting provider. By connecting your server, you let AnaxExp install infrastructure that will be used to deploy your apps. Both server infrastructure and stacks have versioning. We regularly update them by releasing newer versions, such updates can include security updates and performance improvements. Infrastructure maintenance Stacks maintenance The infrastructure we provide is based strictly on containers and powered by Docker and Kubernetes. Schema Edge is a reverse proxy container (nginx) that proxies requests to instances, perform redirects (http https, www non-www) and terminates SSL \u200b","title":"Overview"},{"location":"infrastructure/#single-server-infrastructure","text":"","title":"Single-server infrastructure"},{"location":"infrastructure/#overview","text":"When you connect a server to AnaxExp we deploy container-based infrastructure to this server. This will further allow you to your applications based on stacks. AnaxExp is not hosting provider AnaxExp is not a hosting provider. We believe that there are plenty of reliable providers on the market already. You can connect your own servers from any hosting provider. By connecting your server, you let AnaxExp install infrastructure that will be used to deploy your apps. Both server infrastructure and stacks have versioning. We regularly update them by releasing newer versions, such updates can include security updates and performance improvements. Infrastructure maintenance Stacks maintenance The infrastructure we provide is based strictly on containers and powered by Docker and Kubernetes.","title":"Overview"},{"location":"infrastructure/#schema","text":"Edge is a reverse proxy container (nginx) that proxies requests to instances, perform redirects (http https, www non-www) and terminates SSL \u200b","title":"Schema"},{"location":"infrastructure/cli/","text":"CLI commands Restart docker and kube services systemctl stop kube-apiserver systemctl stop kube-controller systemctl stop kube-kubelet systemctl stop kube-proxy systemctl stop kube-scheduler systemctl stop docker systemctl start docker systemctl start kube-apiserver systemctl start kube-controller systemctl start kube-kubelet systemctl start kube-proxy systemctl start kube-scheduler Check node status kubectl describe node 127 .0.0.1 Check namespaces (deployed instances) kubectl get ns System logs journalctl -f Check disk space df -h ncdu / Check memory stats vmstat -Sm 1","title":"CLI"},{"location":"infrastructure/cli/#cli-commands","text":"","title":"CLI commands"},{"location":"infrastructure/cli/#restart-docker-and-kube-services","text":"systemctl stop kube-apiserver systemctl stop kube-controller systemctl stop kube-kubelet systemctl stop kube-proxy systemctl stop kube-scheduler systemctl stop docker systemctl start docker systemctl start kube-apiserver systemctl start kube-controller systemctl start kube-kubelet systemctl start kube-proxy systemctl start kube-scheduler","title":"Restart docker and kube services"},{"location":"infrastructure/cli/#check-node-status","text":"kubectl describe node 127 .0.0.1","title":"Check node status"},{"location":"infrastructure/cli/#check-namespaces-deployed-instances","text":"kubectl get ns","title":"Check namespaces (deployed instances)"},{"location":"infrastructure/cli/#system-logs","text":"journalctl -f","title":"System logs"},{"location":"infrastructure/cli/#check-disk-space","text":"df -h ncdu /","title":"Check disk space"},{"location":"infrastructure/cli/#check-memory-stats","text":"vmstat -Sm 1","title":"Check memory stats"},{"location":"infrastructure/connecting-server/","text":"Connecting server to AnaxExp Instructions Before connecting your server to AnaxExp please make sure it satisfies requirements and recommendations Amazon Web Services DigitalOcean Linode Azure Google Cloud Platform Custom Cloud Provider Requirements and recommendations CPU architecture must be 64-bit Supported OS Docker must not be installed (we require a specific version) Disabled or configured ufw The following ports must be free: 80 (http), 443 (https), 31222-32222 (containers) When external firewall used \u2013 open inbound ports 80 (http), 443 (https), 31222-32222 SWAP should either not exist (we set it automatically) or be a half as the size of RAM but in range 2 to 12 GB Recommended minimum of server's RAM is 1GB, Disk is 20GB We strongly recommend to avoid connecting working server Supported OS AnaxExp supports the following Linux distributions (x64 only): Debian 9 stretch (recommended) Ubuntu 16.04 LTS Debian 8 jessie Learn how to detect your distributive and the version s Uninstall To uninstall AnaxExp's infrastructure (5.x) from your server simply execute following command as root: $ anaxexp uninstall","title":"Connecting server"},{"location":"infrastructure/connecting-server/#connecting-server-to-anaxexp","text":"","title":"Connecting server to AnaxExp"},{"location":"infrastructure/connecting-server/#instructions","text":"Before connecting your server to AnaxExp please make sure it satisfies requirements and recommendations Amazon Web Services DigitalOcean Linode Azure Google Cloud Platform Custom Cloud Provider","title":"Instructions"},{"location":"infrastructure/connecting-server/#requirements-and-recommendations","text":"CPU architecture must be 64-bit Supported OS Docker must not be installed (we require a specific version) Disabled or configured ufw The following ports must be free: 80 (http), 443 (https), 31222-32222 (containers) When external firewall used \u2013 open inbound ports 80 (http), 443 (https), 31222-32222 SWAP should either not exist (we set it automatically) or be a half as the size of RAM but in range 2 to 12 GB Recommended minimum of server's RAM is 1GB, Disk is 20GB We strongly recommend to avoid connecting working server","title":"Requirements and recommendations"},{"location":"infrastructure/connecting-server/#supported-os","text":"AnaxExp supports the following Linux distributions (x64 only): Debian 9 stretch (recommended) Ubuntu 16.04 LTS Debian 8 jessie Learn how to detect your distributive and the version s","title":"Supported OS"},{"location":"infrastructure/connecting-server/#uninstall","text":"To uninstall AnaxExp's infrastructure (5.x) from your server simply execute following command as root: $ anaxexp uninstall","title":"Uninstall"},{"location":"infrastructure/containers/","text":"Containers Most of the container images provided in our managed stacks based on a light-weight Linux distribution Alpine Linux (based on musl libc). All images are public and available on Docker Hub , their sources can be found on GitHub . Accessing containers via SSH container Some stacks provide an SSHd container to access your codebase, you can find the SSH command on Instance Stack SSHd page. We'll automatically add your public SSH keys to this container. You can add your public keys from Profile Keys Add new key page. as default user If your stack does not have sshd container or you need to access a different container follow these steps: Connect to the server where app instance is deployed by SSH Navigate to Instance Stack [CONTAINER] from AnaxExp dashboard Copy the Access Command command Execute the copied command on the server as root Now you're inside of the container as container default user as root If you need root permissions inside a container and container's default user is not root, follow these steps: Access a container as a default user and get container hostname by executing the following command: echo $HOSTNAME Execute the following command from a host server as root by replacing [HOSTNAME] with your value docker exec -ti --user = root $( docker ps | grep [ HOSTNAME ] | grep -v pause | awk { print $1 } ) sh Accessing containers data from host Containers persistent data can be accessed from the host server under /srv/anaxexp/instances/[INSTANCE_UUID]\u200b This may cause unexpected issues Be careful while modifying containers' files as root \u2013 it could cause unexpected permissions issues because containers have a default user different from root","title":"Containers"},{"location":"infrastructure/containers/#containers","text":"Most of the container images provided in our managed stacks based on a light-weight Linux distribution Alpine Linux (based on musl libc). All images are public and available on Docker Hub , their sources can be found on GitHub .","title":"Containers"},{"location":"infrastructure/containers/#accessing-containers","text":"","title":"Accessing containers"},{"location":"infrastructure/containers/#via-ssh-container","text":"Some stacks provide an SSHd container to access your codebase, you can find the SSH command on Instance Stack SSHd page. We'll automatically add your public SSH keys to this container. You can add your public keys from Profile Keys Add new key page.","title":"via SSH container"},{"location":"infrastructure/containers/#as-default-user","text":"If your stack does not have sshd container or you need to access a different container follow these steps: Connect to the server where app instance is deployed by SSH Navigate to Instance Stack [CONTAINER] from AnaxExp dashboard Copy the Access Command command Execute the copied command on the server as root Now you're inside of the container as container default user","title":"as default user"},{"location":"infrastructure/containers/#as-root","text":"If you need root permissions inside a container and container's default user is not root, follow these steps: Access a container as a default user and get container hostname by executing the following command: echo $HOSTNAME Execute the following command from a host server as root by replacing [HOSTNAME] with your value docker exec -ti --user = root $( docker ps | grep [ HOSTNAME ] | grep -v pause | awk { print $1 } ) sh","title":"as root"},{"location":"infrastructure/containers/#accessing-containers-data-from-host","text":"Containers persistent data can be accessed from the host server under /srv/anaxexp/instances/[INSTANCE_UUID]\u200b This may cause unexpected issues Be careful while modifying containers' files as root \u2013 it could cause unexpected permissions issues because containers have a default user different from root","title":"Accessing containers data from host"},{"location":"infrastructure/disk/","text":"Disk space management Recovery after out of disk space If your server did not have enough disk space to properly operate we highly encourage your to reboot the server as the state of some infrastructure components may be corrupted Using external volumes for applications data If you want AnaxExp to use an external storage (mounted volume) instead of a server disk follow these steps: 1. Creating new volume and attaching to server See your cloud provider documentation AWS DigitalOcean ... 2. Mounting volume Access your server and execute sudo fdisk -l . Find a device name ( /dev/NAME ) of your volume Create ext4 file system on the new volume: $ mkfs -t ext4 /dev/NAME Create a directory where you want to mount your volume: $ mkdir /mnt/my-volume Mount your volume: $ mount /dev/NAME /mnt/my-volume To mount this EBS volume on every system reboot, add an entry for the device to the /etc/fstab file: /dev/NAME /mnt/my-volume ext4 defaults,noatime 0 2 3. Moving docker and anaxexp's data to new volume ! THIS WILL CAUSE DOWNTIME OF ALL APPLICATIONS ON THE SERVER Stop docker and kubernetes services (systemd): systemctl stop kube-apiserver systemctl stop kube-controller systemctl stop kube-kubelet systemctl stop kube-proxy systemctl stop kube-scheduler systemctl stop docker Move docker's and AnaxExp's directories to your volume and symlink them back: mv /var/lib/docker /mnt/my-volume mv /srv/anaxexp /mnt/my-volume ln -s /mnt/my-volume/docker /var/lib/docker ln -s /mnt/my-volume/anaxexp /srv/anaxexp Start services systemctl start docker systemctl start kube-apiserver systemctl start kube-controller systemctl start kube-kubelet systemctl start kube-proxy systemctl start kube-scheduler That's it, from now on applications-related data will be stored on the mounted volume Freeing disk space We recommend connecting servers with at least 20-40G of disk space and using a separate volumes for your applications data. Checking disk space You can check if you have enough disk space on your server by running: $ df -h If you want to learn what exactly on your server takes disk space, you can run: $ du -sh /path/to/directory/* Or using a tool called ncdu $ apt-get install ncdu $ ncdu /path/to/dir What can I clean up? The most heavy directories are usually: \u200b/srv/anaxexp \u2013 contains persistent files of your applications and backups. Read below how to clean it up \u200b/var/lib/docker \u2013 docker's volumes, containers, images data. Do not clean it up Clean up docker's unused volumes and images $ docker system prune --volumes Backups You can delete old backups of your applications by using the following path: /srv/anaxexp/backups/ [ INSTANCE_UUID ] Deleting application instances files Infrastructure 5.x The following applies only for single-server infrastructure version 5.x \u200b When you delete an instance AnaxExp does not delete containers' persistent files (database, codebase, etc) on your server to ensure no valuable data will be lost. Please follow the instructions below to clean up your server from these outdated files: Move outdated files to a separate directory $ docker run --rm -it -v /srv/anaxexp:/srv/anaxexp anaxexp/cleanup API Token Make sure your applications still operate correctly. Delete outdated files $ rm -rf /srv/anaxexp/_deleted","title":"Disk"},{"location":"infrastructure/disk/#disk-space-management","text":"Recovery after out of disk space If your server did not have enough disk space to properly operate we highly encourage your to reboot the server as the state of some infrastructure components may be corrupted","title":"Disk space management"},{"location":"infrastructure/disk/#using-external-volumes-for-applications-data","text":"If you want AnaxExp to use an external storage (mounted volume) instead of a server disk follow these steps:","title":"Using external volumes for applications data"},{"location":"infrastructure/disk/#1-creating-new-volume-and-attaching-to-server","text":"See your cloud provider documentation AWS DigitalOcean ...","title":"1. Creating new volume and attaching to server"},{"location":"infrastructure/disk/#2-mounting-volume","text":"Access your server and execute sudo fdisk -l . Find a device name ( /dev/NAME ) of your volume Create ext4 file system on the new volume: $ mkfs -t ext4 /dev/NAME Create a directory where you want to mount your volume: $ mkdir /mnt/my-volume Mount your volume: $ mount /dev/NAME /mnt/my-volume To mount this EBS volume on every system reboot, add an entry for the device to the /etc/fstab file: /dev/NAME /mnt/my-volume ext4 defaults,noatime 0 2","title":"2. Mounting volume"},{"location":"infrastructure/disk/#3-moving-docker-and-anaxexps-data-to-new-volume","text":"! THIS WILL CAUSE DOWNTIME OF ALL APPLICATIONS ON THE SERVER Stop docker and kubernetes services (systemd): systemctl stop kube-apiserver systemctl stop kube-controller systemctl stop kube-kubelet systemctl stop kube-proxy systemctl stop kube-scheduler systemctl stop docker Move docker's and AnaxExp's directories to your volume and symlink them back: mv /var/lib/docker /mnt/my-volume mv /srv/anaxexp /mnt/my-volume ln -s /mnt/my-volume/docker /var/lib/docker ln -s /mnt/my-volume/anaxexp /srv/anaxexp Start services systemctl start docker systemctl start kube-apiserver systemctl start kube-controller systemctl start kube-kubelet systemctl start kube-proxy systemctl start kube-scheduler That's it, from now on applications-related data will be stored on the mounted volume","title":"3. Moving docker and anaxexp's data to new volume"},{"location":"infrastructure/disk/#freeing-disk-space","text":"We recommend connecting servers with at least 20-40G of disk space and using a separate volumes for your applications data.","title":"Freeing disk space"},{"location":"infrastructure/disk/#checking-disk-space","text":"You can check if you have enough disk space on your server by running: $ df -h If you want to learn what exactly on your server takes disk space, you can run: $ du -sh /path/to/directory/* Or using a tool called ncdu $ apt-get install ncdu $ ncdu /path/to/dir","title":"Checking disk space"},{"location":"infrastructure/disk/#what-can-i-clean-up","text":"The most heavy directories are usually: \u200b/srv/anaxexp \u2013 contains persistent files of your applications and backups. Read below how to clean it up \u200b/var/lib/docker \u2013 docker's volumes, containers, images data. Do not clean it up","title":"What can I clean up?"},{"location":"infrastructure/disk/#clean-up-dockers-unused-volumes-and-images","text":"$ docker system prune --volumes","title":"Clean up docker's unused volumes and images"},{"location":"infrastructure/disk/#backups","text":"You can delete old backups of your applications by using the following path: /srv/anaxexp/backups/ [ INSTANCE_UUID ]","title":"Backups"},{"location":"infrastructure/disk/#deleting-application-instances-files","text":"Infrastructure 5.x The following applies only for single-server infrastructure version 5.x \u200b When you delete an instance AnaxExp does not delete containers' persistent files (database, codebase, etc) on your server to ensure no valuable data will be lost. Please follow the instructions below to clean up your server from these outdated files: Move outdated files to a separate directory $ docker run --rm -it -v /srv/anaxexp:/srv/anaxexp anaxexp/cleanup API Token Make sure your applications still operate correctly. Delete outdated files $ rm -rf /srv/anaxexp/_deleted","title":"Deleting application instances files"},{"location":"infrastructure/env-vars/","text":"Environment Variables Global The following variables exists in all containers: Variable Description $ANAXEXP_INSTANCE_NAME Instance machine name $ANAXEXP_INSTANCE_TYPE Instance type: dev, stage, prod $ANAXEXP_ENVIRONMENT_TYPE Same as $ANAXEXP_INSTANCE_NAME $ANAXEXP_ENVIRONMENT_TYPE Same as $ANAXEXP_INSTANCE_TYPE $ANAXEXP_INSTANCE_UUID Instance UUID $ANAXEXP_APP_NAME Application machine name $ANAXEXP_APP_UUID Application UUID Stack-specific See stacks documentation to see stack-specific environment variables","title":"Environment variables"},{"location":"infrastructure/env-vars/#environment-variables","text":"","title":"Environment Variables"},{"location":"infrastructure/env-vars/#global","text":"The following variables exists in all containers: Variable Description $ANAXEXP_INSTANCE_NAME Instance machine name $ANAXEXP_INSTANCE_TYPE Instance type: dev, stage, prod $ANAXEXP_ENVIRONMENT_TYPE Same as $ANAXEXP_INSTANCE_NAME $ANAXEXP_ENVIRONMENT_TYPE Same as $ANAXEXP_INSTANCE_TYPE $ANAXEXP_INSTANCE_UUID Instance UUID $ANAXEXP_APP_NAME Application machine name $ANAXEXP_APP_UUID Application UUID","title":"Global"},{"location":"infrastructure/env-vars/#stack-specific","text":"See stacks documentation to see stack-specific environment variables","title":"Stack-specific"},{"location":"infrastructure/hsts/","text":"HSTS HSTS (HTTP Strict Transport Security) is a security feature that lets a web site tell browsers that it should only be communicated with using HTTPS, instead of using HTTP. Read more details on Mozilla Developer Network . HSTS specification was subsequently developed to combat SSL stripping attacks Applications deployed via AnaxExp has enabled HSTS by default (header Strict-Transport-Security max-age=31536000 ). Since infrastructure version 5.5.3 HSTS can be configured per domain . How to disable HSTS (Google Chrome) In case you no longer use HTTPS for your app, but your Chrome browser keeps redirecting you to the HTTPS version you should delete this domain from the HSTS set from the page chrome://net-internals/#hsts .","title":"HSTS"},{"location":"infrastructure/hsts/#hsts","text":"HSTS (HTTP Strict Transport Security) is a security feature that lets a web site tell browsers that it should only be communicated with using HTTPS, instead of using HTTP. Read more details on Mozilla Developer Network . HSTS specification was subsequently developed to combat SSL stripping attacks Applications deployed via AnaxExp has enabled HSTS by default (header Strict-Transport-Security max-age=31536000 ). Since infrastructure version 5.5.3 HSTS can be configured per domain . How to disable HSTS (Google Chrome) In case you no longer use HTTPS for your app, but your Chrome browser keeps redirecting you to the HTTPS version you should delete this domain from the HSTS set from the page chrome://net-internals/#hsts .","title":"HSTS"},{"location":"infrastructure/mail-delivery/","text":"Mail delivery Most of managed stack have a mail transfer agent OpenSMTPD (can be deployed as a stand-alone app) as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from [Instance] Stack Settings page. WARNING If you're using a server from a public cloud there's 90% chance that its IP is already compromised and blacklisted by major mail services, hence your emails won't be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a 3 rd party SMTP server. See OpenSMTPD stack description to learn how.","title":"Mail delivery"},{"location":"infrastructure/mail-delivery/#mail-delivery","text":"Most of managed stack have a mail transfer agent OpenSMTPD (can be deployed as a stand-alone app) as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from [Instance] Stack Settings page. WARNING If you're using a server from a public cloud there's 90% chance that its IP is already compromised and blacklisted by major mail services, hence your emails won't be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a 3 rd party SMTP server. See OpenSMTPD stack description to learn how.","title":"Mail delivery"},{"location":"infrastructure/maintenance/","text":"Infrastructure maintenance We constantly improve the infrastructure we deploy to our customers' servers. You can see the version of the infrastructure deployed to your server in the Dashboard on the servers list page. It's not always possible to update the infrastructure automatically so if you want update your infrastructure please contact our support team to schedule the upgrade. Supported version Currently supported infrastructure version is 5.5.0+ You will be notified each time a new version of the infrastructure is released. Stacks and infrastructure maintained separately For stacks maintenance see this article Changelog 5.5.5 Bugfix (5.5.4): installer did not report changes to IP address 5.5.4 Updates to installer, you don't need to update from the previous version 5.5.3 HSTS can now be configured per domain 5.5.2 Security vulnerability fix for dnsmasq CVE-2017-14491 5.5.1 Increased kernel param aio-max-nr for databases Updated paths for systemd unit files Uninstall fixes 5.5.0 New Docker 17.06.1 Improved response codes on Edge 5.4.1 Updated Let's Encrypt client 5.4.0 AUFS replaced with Overlay2 Identified a bug in Debian Kernel 3.16, required upgrade to 4.9 Fixed a bug with kube-controller service definition that sometime caused deployment failures 5.3.0 New kubernetes version Improved agent installer Enabled unattended upgrades Significantly improved performance (less load on CPU and disk IO) 5.2.0 Decoupled services for system containers Improved agent installer Bug fixes 5.1.0 Revamped DNS services Improved agent installer Bug fixes 5.0.0 New kubernetes New installer with frozen docker version Revamped anaxexp agent 4.x First version of cluster infrastructure 3.5.0 Updated Nginx (1.10.1) for a system container Edge New version of AnaxExp agent supporting containers upgrade New version of orchestration system Release date: July 1st, 2016 3.4.0 includeSubdomains option removed from HSTS header Now X-Robots-Tag header added always (not only for 20x, 30x response codes) New version of AnaxExp agent. Now with automated infrastructure update (will be announced later) Exif extension added to PHP 5.6, 7 Fixed bug when X-AnaxExp-Node header missed sometimes Added default nginx host for port 443 with self-signed certificates Release date: June 16th, 2016 3.3.0 $base_url orchestration for Drupal Auto generation of trusted host patterns for Drupal 8 Drupal 7.x, 8.x multi-site support Drupal 8 and WordPress now come with PHP7 and Redis msmtp + opensmtpd replaced with postfix Workaround for Drupal sites/default auto permissions change. This caused problems when settings.php file was changed 3.2.0 WWW redirect actions for domains Basic auth configuration Maintenance mode Dev, staging instances and all instances accessible by technical *.wod.by domains not indexed by search engines (header X-Robots-Tag) 3.1.0 Enable HTTPS for domains (SSL certificates via Let's Encrypt) Backup mirroring features added. 3.0.0 The latest stable version with completely reworked containers structure. 2.0.0 In this version we've moved git to the docroot and made major structural changes. 1.0.0 Production-ready version with lots of improvements. 0.1.0 The first public version of our infrastructure.","title":"Maintenance"},{"location":"infrastructure/maintenance/#infrastructure-maintenance","text":"We constantly improve the infrastructure we deploy to our customers' servers. You can see the version of the infrastructure deployed to your server in the Dashboard on the servers list page. It's not always possible to update the infrastructure automatically so if you want update your infrastructure please contact our support team to schedule the upgrade. Supported version Currently supported infrastructure version is 5.5.0+ You will be notified each time a new version of the infrastructure is released. Stacks and infrastructure maintained separately For stacks maintenance see this article","title":"Infrastructure maintenance"},{"location":"infrastructure/maintenance/#changelog","text":"","title":"Changelog"},{"location":"infrastructure/maintenance/#555","text":"Bugfix (5.5.4): installer did not report changes to IP address","title":"5.5.5"},{"location":"infrastructure/maintenance/#554","text":"Updates to installer, you don't need to update from the previous version","title":"5.5.4"},{"location":"infrastructure/maintenance/#553","text":"HSTS can now be configured per domain","title":"5.5.3"},{"location":"infrastructure/maintenance/#552","text":"Security vulnerability fix for dnsmasq CVE-2017-14491","title":"5.5.2"},{"location":"infrastructure/maintenance/#551","text":"Increased kernel param aio-max-nr for databases Updated paths for systemd unit files Uninstall fixes","title":"5.5.1"},{"location":"infrastructure/maintenance/#550","text":"New Docker 17.06.1 Improved response codes on Edge","title":"5.5.0"},{"location":"infrastructure/maintenance/#541","text":"Updated Let's Encrypt client","title":"5.4.1"},{"location":"infrastructure/maintenance/#540","text":"AUFS replaced with Overlay2 Identified a bug in Debian Kernel 3.16, required upgrade to 4.9 Fixed a bug with kube-controller service definition that sometime caused deployment failures","title":"5.4.0"},{"location":"infrastructure/maintenance/#530","text":"New kubernetes version Improved agent installer Enabled unattended upgrades Significantly improved performance (less load on CPU and disk IO)","title":"5.3.0"},{"location":"infrastructure/maintenance/#520","text":"Decoupled services for system containers Improved agent installer Bug fixes","title":"5.2.0"},{"location":"infrastructure/maintenance/#510","text":"Revamped DNS services Improved agent installer Bug fixes","title":"5.1.0"},{"location":"infrastructure/maintenance/#500","text":"New kubernetes New installer with frozen docker version Revamped anaxexp agent","title":"5.0.0"},{"location":"infrastructure/maintenance/#4x","text":"First version of cluster infrastructure","title":"4.x"},{"location":"infrastructure/maintenance/#350","text":"Updated Nginx (1.10.1) for a system container Edge New version of AnaxExp agent supporting containers upgrade New version of orchestration system Release date: July 1st, 2016","title":"3.5.0"},{"location":"infrastructure/maintenance/#340","text":"includeSubdomains option removed from HSTS header Now X-Robots-Tag header added always (not only for 20x, 30x response codes) New version of AnaxExp agent. Now with automated infrastructure update (will be announced later) Exif extension added to PHP 5.6, 7 Fixed bug when X-AnaxExp-Node header missed sometimes Added default nginx host for port 443 with self-signed certificates Release date: June 16th, 2016","title":"3.4.0"},{"location":"infrastructure/maintenance/#330","text":"$base_url orchestration for Drupal Auto generation of trusted host patterns for Drupal 8 Drupal 7.x, 8.x multi-site support Drupal 8 and WordPress now come with PHP7 and Redis msmtp + opensmtpd replaced with postfix Workaround for Drupal sites/default auto permissions change. This caused problems when settings.php file was changed","title":"3.3.0"},{"location":"infrastructure/maintenance/#320","text":"WWW redirect actions for domains Basic auth configuration Maintenance mode Dev, staging instances and all instances accessible by technical *.wod.by domains not indexed by search engines (header X-Robots-Tag)","title":"3.2.0"},{"location":"infrastructure/maintenance/#310","text":"Enable HTTPS for domains (SSL certificates via Let's Encrypt) Backup mirroring features added.","title":"3.1.0"},{"location":"infrastructure/maintenance/#300","text":"The latest stable version with completely reworked containers structure.","title":"3.0.0"},{"location":"infrastructure/maintenance/#200","text":"In this version we've moved git to the docroot and made major structural changes.","title":"2.0.0"},{"location":"infrastructure/maintenance/#100","text":"Production-ready version with lots of improvements.","title":"1.0.0"},{"location":"infrastructure/maintenance/#010","text":"The first public version of our infrastructure.","title":"0.1.0"},{"location":"infrastructure/monitoring/","text":"Monitoring Single-server infrastructure monitoring As of infrastructure 5.x AnaxExp does not provide monitoring tools for your servers, we recommend connecting your server manually to NewRelic Servers or NewRelic Infrastructure (or other 3 rd party monitoring tool of your choice) to track essentials metrics like CPU / RAM / Disk consumption. Nothing specific required during the installation, just follow the official guide. Containers monitoring Additionally, if you want to see resources consumption per container or per application, we recommend using ctop . Installation: sudo wget https://github.com/bcicen/ctop/releases/download/v0.6.1/ctop-0.6.1-linux-amd64 -O /usr/local/bin/ctop sudo chmod +x /usr/local/bin/ctop Show containers of a specific application instance: ctop -f [ INSTANCE UUID ] Press s to sort containers by CPU / RAM consumption. You can identify which application a container belongs by copying a container ID (CID column) and executing: docker exec [ CONTAINER_ID ] sh -c echo $ANAXEXP_APP_NAME","title":"Monitoring"},{"location":"infrastructure/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"infrastructure/monitoring/#single-server-infrastructure-monitoring","text":"As of infrastructure 5.x AnaxExp does not provide monitoring tools for your servers, we recommend connecting your server manually to NewRelic Servers or NewRelic Infrastructure (or other 3 rd party monitoring tool of your choice) to track essentials metrics like CPU / RAM / Disk consumption. Nothing specific required during the installation, just follow the official guide.","title":"Single-server infrastructure monitoring"},{"location":"infrastructure/monitoring/#containers-monitoring","text":"Additionally, if you want to see resources consumption per container or per application, we recommend using ctop . Installation: sudo wget https://github.com/bcicen/ctop/releases/download/v0.6.1/ctop-0.6.1-linux-amd64 -O /usr/local/bin/ctop sudo chmod +x /usr/local/bin/ctop Show containers of a specific application instance: ctop -f [ INSTANCE UUID ] Press s to sort containers by CPU / RAM consumption. You can identify which application a container belongs by copying a container ID (CID column) and executing: docker exec [ CONTAINER_ID ] sh -c echo $ANAXEXP_APP_NAME","title":"Containers monitoring"},{"location":"infrastructure/security/","text":"Security When you connect a server to AnaxExp we enable automatic security updates for the system. Significant updates to core Linux components (such as Linux kernel) should be performed manually by you or happen during the upgrade of infrastructure . We usually are very conservative when it comes to system updates because for us stability is most important. Infrastructure maintenance Stacks maintenance We release unplanned updates to our infrastructure for all critical security updates and notify all affected customers by email (you cannot unsubscribe from those emails).","title":"Security"},{"location":"infrastructure/security/#security","text":"When you connect a server to AnaxExp we enable automatic security updates for the system. Significant updates to core Linux components (such as Linux kernel) should be performed manually by you or happen during the upgrade of infrastructure . We usually are very conservative when it comes to system updates because for us stability is most important. Infrastructure maintenance Stacks maintenance We release unplanned updates to our infrastructure for all critical security updates and notify all affected customers by email (you cannot unsubscribe from those emails).","title":"Security"},{"location":"infrastructure/ufw/","text":"UFW configuration If you want to use ufw (uncomplicated firewall) on a server connected to AnaxExp, you\u2019ll need to do additional configuration. Docker uses a bridge to manage container networking. By default, UFW drops all forwarding traffic. You must set UFW\u2019s forwarding policy appropriately. Steps: Edit the UFW configuration file, which is usually /etc/default/ufw or /etc/sysconfig/ufw. Set the DEFAULT_FORWARD_POLICY policy to ACCEPT. DEFAULT_FORWARD_POLICY= ACCEPT Add the following rules: $ ufw allow ssh $ ufw allow http $ ufw allow https $ ufw allow 31222 :32222/tcp $ ufw allow 4001 /tcp $ ufw allow 6443 /tcp Enable UFW: $ ufw enable","title":"UFW"},{"location":"infrastructure/ufw/#ufw-configuration","text":"If you want to use ufw (uncomplicated firewall) on a server connected to AnaxExp, you\u2019ll need to do additional configuration. Docker uses a bridge to manage container networking. By default, UFW drops all forwarding traffic. You must set UFW\u2019s forwarding policy appropriately. Steps: Edit the UFW configuration file, which is usually /etc/default/ufw or /etc/sysconfig/ufw. Set the DEFAULT_FORWARD_POLICY policy to ACCEPT. DEFAULT_FORWARD_POLICY= ACCEPT Add the following rules: $ ufw allow ssh $ ufw allow http $ ufw allow https $ ufw allow 31222 :32222/tcp $ ufw allow 4001 /tcp $ ufw allow 6443 /tcp Enable UFW: $ ufw enable","title":"UFW configuration"},{"location":"integrations/aws/","text":"AWS Connecting EC2 node Currently, we don't have a native integration with AWS. But you still can connect your server by following these steps: Learn requirements and recommendations Login to your AWS Console Choose EC2 (Virtual Servers in the Cloud) Choose Instances Click Launch Instance Choose an image (AMI) from the list of supported OS Choose an instance type. Do not use t2 (burstable performance) instances unless you know what you're doing Go to 4. Add Storage (vertical tabs) and set Size (GiB) to at least 20 Go to 6. Configure Security Group . Besides the default SSH rule (port 22), add the following rules: HTTP, HTTPS and custom TCP rule with the range 31222-32222 as shown below: Click Review and Launch Now it's all set on AWS side. Now connect the server ( Servers Connect AWS ) and follow the instructions Sending emails via Simple Email Service (SES) You can send emails from your applications via SES by configuring mail transfer agent OpenSMTPD: Make sure your application stack has enabled OpenSMTPD service Verify your domain or email address you will use to send emails from. AWS will not send emails from unverified sender Obtain your SMTP credentials Obtain SMTP Server Name from your AWS console. Open \"SES Email Sending SMTP Setting\" Add the following environment variables to OpenSMTPD service (replace [Tokens] with your values): RELAY_HOST=[Server Name] RELAY_USER=[SMTP Username] RELAY_PASSWORD=[Password] Send a test email from OpenSMTPD container Backups mirroring to Simple Safe Storage (S3) For stacks providing backups orchestration you can set up backups mirroring to your own AWS S3 bucket. Enable backups mirroring from Instance Backups Mirroing page and specify AWS credentials: AWS Access Key Id AWS Secret Access Key Bucket name Bucket region","title":"AWS"},{"location":"integrations/aws/#aws","text":"","title":"AWS"},{"location":"integrations/aws/#connecting-ec2-node","text":"Currently, we don't have a native integration with AWS. But you still can connect your server by following these steps: Learn requirements and recommendations Login to your AWS Console Choose EC2 (Virtual Servers in the Cloud) Choose Instances Click Launch Instance Choose an image (AMI) from the list of supported OS Choose an instance type. Do not use t2 (burstable performance) instances unless you know what you're doing Go to 4. Add Storage (vertical tabs) and set Size (GiB) to at least 20 Go to 6. Configure Security Group . Besides the default SSH rule (port 22), add the following rules: HTTP, HTTPS and custom TCP rule with the range 31222-32222 as shown below: Click Review and Launch Now it's all set on AWS side. Now connect the server ( Servers Connect AWS ) and follow the instructions","title":"Connecting EC2 node"},{"location":"integrations/aws/#sending-emails-via-simple-email-service-ses","text":"You can send emails from your applications via SES by configuring mail transfer agent OpenSMTPD: Make sure your application stack has enabled OpenSMTPD service Verify your domain or email address you will use to send emails from. AWS will not send emails from unverified sender Obtain your SMTP credentials Obtain SMTP Server Name from your AWS console. Open \"SES Email Sending SMTP Setting\" Add the following environment variables to OpenSMTPD service (replace [Tokens] with your values): RELAY_HOST=[Server Name] RELAY_USER=[SMTP Username] RELAY_PASSWORD=[Password] Send a test email from OpenSMTPD container","title":"Sending emails via Simple Email Service (SES)"},{"location":"integrations/aws/#backups-mirroring-to-simple-safe-storage-s3","text":"For stacks providing backups orchestration you can set up backups mirroring to your own AWS S3 bucket. Enable backups mirroring from Instance Backups Mirroing page and specify AWS credentials: AWS Access Key Id AWS Secret Access Key Bucket name Bucket region","title":"Backups mirroring to Simple Safe Storage (S3)"},{"location":"integrations/azure/","text":"Azure Connecting virtual machine Currently, we don't have a native integration with Azure. But you still can connect your server by following these steps: Learn requirements and recommendations Login to Azure portal with your account Click Virtual Machines Add Select OS from the list of supported OS Enter the host name and the user name for your virtual machine. You will use this username later to access your server by SSH Add your public key or the password to access later by SSH Choose your pricing tier On the step 3 choose Network security groups and create a new group Besides the default SSH rule (port 22), add 3 additional rules for the following ports: 80 (HTTP), 443 (HTTPS), 31222-32222 (containers ports) as shown below: Click create. Azure will spin up your new virtual machine Optional but recommended: attach data disk as described here . Prepare directories to use the data disk: # If your data disk mounted to /mnt/data1 mkdir -p /mnt/data1/anaxexp mkdir -p /mnt/data1/docker ln -s /mnt/data1/anaxexp /srv/anaxexp ln -s /mnt/data1/docker /var/lib/docker Now it's all set on Azure's side. Now connect the server ( Servers Connect Microsoft Azure ) and follow the instructions","title":"Azure"},{"location":"integrations/azure/#azure","text":"","title":"Azure"},{"location":"integrations/azure/#connecting-virtual-machine","text":"Currently, we don't have a native integration with Azure. But you still can connect your server by following these steps: Learn requirements and recommendations Login to Azure portal with your account Click Virtual Machines Add Select OS from the list of supported OS Enter the host name and the user name for your virtual machine. You will use this username later to access your server by SSH Add your public key or the password to access later by SSH Choose your pricing tier On the step 3 choose Network security groups and create a new group Besides the default SSH rule (port 22), add 3 additional rules for the following ports: 80 (HTTP), 443 (HTTPS), 31222-32222 (containers ports) as shown below: Click create. Azure will spin up your new virtual machine Optional but recommended: attach data disk as described here . Prepare directories to use the data disk: # If your data disk mounted to /mnt/data1 mkdir -p /mnt/data1/anaxexp mkdir -p /mnt/data1/docker ln -s /mnt/data1/anaxexp /srv/anaxexp ln -s /mnt/data1/docker /var/lib/docker Now it's all set on Azure's side. Now connect the server ( Servers Connect Microsoft Azure ) and follow the instructions","title":"Connecting virtual machine"},{"location":"integrations/bitbucket/","text":"BitBucket integration Connecting repository Connect your BitBucket account by adding a new integration Navigate My repos Connect , select BitBucket and your integration Click Proceed . Now select the repository you'd like to connect Configuring git hooks for auto-deployment Navigate to your git repo page in AnaxExp Dashboard and copy the Webhook URL Open your repository page on BitBucket. Navigate to Settings Webhooks . Click Add webhook button Enter webhook title, paste the URL you copied before to URL input and Save the form","title":"BitBucket"},{"location":"integrations/bitbucket/#bitbucket-integration","text":"","title":"BitBucket integration"},{"location":"integrations/bitbucket/#connecting-repository","text":"Connect your BitBucket account by adding a new integration Navigate My repos Connect , select BitBucket and your integration Click Proceed . Now select the repository you'd like to connect","title":"Connecting repository"},{"location":"integrations/bitbucket/#configuring-git-hooks-for-auto-deployment","text":"Navigate to your git repo page in AnaxExp Dashboard and copy the Webhook URL Open your repository page on BitBucket. Navigate to Settings Webhooks . Click Add webhook button Enter webhook title, paste the URL you copied before to URL input and Save the form","title":"Configuring git hooks for auto-deployment"},{"location":"integrations/cloudflare/","text":"CloudFlare integration HTTPS If you're using SSL with CloudFlare there're 2 ways how it integrate it with AnaxExp. Simple way (less secure) Open your CloudFlare dashboard and navigate to the Crypto tab of your domain Go to block SSL and choose Flexible mode All traffic before CloudFlare will be secured, however the traffic between CloudFlare and AnaxExp will be unsecured. Secure way Open your CloudFlare dashboard and navigate to the Crypto tab of your domain Go to block SSL and choose Full (strict) mode Open AnaxExp dashboard and navigate to the Domains tab of your instance Click Get certificate for your domain and choose Let's Encrypt a as provider Two certificates will be used: the first, on CloudFlare side, to encrypt traffic before CloudFlare and the second (Let's Encrypt) to secure traffic between CloudFlare and AnaxExp.","title":"CloudFlare"},{"location":"integrations/cloudflare/#cloudflare-integration","text":"","title":"CloudFlare integration"},{"location":"integrations/cloudflare/#https","text":"If you're using SSL with CloudFlare there're 2 ways how it integrate it with AnaxExp.","title":"HTTPS"},{"location":"integrations/cloudflare/#simple-way-less-secure","text":"Open your CloudFlare dashboard and navigate to the Crypto tab of your domain Go to block SSL and choose Flexible mode All traffic before CloudFlare will be secured, however the traffic between CloudFlare and AnaxExp will be unsecured.","title":"Simple way (less secure)"},{"location":"integrations/cloudflare/#secure-way","text":"Open your CloudFlare dashboard and navigate to the Crypto tab of your domain Go to block SSL and choose Full (strict) mode Open AnaxExp dashboard and navigate to the Domains tab of your instance Click Get certificate for your domain and choose Let's Encrypt a as provider Two certificates will be used: the first, on CloudFlare side, to encrypt traffic before CloudFlare and the second (Let's Encrypt) to secure traffic between CloudFlare and AnaxExp.","title":"Secure way"},{"location":"integrations/custom/","text":"Custom provider Connecting custom git repository Navigate to My repos Connect and select Custom git provider Enter title and URL to your git repository. Click Proceed Copy the public key generated by AnaxExp to ~/.ssh/authorized_keys on a server with your git. Add git hook for auto-deployment Navigate to your git repo page in AnaxExp Dashboard and copy the auto-deployment Webhook URL Connect to the server with your origin git repository Go to .git/hooks directory and create a new file post-receive.sh with the following content (replace WEBHOOK URL in this file with the URL you copied before) #!/bin/sh while read oldrev newrev ref do curl -H Content-type: application/json -X POST -d { ref : $ref } WEBHOOK URL done Now make this file executable chmod +X post-receive.sh Connecting custom server In case there's no native integration for your hosting provider, you can always connect your node (list of supported OS by using feature \"Custom server\" regardless what hosting you're using: Learn requirements and recommendations Go to My servers Add . Select Custom server and click proceed You will see the page with requirements and instructions. Make sure all requirements are met Connect to your server by SSH and execute the agent installation command as a root. This command will download our agent which sets up the infrastructure and connects your server to our platform Now you can deploy apps to this server","title":"Custom provider"},{"location":"integrations/custom/#custom-provider","text":"","title":"Custom provider"},{"location":"integrations/custom/#connecting-custom-git-repository","text":"Navigate to My repos Connect and select Custom git provider Enter title and URL to your git repository. Click Proceed Copy the public key generated by AnaxExp to ~/.ssh/authorized_keys on a server with your git.","title":"Connecting custom git repository"},{"location":"integrations/custom/#add-git-hook-for-auto-deployment","text":"Navigate to your git repo page in AnaxExp Dashboard and copy the auto-deployment Webhook URL Connect to the server with your origin git repository Go to .git/hooks directory and create a new file post-receive.sh with the following content (replace WEBHOOK URL in this file with the URL you copied before) #!/bin/sh while read oldrev newrev ref do curl -H Content-type: application/json -X POST -d { ref : $ref } WEBHOOK URL done Now make this file executable chmod +X post-receive.sh","title":"Add git hook for auto-deployment"},{"location":"integrations/custom/#connecting-custom-server","text":"In case there's no native integration for your hosting provider, you can always connect your node (list of supported OS by using feature \"Custom server\" regardless what hosting you're using: Learn requirements and recommendations Go to My servers Add . Select Custom server and click proceed You will see the page with requirements and instructions. Make sure all requirements are met Connect to your server by SSH and execute the agent installation command as a root. This command will download our agent which sets up the infrastructure and connects your server to our platform Now you can deploy apps to this server","title":"Connecting custom server"},{"location":"integrations/digitalocean/","text":"DigitalOcean Connecting droplet AnaxExp provides a native integration with DigitalOcean: Connect your DigitalOcean account via AnaxExp dashboard ( Integrations ) Navigate to Servers Connect , select DigitalOcean from the list of providers and your integration Click proceed, choose droplet region, size and click Create Droplet AnaxExp will spin a new droplet and automatically install all required infrastructure Accessing droplet You will be able to access newly created droplet using SSH keys from your DigitalOcean account, if you don't have any, we will add SSH keys from your AnaxExp profile, and if you neither have those, you will receive an email with root credentials from DigitalOcean","title":"DigitalOcean"},{"location":"integrations/digitalocean/#digitalocean","text":"","title":"DigitalOcean"},{"location":"integrations/digitalocean/#connecting-droplet","text":"AnaxExp provides a native integration with DigitalOcean: Connect your DigitalOcean account via AnaxExp dashboard ( Integrations ) Navigate to Servers Connect , select DigitalOcean from the list of providers and your integration Click proceed, choose droplet region, size and click Create Droplet AnaxExp will spin a new droplet and automatically install all required infrastructure","title":"Connecting droplet"},{"location":"integrations/digitalocean/#accessing-droplet","text":"You will be able to access newly created droplet using SSH keys from your DigitalOcean account, if you don't have any, we will add SSH keys from your AnaxExp profile, and if you neither have those, you will receive an email with root credentials from DigitalOcean","title":"Accessing droplet"},{"location":"integrations/docker-registry/","text":"Docker registry integration If you store docker images from your stack in a private docker registry, you can add this integration to provide credentials. We will automatically add a secret token from an appropriate registry provider when you pull private images.","title":"Docker registry"},{"location":"integrations/docker-registry/#docker-registry-integration","text":"If you store docker images from your stack in a private docker registry, you can add this integration to provide credentials. We will automatically add a secret token from an appropriate registry provider when you pull private images.","title":"Docker registry integration"},{"location":"integrations/gcp/","text":"Google Cloud Platform Connecting GCE virtual machine Currently, we do not have a native integration with GCP. But you still can connect your server by following these steps: Learn requirements and recommendations Log in to GCP console with your account Create a new project if you don't have any Navigate to the project page and click Get started under Try Compute Engine block Once GCP prepare Compute Engine for your project, create a new virtual machine instance Set VM instance attributes (list of supported OS and submit Create button Navigate to a newly created instance page and find network link. Click on it We need to add a new firewall rule for container ports Allow access for ports range 31222-32222 Now it's all set on GCP side. Now connect the server ( Servers Connect GCP ) and follow the instructions","title":"GCP"},{"location":"integrations/gcp/#google-cloud-platform","text":"","title":"Google Cloud Platform"},{"location":"integrations/gcp/#connecting-gce-virtual-machine","text":"Currently, we do not have a native integration with GCP. But you still can connect your server by following these steps: Learn requirements and recommendations Log in to GCP console with your account Create a new project if you don't have any Navigate to the project page and click Get started under Try Compute Engine block Once GCP prepare Compute Engine for your project, create a new virtual machine instance Set VM instance attributes (list of supported OS and submit Create button Navigate to a newly created instance page and find network link. Click on it We need to add a new firewall rule for container ports Allow access for ports range 31222-32222 Now it's all set on GCP side. Now connect the server ( Servers Connect GCP ) and follow the instructions","title":"Connecting GCE virtual machine"},{"location":"integrations/github/","text":"GitHub integration Connecting repository Connect your GitHub account by adding a new integration Navigate My repos Connect , select GitHub and your integration Click Proceed . Now select the repository you'd like to connect Add git hook for auto-deployment Navigate to your git repository page in AnaxExp Dashboard and copy the auto-deployment Webhook URL Open your repo page on GitHub. Navigate to Settings Webhooks services . Click Add webhook button Paste the URL you copied before to Payload URL input and submit the form","title":"GitHub"},{"location":"integrations/github/#github-integration","text":"","title":"GitHub integration"},{"location":"integrations/github/#connecting-repository","text":"Connect your GitHub account by adding a new integration Navigate My repos Connect , select GitHub and your integration Click Proceed . Now select the repository you'd like to connect","title":"Connecting repository"},{"location":"integrations/github/#add-git-hook-for-auto-deployment","text":"Navigate to your git repository page in AnaxExp Dashboard and copy the auto-deployment Webhook URL Open your repo page on GitHub. Navigate to Settings Webhooks services . Click Add webhook button Paste the URL you copied before to Payload URL input and submit the form","title":"Add git hook for auto-deployment"},{"location":"integrations/gitlab/","text":"GitLab integration Connecting repository Go to your repository page on GitLab. Choose SSH type of the clone URL and copy it \u200b* Navigate My repos Connect and paste the URL \u200b* Click Connect the repository. Copy the deployment key generated by AnaxExp. \u200b Configuring git hooks for auto-deployment Navigate to your git repository page in AnaxExp Dashboard and copy Webhook URL Open your repository page on GitLab. Navigate to Settings Web Hooks Paste the URL you copied before to URL input and submit the form","title":"GitLab"},{"location":"integrations/gitlab/#gitlab-integration","text":"","title":"GitLab integration"},{"location":"integrations/gitlab/#connecting-repository","text":"Go to your repository page on GitLab. Choose SSH type of the clone URL and copy it \u200b* Navigate My repos Connect and paste the URL \u200b* Click Connect the repository. Copy the deployment key generated by AnaxExp. \u200b","title":"Connecting repository"},{"location":"integrations/gitlab/#configuring-git-hooks-for-auto-deployment","text":"Navigate to your git repository page in AnaxExp Dashboard and copy Webhook URL Open your repository page on GitLab. Navigate to Settings Web Hooks Paste the URL you copied before to URL input and submit the form","title":"Configuring git hooks for auto-deployment"},{"location":"integrations/linode/","text":"Linode Connecting Linode server Currently, we don't have a native integration with Linode. But you still can connect your server by following these steps: Learn requirements and recommendations Login to Linode manager Spin up a new linode Wait for linode being created. Once its status is set to \"Brand New\". Navigate to this linode dashboard Click \"Deploy an image\" By default Linode uses a custom Linux Kernel version, we should replace it to the Kernel version that comes with the Linux distribution. Click Edit to view a distribution\u2019s configuration profile options as shown below and change Kernel to GRUB2 : Choose image attributes (list of supported OS and deploy Wait until all disks are create and boot the linode Now it's all set on Linode's side. Now connect the server ( Servers Connect Linode ) and follow the instructions","title":"Linode"},{"location":"integrations/linode/#linode","text":"","title":"Linode"},{"location":"integrations/linode/#connecting-linode-server","text":"Currently, we don't have a native integration with Linode. But you still can connect your server by following these steps: Learn requirements and recommendations Login to Linode manager Spin up a new linode Wait for linode being created. Once its status is set to \"Brand New\". Navigate to this linode dashboard Click \"Deploy an image\" By default Linode uses a custom Linux Kernel version, we should replace it to the Kernel version that comes with the Linux distribution. Click Edit to view a distribution\u2019s configuration profile options as shown below and change Kernel to GRUB2 : Choose image attributes (list of supported OS and deploy Wait until all disks are create and boot the linode Now it's all set on Linode's side. Now connect the server ( Servers Connect Linode ) and follow the instructions","title":"Connecting Linode server"},{"location":"integrations/sendgrid/","text":"SendGrid integration You can send emails from your applications via SendGrid by configuring mail transfer agent OpenSMTPD: Make sure your application stack has enabled OpenSMTPD service Log in to your SendGrid account and visit guide https://app.sendgrid.com/guide Start with Integrate using our Web API or SMTP relay option Choose SMTP Relay Enter API Key to generate password Add the following environment variables to OpenSMTPD service (replace [Tokens] with your values): RELAY_HOST=[Server] RELAY_USER=[Username] RELAY_PASSWORD=[Password] Proceed to the next step in SendGrid dashboard to verify integration Send a test email from OpenSMTPD container","title":"SendGrid"},{"location":"integrations/sendgrid/#sendgrid-integration","text":"You can send emails from your applications via SendGrid by configuring mail transfer agent OpenSMTPD: Make sure your application stack has enabled OpenSMTPD service Log in to your SendGrid account and visit guide https://app.sendgrid.com/guide Start with Integrate using our Web API or SMTP relay option Choose SMTP Relay Enter API Key to generate password Add the following environment variables to OpenSMTPD service (replace [Tokens] with your values): RELAY_HOST=[Server] RELAY_USER=[Username] RELAY_PASSWORD=[Password] Proceed to the next step in SendGrid dashboard to verify integration Send a test email from OpenSMTPD container","title":"SendGrid integration"},{"location":"stacks/","text":"Stack Overview Stack is a pre-configured infrastructure package for your application. Every stack consist of services, they can be mandatory or optional. Every service has at least one container implementation, e.g. MariaDB or PostgreSQL containers for a database management service. You can find all stacks offered by AnaxExp at https://anaxexp.com/stacks Redeployment Redeployment operation will reload infrastructure of an application's scheme on a server. It is useful if you want to apply updated configuration of multiple services at once. Also, it may be useful for troubleshooting. During redeployment the entire scheme of an application's infrastructure will be reloaded. However, only services with changes in their configuration will be restarted. For example, if you've updated a container image but did not change image's tag it will not be restarted (for this case we have an API method to force redeployment by image name).","title":"Overview"},{"location":"stacks/#stack","text":"","title":"Stack"},{"location":"stacks/#overview","text":"Stack is a pre-configured infrastructure package for your application. Every stack consist of services, they can be mandatory or optional. Every service has at least one container implementation, e.g. MariaDB or PostgreSQL containers for a database management service. You can find all stacks offered by AnaxExp at https://anaxexp.com/stacks","title":"Overview"},{"location":"stacks/#redeployment","text":"Redeployment operation will reload infrastructure of an application's scheme on a server. It is useful if you want to apply updated configuration of multiple services at once. Also, it may be useful for troubleshooting. During redeployment the entire scheme of an application's infrastructure will be reloaded. However, only services with changes in their configuration will be restarted. For example, if you've updated a container image but did not change image's tag it will not be restarted (for this case we have an API method to force redeployment by image name).","title":"Redeployment"},{"location":"stacks/config/","text":"Stack configuration You can customize stacks for specific application instance by configuring stack services via environment variables. Go to Instance Stack , find a service you'd like to customize and click on a cogwheel icon. In a modal window you can edit environment variables for this service, disable it (available only for optional services) and manage # of replicas. To apply changes you must redeploy your stack. Resources You can configure resources (Memory and CPU) requests and limits per container. Request is a minimum amount of RAM (in megabytes) and/or CPU (in cores) that must be available on the server in order to deploy a container. A container with CPU request equal to 0.5 cores is guaranteed half as much CPU as one that asks for 1 CPU. CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine. The amount of available resources on a server is NOT defined by the actual (real) resources usage but calculated as a difference between total server resources and total resources requests of all containers deployed to this server. Limit is the maximum of resources that container allowed to consume. If a container reaches the limit of RAM it will start using swap space (if available), if swapping disabled the container will be terminated. If it is restartable, it will be restarted, as with any other type of runtime failure. CPU limit is the total amount of CPU time that a container can use every 100ms. A container cannot use more than its share of CPU time during this interval, it will not be killed for excessive CPU usage. If you specify a limit without specifying a request, the request will be set equal to the limit. Ports You can publish container ports via two methods: via edge (controlled under Domains tab) and set a dynamic high port (generated port can be found on a service page). Custom Docker Image If changes you need to make can't be done via environment variables there's always an option to use a custom docker image: Build your own docker container image based in ours ( FROM instruction) Fork our stack Replace a service image to your in the template Deploy a new app with your custom stack","title":"Configuration"},{"location":"stacks/config/#stack-configuration","text":"You can customize stacks for specific application instance by configuring stack services via environment variables. Go to Instance Stack , find a service you'd like to customize and click on a cogwheel icon. In a modal window you can edit environment variables for this service, disable it (available only for optional services) and manage # of replicas. To apply changes you must redeploy your stack.","title":"Stack configuration"},{"location":"stacks/config/#resources","text":"You can configure resources (Memory and CPU) requests and limits per container. Request is a minimum amount of RAM (in megabytes) and/or CPU (in cores) that must be available on the server in order to deploy a container. A container with CPU request equal to 0.5 cores is guaranteed half as much CPU as one that asks for 1 CPU. CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine. The amount of available resources on a server is NOT defined by the actual (real) resources usage but calculated as a difference between total server resources and total resources requests of all containers deployed to this server. Limit is the maximum of resources that container allowed to consume. If a container reaches the limit of RAM it will start using swap space (if available), if swapping disabled the container will be terminated. If it is restartable, it will be restarted, as with any other type of runtime failure. CPU limit is the total amount of CPU time that a container can use every 100ms. A container cannot use more than its share of CPU time during this interval, it will not be killed for excessive CPU usage. If you specify a limit without specifying a request, the request will be set equal to the limit.","title":"Resources"},{"location":"stacks/config/#ports","text":"You can publish container ports via two methods: via edge (controlled under Domains tab) and set a dynamic high port (generated port can be found on a service page).","title":"Ports"},{"location":"stacks/config/#custom-docker-image","text":"If changes you need to make can't be done via environment variables there's always an option to use a custom docker image: Build your own docker container image based in ours ( FROM instruction) Fork our stack Replace a service image to your in the template Deploy a new app with your custom stack","title":"Custom Docker Image"},{"location":"stacks/maintenance/","text":"Stacks maintenance We regularly update stacks by releasing new versions, such updates can include security updates and performance improvements. You can find more details under the Changelog tab on a stack page. Stacks and infrastructure maintained separately For infrastructure maintenance see this article The stack has a requirement for the minimal version of infrastructure of the server where this stack is deployed. You can find a stack's version of your application on the list of your apps or under Stack tab of your instance. If a stack is outdated you will see an appropriate indicator. \u200b \u200b \u200b Updating organization stack When a new version of a stack has been released your stack will be marked as outdated. You can update a stack to the latest version from a stack page in your organization. This update will not affect your applications, it only updates the version of a stack inside of your organization. Upgrading application stack Once you update a stack in your organization, you can upgrade the application instances. Go the Instance Stack Operations page and click Upgrade . The downtime depends on the number of services affected in a new version and what deployment strategy used in these services.","title":"Maintenance"},{"location":"stacks/maintenance/#stacks-maintenance","text":"We regularly update stacks by releasing new versions, such updates can include security updates and performance improvements. You can find more details under the Changelog tab on a stack page. Stacks and infrastructure maintained separately For infrastructure maintenance see this article The stack has a requirement for the minimal version of infrastructure of the server where this stack is deployed. You can find a stack's version of your application on the list of your apps or under Stack tab of your instance. If a stack is outdated you will see an appropriate indicator. \u200b \u200b \u200b","title":"Stacks maintenance"},{"location":"stacks/maintenance/#updating-organization-stack","text":"When a new version of a stack has been released your stack will be marked as outdated. You can update a stack to the latest version from a stack page in your organization. This update will not affect your applications, it only updates the version of a stack inside of your organization.","title":"Updating organization stack"},{"location":"stacks/maintenance/#upgrading-application-stack","text":"Once you update a stack in your organization, you can upgrade the application instances. Go the Instance Stack Operations page and click Upgrade . The downtime depends on the number of services affected in a new version and what deployment strategy used in these services.","title":"Upgrading application stack"},{"location":"stacks/template/","text":"Stack Template You can create custom stacks by defining one via template or by forking stacks provided by AnaxExp. Stack template is a YML defining services, it's basically a simplified and limited version of kubernetes schema in a format very close to docker compose. Managed stacks are more functional Stacks provided by AnaxExp (including forks) may have additional configurations not covered by templates. Service Configuration Reference This section contains a list of all configuration options supported by a service definition. Name Description Mandatory Schema image The image the container is running. \u2713 string entrypoint Entrypoint string or array. The docker image\u2019s ENTRYPOINT is used if this is not provided. string, string array command Arguments to the entrypoint. The docker image\u2019s CMD is used if this is not provided. string, string array working_dir Container\u2019s working directory. If not specified, the container runtime\u2019s default will be used, which might be configured in the container image. string environment List of environment variables to set in the container. string array volumes Volumes to mount into the container\u2019s filesystem. string array memory Memory rules and limitations int, string cpu CPU resources rules and limitations int, string ports List of ports to expose from the container. string array check_ready Describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic. string array deployment Deployment enables declarative updates for services. string array security_context Security context holds security configuration that will be applied to a container. string array memory You can specify memory limits and requests for a container in the following formats: 128 - Request 128Mb of memory. 512:1024 - Request 512Mb of memory. Limit 1Gb of memory. Limit is a maximum memory (in megabytes) available for a container. When a container exceeds this limit, it will be terminated (and automatically started again). Request defines how much memory must be available on a server to start this container (used in clusters). cpu You can specify CPU limits and requests for a container in the following formats: 1000 - Request 1 Core. 1500 - Request 1.5 Core. 650 - Request 0.65 Core. 200:250 - Request 0.2 Core. Limit 0.25 Core. Minimum value is 100. Limit is a # of CPU cores (1000 for 1 core) available for a container. When a container exceeds this limit, it will be terminated (and automatically started again). Request defines how many CPU cores must be available on a server to start this container (used in clusters). ports Expose ports for a container in the format [PUBLIC_PORT::BUNDLE_PORT:CONTAINER_PORT] . Examples: Map container's port 8080 to 80 for other containers within a stack. services: backend: image: example/backend ports: - 80:8080 Same as \"8080:8080\" 8080 Map 8080 to a public port within the range 31222-32222. auto::8080 Map 8080 to a public port 80, edge is a reverse proxy handling 80 and 443 ports. A short technical domain *.wod.by will be generated for the first public port exposed via edge. edge::8080 Map 8080 to a public 32223. 32223::8080 Map 8080 to 80 both public and within a stack. edge::80:8080 Map 8080 to 80 within a stack, assign public port automatically. auto::80:8080 Map 8080 to 80 within a stack, assign a specific public port from range 31222-32222 32223::80:8080 check-ready Name Description Mandatory Schema exec Exec specifies the action to take. \u2713 command - array of strings initial_delay_seconds Number of seconds after the container has started before liveness probes are initiated. int period_seconds How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. int failure_threshold Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. int success_threshold Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Minimum value is 1. int timeout_seconds Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. int deployment Name Description Mandatory Schema strategy rolling_update or recreate ( rolling_update by default). Always use recreate strategy for stateful services like database and search engines. string replicas Number of desired containers. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. int min_ready_seconds Minimum number of seconds for which a newly created service should be ready without any of its container crashing, for it to be considered available. Defaults to 0. int progress_deadline_seconds The maximum time in seconds for a deployment to make progress before it is considered to be failed, not set by default. int max_surge The maximum number of containers that can be scheduled above the desired number of containers. Value must be an absolute number. Defaults to 1. int max_unavailable The maximum number of containers that can be unavailable during the update. Value must be an absolute number. Defaults to 1. int type If set to CI the service will be used for CI deployments . CI services excluded from the initial deployment (replicas set to 0) until you deploy your first build to avoid deployment failures. string security_context Name Description Mandatory Schema privileged Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. boolean capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. array of strings read_only_root_filesystem Whether this container has a read-only root filesystem. Default is false. string run_as_non_root Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. string run_as_user The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. int Global Volumes You can define global volumes and use them in services under volumes . We recommend using ./ as a host path to mount volumes which is an equals /srv/anaxexp/instances/ Instance UUID . Example: services: myapp: image: example/myapp ports: - edge::80/tcp volumes: - docroot:/var/www - db:/var/www volumes: docroot: path: ./docroot db: path: ./db Variable Substitution You can define variables and substitute them in services under environment . Example: services: backend: image: example/backend ports: - edge::80/tcp environment: username: %user password: %pass variables: user: admin pass: auto:password:64 Examples services: db: image: mysql environment: MYSQL_ROOT_PASSWORD: %db_password volumes: - ./mysql:/var/lib/mysql deployment: strategy: recreate php: image: php environment: DB_USER: root DB_PASSWORD: %db_password memory: 512:1024 cpu: 900 deployment: type: ci security_context: capabilities: add: - SYS_PTRACE drop: - SYS_ADMIN nginx: image: nginx ports: - edge::80:80 variables: db_password: auto:password:64 Permissions If you mount volumes from the server, the owner of the mounted directory in a container will be root (UID 0). This may cause issues because very often main process run from a different user. To avoid potential problems make sure you're either fixing volumes permissions in your container entrypoint script (recommended) or run the main process as root.","title":"Template"},{"location":"stacks/template/#stack-template","text":"You can create custom stacks by defining one via template or by forking stacks provided by AnaxExp. Stack template is a YML defining services, it's basically a simplified and limited version of kubernetes schema in a format very close to docker compose. Managed stacks are more functional Stacks provided by AnaxExp (including forks) may have additional configurations not covered by templates.","title":"Stack Template"},{"location":"stacks/template/#service-configuration-reference","text":"This section contains a list of all configuration options supported by a service definition. Name Description Mandatory Schema image The image the container is running. \u2713 string entrypoint Entrypoint string or array. The docker image\u2019s ENTRYPOINT is used if this is not provided. string, string array command Arguments to the entrypoint. The docker image\u2019s CMD is used if this is not provided. string, string array working_dir Container\u2019s working directory. If not specified, the container runtime\u2019s default will be used, which might be configured in the container image. string environment List of environment variables to set in the container. string array volumes Volumes to mount into the container\u2019s filesystem. string array memory Memory rules and limitations int, string cpu CPU resources rules and limitations int, string ports List of ports to expose from the container. string array check_ready Describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic. string array deployment Deployment enables declarative updates for services. string array security_context Security context holds security configuration that will be applied to a container. string array","title":"Service Configuration Reference"},{"location":"stacks/template/#memory","text":"You can specify memory limits and requests for a container in the following formats: 128 - Request 128Mb of memory. 512:1024 - Request 512Mb of memory. Limit 1Gb of memory. Limit is a maximum memory (in megabytes) available for a container. When a container exceeds this limit, it will be terminated (and automatically started again). Request defines how much memory must be available on a server to start this container (used in clusters).","title":"memory"},{"location":"stacks/template/#cpu","text":"You can specify CPU limits and requests for a container in the following formats: 1000 - Request 1 Core. 1500 - Request 1.5 Core. 650 - Request 0.65 Core. 200:250 - Request 0.2 Core. Limit 0.25 Core. Minimum value is 100. Limit is a # of CPU cores (1000 for 1 core) available for a container. When a container exceeds this limit, it will be terminated (and automatically started again). Request defines how many CPU cores must be available on a server to start this container (used in clusters).","title":"cpu"},{"location":"stacks/template/#ports","text":"Expose ports for a container in the format [PUBLIC_PORT::BUNDLE_PORT:CONTAINER_PORT] . Examples: Map container's port 8080 to 80 for other containers within a stack. services: backend: image: example/backend ports: - 80:8080 Same as \"8080:8080\" 8080 Map 8080 to a public port within the range 31222-32222. auto::8080 Map 8080 to a public port 80, edge is a reverse proxy handling 80 and 443 ports. A short technical domain *.wod.by will be generated for the first public port exposed via edge. edge::8080 Map 8080 to a public 32223. 32223::8080 Map 8080 to 80 both public and within a stack. edge::80:8080 Map 8080 to 80 within a stack, assign public port automatically. auto::80:8080 Map 8080 to 80 within a stack, assign a specific public port from range 31222-32222 32223::80:8080","title":"ports"},{"location":"stacks/template/#check-ready","text":"Name Description Mandatory Schema exec Exec specifies the action to take. \u2713 command - array of strings initial_delay_seconds Number of seconds after the container has started before liveness probes are initiated. int period_seconds How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. int failure_threshold Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. int success_threshold Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Minimum value is 1. int timeout_seconds Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. int","title":"check-ready"},{"location":"stacks/template/#deployment","text":"Name Description Mandatory Schema strategy rolling_update or recreate ( rolling_update by default). Always use recreate strategy for stateful services like database and search engines. string replicas Number of desired containers. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1. int min_ready_seconds Minimum number of seconds for which a newly created service should be ready without any of its container crashing, for it to be considered available. Defaults to 0. int progress_deadline_seconds The maximum time in seconds for a deployment to make progress before it is considered to be failed, not set by default. int max_surge The maximum number of containers that can be scheduled above the desired number of containers. Value must be an absolute number. Defaults to 1. int max_unavailable The maximum number of containers that can be unavailable during the update. Value must be an absolute number. Defaults to 1. int type If set to CI the service will be used for CI deployments . CI services excluded from the initial deployment (replicas set to 0) until you deploy your first build to avoid deployment failures. string","title":"deployment"},{"location":"stacks/template/#security_context","text":"Name Description Mandatory Schema privileged Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. boolean capabilities The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. array of strings read_only_root_filesystem Whether this container has a read-only root filesystem. Default is false. string run_as_non_root Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. string run_as_user The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. int","title":"security_context"},{"location":"stacks/template/#global-volumes","text":"You can define global volumes and use them in services under volumes . We recommend using ./ as a host path to mount volumes which is an equals /srv/anaxexp/instances/ Instance UUID . Example: services: myapp: image: example/myapp ports: - edge::80/tcp volumes: - docroot:/var/www - db:/var/www volumes: docroot: path: ./docroot db: path: ./db","title":"Global Volumes"},{"location":"stacks/template/#variable-substitution","text":"You can define variables and substitute them in services under environment . Example: services: backend: image: example/backend ports: - edge::80/tcp environment: username: %user password: %pass variables: user: admin pass: auto:password:64","title":"Variable Substitution"},{"location":"stacks/template/#examples","text":"services: db: image: mysql environment: MYSQL_ROOT_PASSWORD: %db_password volumes: - ./mysql:/var/lib/mysql deployment: strategy: recreate php: image: php environment: DB_USER: root DB_PASSWORD: %db_password memory: 512:1024 cpu: 900 deployment: type: ci security_context: capabilities: add: - SYS_PTRACE drop: - SYS_ADMIN nginx: image: nginx ports: - edge::80:80 variables: db_password: auto:password:64","title":"Examples"},{"location":"stacks/template/#permissions","text":"If you mount volumes from the server, the owner of the mounted directory in a container will be root (UID 0). This may cause issues because very often main process run from a different user. To avoid potential problems make sure you're either fixing volumes permissions in your container entrypoint script (recommended) or run the main process as root.","title":"Permissions"},{"location":"stacks/_includes/email-delivery-warning/","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services .","title":"Email delivery warning"},{"location":"stacks/_includes/email-delivery/","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from Application Stack Settings page.","title":"Email delivery"},{"location":"stacks/_includes/php-cicd/","text":"CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: php crond sshd HTTP server: nginx or apache","title":"Php cicd"},{"location":"stacks/_includes/containers/apache/","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Apache"},{"location":"stacks/_includes/containers/blackfire/","text":"You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation","title":"Blackfire"},{"location":"stacks/_includes/containers/mailhog/","text":"If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ).","title":"Mailhog"},{"location":"stacks/_includes/containers/memcached/","text":"You can check the status of memcached and its hits by running the following command. watch echo stats | nc 127.0.0.1 11211","title":"Memcached"},{"location":"stacks/_includes/containers/nginx/","text":"Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Restarting nginx as default user: sudo nginx -s reload","title":"Nginx"},{"location":"stacks/_includes/containers/node/","text":"Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file.","title":"Node"},{"location":"stacks/_includes/containers/php-apache/","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Php apache"},{"location":"stacks/_includes/containers/php-crond/","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page.","title":"Php crond"},{"location":"stacks/_includes/containers/php-dev/","text":"Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug Xdebug (local) Debugging web requests Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1 Debugging CLI requests Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version: Linux, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make ) macOS, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist Windows Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost IDE configuration You must additionally configure your IDE to debug CLI requests. PHPStorm Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings NewRelic You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME . Profiling You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions.","title":"Php dev"},{"location":"stacks/_includes/containers/php-dev/#xdebug-remote","text":"Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug","title":"Xdebug (remote)"},{"location":"stacks/_includes/containers/php-dev/#xdebug-local","text":"","title":"Xdebug (local)"},{"location":"stacks/_includes/containers/php-dev/#debugging-web-requests","text":"Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1","title":"Debugging web requests"},{"location":"stacks/_includes/containers/php-dev/#debugging-cli-requests","text":"Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version:","title":"Debugging CLI requests"},{"location":"stacks/_includes/containers/php-dev/#linux-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make )","title":"Linux, Docker"},{"location":"stacks/_includes/containers/php-dev/#macos-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist","title":"macOS, Docker"},{"location":"stacks/_includes/containers/php-dev/#windows","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost","title":"Windows"},{"location":"stacks/_includes/containers/php-dev/#ide-configuration","text":"You must additionally configure your IDE to debug CLI requests.","title":"IDE configuration"},{"location":"stacks/_includes/containers/php-dev/#phpstorm","text":"Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings","title":"PHPStorm"},{"location":"stacks/_includes/containers/php-dev/#newrelic","text":"You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME .","title":"NewRelic"},{"location":"stacks/_includes/containers/php-dev/#profiling","text":"You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions.","title":"Profiling"},{"location":"stacks/_includes/containers/php-rsyslog/","text":"Rsyslog can be used to stream your applications logs. It's similar to using syslog, however there's no syslog in PHP container (one process per container). Rsyslog will stream all incoming logs to a container output. You can use Monolog with SyslogUdpHandler to stream logs to rsyslog.","title":"Php rsyslog"},{"location":"stacks/_includes/containers/php-sshd/","text":"A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance.","title":"Php sshd"},{"location":"stacks/_includes/containers/php/","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR","title":"Php"},{"location":"stacks/_includes/containers/php/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/_includes/containers/php/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR","title":"Environment variables"},{"location":"stacks/_includes/containers/webgrind/","text":"Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Webgrind"},{"location":"stacks/_includes/local/db-data-persistence/","text":"Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory.","title":"Db data persistence"},{"location":"stacks/_includes/local/db-import-export/","text":"MariaDB See MariaDB stack documentation PostgreSQL See PostgreSQL stack documentation","title":"Db import export"},{"location":"stacks/_includes/local/db-import-export/#mariadb","text":"See MariaDB stack documentation","title":"MariaDB"},{"location":"stacks/_includes/local/db-import-export/#postgresql","text":"See PostgreSQL stack documentation","title":"PostgreSQL"},{"location":"stacks/_includes/local/docker-for-mac/","text":"There two major problems macOS users face with when using Docker for mac: macOS permissions issues To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user. Bind mounts performance Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved. User-guided caching Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\". Docker-sync Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page .","title":"Docker for mac"},{"location":"stacks/_includes/local/docker-for-mac/#macos-permissions-issues","text":"To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user.","title":"macOS permissions issues"},{"location":"stacks/_includes/local/docker-for-mac/#bind-mounts-performance","text":"Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved.","title":"Bind mounts performance"},{"location":"stacks/_includes/local/docker-for-mac/#user-guided-caching","text":"Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\".","title":"User-guided caching"},{"location":"stacks/_includes/local/docker-for-mac/#docker-sync","text":"Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page .","title":"Docker-sync"},{"location":"stacks/_includes/local/make-commands/","text":"We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service","title":"Make commands"},{"location":"stacks/_includes/local/multiple-projects/","text":"Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Multiple projects"},{"location":"stacks/_includes/local/permissions/","text":"You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container. Linux Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions. macOS Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user. Windows Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root Different uid/gid? You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default)","title":"Permissions"},{"location":"stacks/_includes/local/permissions/#linux","text":"Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions.","title":"Linux"},{"location":"stacks/_includes/local/permissions/#macos","text":"Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user.","title":"macOS"},{"location":"stacks/_includes/local/permissions/#windows","text":"Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root","title":"Windows"},{"location":"stacks/_includes/local/permissions/#different-uidgid","text":"You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default)","title":"Different uid/gid?"},{"location":"stacks/athenapdf/","text":"AthenaPDF AthenaPDF is an HTML to PDF converter and drop-in replacement for wkhtmltopdf Full documentation Usage via AnaxExp, port 80 : $ curl http://athenapdf/convert \\? auth \\= anaxexp-athenapdf \\ url \\= http://google.com/ | out.pdf For local environment (port 8080 ): $ curl http://athenapdf:8080/convert \\? auth \\= anaxexp-athenapdf \\ url \\= http://google.com/ | out.pdf Environment variables Environment Variable Default Value Description WEAVER_AUTH_KEY anaxexp-athenapdf WEAVER_ATHENA_CMD athenapdf -S WEAVER_MAX_WORKERS 10 WEAVER_MAX_CONVERSION_QUEUE 50 WEAVER_WORKER_TIMEOUT 90 WEAVER_CONVERSION_FALLBACK false Changelog 1.0.3 Default memory request set to 16m 1.0.2 Version freeze to 2.10.0 1.0.1 AthenaPDF now has public port with technical domain by default 1.0.0 Initial release","title":"AthenaPDF"},{"location":"stacks/athenapdf/#athenapdf","text":"AthenaPDF is an HTML to PDF converter and drop-in replacement for wkhtmltopdf Full documentation","title":"AthenaPDF"},{"location":"stacks/athenapdf/#usage","text":"via AnaxExp, port 80 : $ curl http://athenapdf/convert \\? auth \\= anaxexp-athenapdf \\ url \\= http://google.com/ | out.pdf For local environment (port 8080 ): $ curl http://athenapdf:8080/convert \\? auth \\= anaxexp-athenapdf \\ url \\= http://google.com/ | out.pdf","title":"Usage"},{"location":"stacks/athenapdf/#environment-variables","text":"Environment Variable Default Value Description WEAVER_AUTH_KEY anaxexp-athenapdf WEAVER_ATHENA_CMD athenapdf -S WEAVER_MAX_WORKERS 10 WEAVER_MAX_CONVERSION_QUEUE 50 WEAVER_WORKER_TIMEOUT 90 WEAVER_CONVERSION_FALLBACK false","title":"Environment variables"},{"location":"stacks/athenapdf/#changelog","text":"","title":"Changelog"},{"location":"stacks/athenapdf/#103","text":"Default memory request set to 16m","title":"1.0.3"},{"location":"stacks/athenapdf/#102","text":"Version freeze to 2.10.0","title":"1.0.2"},{"location":"stacks/athenapdf/#101","text":"AthenaPDF now has public port with technical domain by default","title":"1.0.1"},{"location":"stacks/athenapdf/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/cachet/","text":"Cachet stack documentation Cachet can be configured with the following environment variables Mail delivery No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Cron By default we run the following cron command from crond container every hour: /usr/local/bin/php -q ./artisan schedule:run Containers PHP PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR Crond A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. OpenSMTPD PostgreSQL Redis Changelog 1.1.1 Cachet updated to 2.3.5 Cachet image rebased to latest anaxexp/php image Added Nginx 1.14, 1.15 PostgreSQL Version 10 added Version 9.6 updated to 9.6.9 PHP error reporting now exludes strict and deprecated errors 1.1.0 Nginx image anaxexp/cachet-nginx replaced with anaxexp/php-nginx Now when your upgrade stack with a new version of Cachet, your source code will be updated Default memory request set to: Cachet: 64m Crond: 4m PostgreSQL: 64m Redis: 4m OpenSMTPD: 64m 1.0.1 Updated Redis (1.0.2) and Postgres (1.0.1) services 1.0.0 Initial release","title":"Cachet"},{"location":"stacks/cachet/#cachet-stack-documentation","text":"Cachet can be configured with the following environment variables","title":"Cachet stack documentation"},{"location":"stacks/cachet/#mail-delivery","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services .","title":"Mail delivery"},{"location":"stacks/cachet/#cron","text":"By default we run the following cron command from crond container every hour: /usr/local/bin/php -q ./artisan schedule:run","title":"Cron"},{"location":"stacks/cachet/#containers","text":"","title":"Containers"},{"location":"stacks/cachet/#php","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel","title":"PHP"},{"location":"stacks/cachet/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/cachet/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR","title":"Environment variables"},{"location":"stacks/cachet/#crond","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page.","title":"Crond"},{"location":"stacks/cachet/#opensmtpd","text":"","title":"OpenSMTPD"},{"location":"stacks/cachet/#postgresql","text":"","title":"PostgreSQL"},{"location":"stacks/cachet/#redis","text":"","title":"Redis"},{"location":"stacks/cachet/#changelog","text":"","title":"Changelog"},{"location":"stacks/cachet/#111","text":"Cachet updated to 2.3.5 Cachet image rebased to latest anaxexp/php image Added Nginx 1.14, 1.15 PostgreSQL Version 10 added Version 9.6 updated to 9.6.9 PHP error reporting now exludes strict and deprecated errors","title":"1.1.1"},{"location":"stacks/cachet/#110","text":"Nginx image anaxexp/cachet-nginx replaced with anaxexp/php-nginx Now when your upgrade stack with a new version of Cachet, your source code will be updated Default memory request set to: Cachet: 64m Crond: 4m PostgreSQL: 64m Redis: 4m OpenSMTPD: 64m","title":"1.1.0"},{"location":"stacks/cachet/#101","text":"Updated Redis (1.0.2) and Postgres (1.0.1) services","title":"1.0.1"},{"location":"stacks/cachet/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/docker-registry/","text":"Docker registry stack documentation Add the following environment variables to use redis for layer metadata cache: REGISTRY_STORAGE_CACHE_BLOBDESCRIPTOR: redis REGISTRY_REDIS_ADDR: redis:6379 REGISTRY_REDIS_PASSWORD: [YOUR REDIS PASSWORD] You can acquire auto-generated redis password from App Stack Redis page. Changelog 0.1.1 Default memory request set to 8m 0.1.0 Initial release","title":"Docker registry"},{"location":"stacks/docker-registry/#docker-registry-stack-documentation","text":"Add the following environment variables to use redis for layer metadata cache: REGISTRY_STORAGE_CACHE_BLOBDESCRIPTOR: redis REGISTRY_REDIS_ADDR: redis:6379 REGISTRY_REDIS_PASSWORD: [YOUR REDIS PASSWORD] You can acquire auto-generated redis password from App Stack Redis page.","title":"Docker registry stack documentation"},{"location":"stacks/docker-registry/#changelog","text":"","title":"Changelog"},{"location":"stacks/docker-registry/#011","text":"Default memory request set to 8m","title":"0.1.1"},{"location":"stacks/docker-registry/#010","text":"Initial release","title":"0.1.0"},{"location":"stacks/drupal/","text":"Drupal stack documentation Stack pages: Drupal 8 Drupal 7 Drupal 6 Docker4Drupal (local environment) Deployment See main code deployment article to learn about code deployment options on AnaxExp. Vanilla Drupal For demo purposes and simple Drupal installations you can use Vanilla Drupal deployment option. In this case Drupal code that comes with the Docker image will be used. In case of changes all data made to your codebase will persist but there will be no versions control. Direct git integration We recommend using Composer to manage dependencies in your repository. Dependencies will be installed via post-deployment scripts : Fork our boilerplate or use the original composer template for Drupal Create anaxexp.yml in repository root (our boilerplate already has it) with the following content: pipeline: - name: Install dependencies type: command command: composer install --prefer-dist -n --no-dev directory: $APP_ROOT On the 3 rd step of new application deployment form: Specify web in Codebase dir (default name of subdir with Drupal's codebase) Make sure git branch matches to your Drupal version CI/CD CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: php crond sshd HTTP server: nginx or apache Drush You can execute drush commands remotely via drush aliases. Download drush aliases from Profile Misc Drush aliases page and place them to ~/.drush . Execute commands (replace [tokens] with the real values) like this: $ drush @ [ organization ] . [ application ] . [ instance ] [ drush command ] The domain marked as primary will be used as -l for drush aliases. Import From drush archive This option is available on the 3 rd step of a new application deployment form. First, make sure you have Drush installed , go to your Drupal root directory and execute a command: $ drush archive-dump or $ drush ard You should see output like: $ drush ard Archive saved to /Users/johndoe/drush-backups/archive-dump/20150604001227/drupalapp.20150604_001228.tar.gz /Users/johndoe/drush-backups/archive-dump/20150604001227/drupalapp.20150604_001228.tar.gz Now navigate to Apps Deploy and choose drush archive on the 3 rd step From separate archives Import Drupal via separate archives for database and files. We support .zip , .gz , .tar.gz , .tgz and .tar archives. This option is available on the 3 rd step of a new application deployment form and also on [Instance] Import page of existing instance. Manual import In case your import data is huge it makes sense to import it manually from the server. Follow these steps: Deploy your Drupal website from a git repository without importing data Once the app is deployed, go to Stack SSH and copy SSH command Connect to the container by SSH Copy your database archive here via wget or scp , make sure it's gzipped Import unpacked database dump using drush sql-cli my-db-dump.sql Now let's import your files, cd to /mnt/files , use public and private subdirectories according to your needs Copy your files archive here via wget or scp and unpack the archive That's it! Clear Drupal cache and remove import artifacts Import between instances You can import database and files from one instance to another regardless of whether instances are on the same server or not. Go to [Instance] Import tab and select an instance where you'd like to import database/files from. Backups Files Both public and private files directories under sites/*/files will be backed up. Codebase will not be backed up. Database MariaDB backups run with --single-transaction option to avoid tables locks. Data of the following tables excluded from DB dump: cache cache_% ctools_object_cache ctools_views_cache flood history queue search_index semaphore sequences sessions watchdog Drupal settings Default site When you deploy a new Drupal application you can optionally specify Site directory on the 3 rd step, if not specified we use default . If directory does not exist AnaxExp will create it automatically. For example if you have a directory sites/my-drupal-site/* you should specify my-drupal-site . This directory will be used to locate settings.php file and for building sites.php mapping. The default cron job and orchestrations performed from AnaxExp dashboard such Drupal cache flush will be applied for this site. settings.php AnaxExp automatically adds the include of anaxexp.settings.php to default site's settings.php . The anaxexp.settings.php file contains configuration settings for integration with AnaxExp services such as Database, Cache storage and Reverse Caching Proxy. You can override settings from this file in your sites/*/settings.php file after the include of anaxexp.settings.php . If your settings.php file already has the line with anaxexp.settings.php include the include will not be added again. Do not edit anaxexp.settings.php , all changes to this file will be reset with the next deployment. sites.php AnaxExp automatically adds the include of anaxexp.sites.php to sites.php . The file anaxexp.sites.php generated automatically and contains the mapping of all attached domains to the site directory you've specified for this application. Trusted hosts patterns All domains attached to an HTTP server (Nginx or Apache) or Varnish will be added to Drupal's trusted hosts patterns. Sync directory AnaxExp automatically creates sync directory with a salt and specify it inside of anaxexp.settings.php . Files Files for Drupal located in /mnt/files and symlinked to sites/[SITE NAME]/files . Base URL The domain marked with primary flag will be used as a $base_url in settings.php file and as an -l parameter for the default cron job . Mail delivery No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from Application Stack Settings page. Cron By default we run the following cron command from crond container every hour: drush -r ${HTTP_ROOT} -l ${ANAXEXP_HOST_PRIMARY} cron $ANAXEXP_HOST_PRIMARY is a domain marked as primary. You can customize crontab from [Instance] Stack Settings page. Redirects If you need to make a redirect from one domain to another you can do it by customizing configuration files of nginx or by adding the snippets below to your settings.php file. Redirect from one domain to another: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { if ($_SERVER[ HTTP_HOST ] == redirect-from-domain.com ) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from multiple domains: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { $redirect_from = array( redirect-from-domain-1.com , redirect-from-domain-2.com , ); if (in_array($_SERVER[ HTTP_HOST ], $redirect_from)) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from HTTP to HTTPS can be enabled on a domain edit page from the dashboard. Multi-site There two ways how you can deploy your multi-site Drupal application via AnaxExp Deploy sites as separate app instances Deploy every site as a separate application instance by specifying the default site of every subsite. In this scheme every site will be deployed as a separate application instance with its own database. Deploy all sites in one instance Perform the following modifications in your code: Add the following include in sites/*/settings.php of all sites, specify db prefix for every site: include /var/www/conf/anaxexp.settings.php ; // After the include specify db prefix for every site. // $databases[ default ][ default ][ prefix ] = site_prefix_ ; In your sites.php file the following lines with include and your domains to directories mapping: include /var/www/conf/anaxexp.sites.php ; // Add mapping after the include to override: $sites[ your.domain.com ] = dir.name.under.sites ; Also, you'll have to update cron jobs to run it for every site, not only the primary. Please note that all dashboard orchestration such as Drupal cache clear will be applied only to the default site. In this scheme every site will use the database by default. Cache control You can clear caches and control cache settings from [Instance] Cache page. The following actions are available: Clear application cache (drush cc) Clear redis cache (if enabled) Clear varnish cache (if enabled) Clear all caches Enable/disable opcache Enable/disable redis integration","title":"Overview"},{"location":"stacks/drupal/#drupal-stack-documentation","text":"Stack pages: Drupal 8 Drupal 7 Drupal 6 Docker4Drupal (local environment)","title":"Drupal stack documentation"},{"location":"stacks/drupal/#deployment","text":"See main code deployment article to learn about code deployment options on AnaxExp.","title":"Deployment"},{"location":"stacks/drupal/#vanilla-drupal","text":"For demo purposes and simple Drupal installations you can use Vanilla Drupal deployment option. In this case Drupal code that comes with the Docker image will be used. In case of changes all data made to your codebase will persist but there will be no versions control.","title":"Vanilla Drupal"},{"location":"stacks/drupal/#direct-git-integration","text":"We recommend using Composer to manage dependencies in your repository. Dependencies will be installed via post-deployment scripts : Fork our boilerplate or use the original composer template for Drupal Create anaxexp.yml in repository root (our boilerplate already has it) with the following content: pipeline: - name: Install dependencies type: command command: composer install --prefer-dist -n --no-dev directory: $APP_ROOT On the 3 rd step of new application deployment form: Specify web in Codebase dir (default name of subdir with Drupal's codebase) Make sure git branch matches to your Drupal version","title":"Direct git integration"},{"location":"stacks/drupal/#cicd","text":"CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: php crond sshd HTTP server: nginx or apache","title":"CI/CD"},{"location":"stacks/drupal/#drush","text":"You can execute drush commands remotely via drush aliases. Download drush aliases from Profile Misc Drush aliases page and place them to ~/.drush . Execute commands (replace [tokens] with the real values) like this: $ drush @ [ organization ] . [ application ] . [ instance ] [ drush command ] The domain marked as primary will be used as -l for drush aliases.","title":"Drush"},{"location":"stacks/drupal/#import","text":"","title":"Import"},{"location":"stacks/drupal/#from-drush-archive","text":"This option is available on the 3 rd step of a new application deployment form. First, make sure you have Drush installed , go to your Drupal root directory and execute a command: $ drush archive-dump or $ drush ard You should see output like: $ drush ard Archive saved to /Users/johndoe/drush-backups/archive-dump/20150604001227/drupalapp.20150604_001228.tar.gz /Users/johndoe/drush-backups/archive-dump/20150604001227/drupalapp.20150604_001228.tar.gz Now navigate to Apps Deploy and choose drush archive on the 3 rd step","title":"From drush archive"},{"location":"stacks/drupal/#from-separate-archives","text":"Import Drupal via separate archives for database and files. We support .zip , .gz , .tar.gz , .tgz and .tar archives. This option is available on the 3 rd step of a new application deployment form and also on [Instance] Import page of existing instance.","title":"From separate archives"},{"location":"stacks/drupal/#manual-import","text":"In case your import data is huge it makes sense to import it manually from the server. Follow these steps: Deploy your Drupal website from a git repository without importing data Once the app is deployed, go to Stack SSH and copy SSH command Connect to the container by SSH Copy your database archive here via wget or scp , make sure it's gzipped Import unpacked database dump using drush sql-cli my-db-dump.sql Now let's import your files, cd to /mnt/files , use public and private subdirectories according to your needs Copy your files archive here via wget or scp and unpack the archive That's it! Clear Drupal cache and remove import artifacts","title":"Manual import"},{"location":"stacks/drupal/#import-between-instances","text":"You can import database and files from one instance to another regardless of whether instances are on the same server or not. Go to [Instance] Import tab and select an instance where you'd like to import database/files from.","title":"Import between instances"},{"location":"stacks/drupal/#backups","text":"","title":"Backups"},{"location":"stacks/drupal/#files","text":"Both public and private files directories under sites/*/files will be backed up. Codebase will not be backed up.","title":"Files"},{"location":"stacks/drupal/#database","text":"MariaDB backups run with --single-transaction option to avoid tables locks. Data of the following tables excluded from DB dump: cache cache_% ctools_object_cache ctools_views_cache flood history queue search_index semaphore sequences sessions watchdog","title":"Database"},{"location":"stacks/drupal/#drupal-settings","text":"","title":"Drupal settings"},{"location":"stacks/drupal/#default-site","text":"When you deploy a new Drupal application you can optionally specify Site directory on the 3 rd step, if not specified we use default . If directory does not exist AnaxExp will create it automatically. For example if you have a directory sites/my-drupal-site/* you should specify my-drupal-site . This directory will be used to locate settings.php file and for building sites.php mapping. The default cron job and orchestrations performed from AnaxExp dashboard such Drupal cache flush will be applied for this site.","title":"Default site"},{"location":"stacks/drupal/#settingsphp","text":"AnaxExp automatically adds the include of anaxexp.settings.php to default site's settings.php . The anaxexp.settings.php file contains configuration settings for integration with AnaxExp services such as Database, Cache storage and Reverse Caching Proxy. You can override settings from this file in your sites/*/settings.php file after the include of anaxexp.settings.php . If your settings.php file already has the line with anaxexp.settings.php include the include will not be added again. Do not edit anaxexp.settings.php , all changes to this file will be reset with the next deployment.","title":"settings.php"},{"location":"stacks/drupal/#sitesphp","text":"AnaxExp automatically adds the include of anaxexp.sites.php to sites.php . The file anaxexp.sites.php generated automatically and contains the mapping of all attached domains to the site directory you've specified for this application.","title":"sites.php"},{"location":"stacks/drupal/#trusted-hosts-patterns","text":"All domains attached to an HTTP server (Nginx or Apache) or Varnish will be added to Drupal's trusted hosts patterns.","title":"Trusted hosts patterns"},{"location":"stacks/drupal/#sync-directory","text":"AnaxExp automatically creates sync directory with a salt and specify it inside of anaxexp.settings.php .","title":"Sync directory"},{"location":"stacks/drupal/#files_1","text":"Files for Drupal located in /mnt/files and symlinked to sites/[SITE NAME]/files .","title":"Files"},{"location":"stacks/drupal/#base-url","text":"The domain marked with primary flag will be used as a $base_url in settings.php file and as an -l parameter for the default cron job .","title":"Base URL"},{"location":"stacks/drupal/#mail-delivery","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from Application Stack Settings page.","title":"Mail delivery"},{"location":"stacks/drupal/#cron","text":"By default we run the following cron command from crond container every hour: drush -r ${HTTP_ROOT} -l ${ANAXEXP_HOST_PRIMARY} cron $ANAXEXP_HOST_PRIMARY is a domain marked as primary. You can customize crontab from [Instance] Stack Settings page.","title":"Cron"},{"location":"stacks/drupal/#redirects","text":"If you need to make a redirect from one domain to another you can do it by customizing configuration files of nginx or by adding the snippets below to your settings.php file. Redirect from one domain to another: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { if ($_SERVER[ HTTP_HOST ] == redirect-from-domain.com ) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from multiple domains: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { $redirect_from = array( redirect-from-domain-1.com , redirect-from-domain-2.com , ); if (in_array($_SERVER[ HTTP_HOST ], $redirect_from)) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from HTTP to HTTPS can be enabled on a domain edit page from the dashboard.","title":"Redirects"},{"location":"stacks/drupal/#multi-site","text":"There two ways how you can deploy your multi-site Drupal application via AnaxExp","title":"Multi-site"},{"location":"stacks/drupal/#deploy-sites-as-separate-app-instances","text":"Deploy every site as a separate application instance by specifying the default site of every subsite. In this scheme every site will be deployed as a separate application instance with its own database.","title":"Deploy sites as separate app instances"},{"location":"stacks/drupal/#deploy-all-sites-in-one-instance","text":"Perform the following modifications in your code: Add the following include in sites/*/settings.php of all sites, specify db prefix for every site: include /var/www/conf/anaxexp.settings.php ; // After the include specify db prefix for every site. // $databases[ default ][ default ][ prefix ] = site_prefix_ ; In your sites.php file the following lines with include and your domains to directories mapping: include /var/www/conf/anaxexp.sites.php ; // Add mapping after the include to override: $sites[ your.domain.com ] = dir.name.under.sites ; Also, you'll have to update cron jobs to run it for every site, not only the primary. Please note that all dashboard orchestration such as Drupal cache clear will be applied only to the default site. In this scheme every site will use the database by default.","title":"Deploy all sites in one instance"},{"location":"stacks/drupal/#cache-control","text":"You can clear caches and control cache settings from [Instance] Cache page. The following actions are available: Clear application cache (drush cc) Clear redis cache (if enabled) Clear varnish cache (if enabled) Clear all caches Enable/disable opcache Enable/disable redis integration","title":"Cache control"},{"location":"stacks/drupal/changelog/","text":"Drupal stack changelog This is the changelog for Drupal stack deployed via AnaxExp, for docker4drupal changes see GitHub releases page . 5.1.0 Drupal Vanilla Drupal core updated to 8.5.4 We now set $settings['reverse_proxy_addresses'] and $settings['reverse_proxy'] in anaxexp.settings.php file. You can also add additional proxy addresses via env var DRUPAL_REVERSE_PROXY_ADDRESSES PHP \u2b50\ufe0f Added new PHP 7.2 Added php tidy extension Added tideways xhprof extension https://github.com/anaxexp/drupal-php#49 (disabled by default) auto_prepend_file and auto_append_file are now configurable Updated PHP extensions: GRPC 1.12.0, igbinary 2.0.6, mongodb 1.4.4 MariaDB: New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2b50\ufe0f Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container. See MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application Default innodb_buffer_pool_instances set to 1 Nginx: Added new Nginx 1.15, dropped legacy Nginx 1.12 \u2b50\ufe0f Added mog_pagespeed module. Disabled by default, to enable add NGINX_PAGESPEED=on to nginx service Added new modules: http_image_filter_module http_slice_module http_xslt_module stream_geoip_module stream_realip_module stream_ssl_preread_module Varnish Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4 \u2757Static files no longer cached unless you set VARNISH_CACHE_STATIC_FILES https://github.com/anaxexp/drupal-varnish/pull/4 Added VARNISH_SECONDARY_STORAGE_CONDITION to specify the condition when to use secondary storage https://github.com/anaxexp/drupal-varnish/pull/3 Webgrind: error reporting now exludes strict and deprecated errors, rebased to latest PHP 7.1 image Upgrade instructions \u2757Make sure the new default size of innodb_buffer_pool_instances (128M) is enough for your project, see MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application 5.0.7 PHP: Security update : 7.2.5, 7.1.17, 7.0.30 New php extensions added: GMP and igbinary APCu extension updated to 5.0.11 for PHP 7.x APCu serialized is now configurable with $PHP_APCU_SERIALIZER Shell prompt in PHP containers now shows current user, application name and instance name Added new helper script files_chown Bugfix: iconv implementation missing anaxexp/php#25 Vanilla Drupal: Security update : 8.5.3 Bugfix: drush cache permission issues anaxexp/drupal#261 Drupal node container rebased to anaxexp/node with freezed node version Added Nginx 1.14, patch update for 1.13 Nginx's underscores_in_headers is now configurable via $NGINX_UNDERSCORES_IN_HEADERS 5.0.6 PHP log errors max length set to unlimited Bugfix: PHP errors didn't show up in the container output Bugfix: APACHE_LIMITED_ACCESS support from 5.0.4 release was missing 5.0.5 PHP: Updated to 7.2.4, 7.1.16, 7.0.35 ( security update ) Added jpegoptim 5.0.4 Vanilla Drupal updated to 8.5.1 ( security update ) Apache: Updated to 2.4.33 ( security update ) New environment variable APACHE_LIMITED_ACCESS to remove Require all granted when you need to limit access by IP 5.0.3 PHP: PHP extension grpc updated to 1.10.0 Added environment variables for PHP session runtime configuration Improved error reporting and progress messages for public files directory init Bugfix: global drush used instead of drush launcher Solr: New 7.2 version Patch update: 6.6.3 Solr 7.x config sources updated to search_api_solr 8.x-2.0-alpha3 Nginx updated to 1.13.10 You can now override apache config with APACHE_INCLUDE_CONF 5.0.2 Cron now runs from www-data user instead of anaxexp Vanilla Drupal updated to 8.5.0 Drupal console launcher updated to 1.7.0 and freezed files_chmod script now sets permissions with execution allowed only for directories 5.0.1 PHP updated to 7.2.3, 7.1.15, 7.0.28 (security updates) Global drush freezed to 8.x 5.0.0 Changes since 4.4.1 All containers now have resources request as listed here in Resources column , in addition, crond has CPU limit PHP: Container default user has been changed to anaxexp (uid/gid 1000), see https://github.com/anaxexp/php#users-and-permissions for more details PHP updated to 7.1.14, 7.0.27, 5.6.33 (security updates) Rebased to Alpine Linux 3.7 Now when your upgrade stack with a new version of vanilla Drupal, your source code will be updated You can monitor PHP with NewRelic APM allow_url_fopen and default_socket_timeout is now configurable New php extensions added: newrelic, grpc, ds Global drush updated to 9.x for PHP 7.x Drush launcher updated to 0.5.1 Deprecated environment variables dropped (listed in 4.4.0 changes ) Added postgresql client bins (pg_dump, pg_restore, ...) Added redis-cli Updated php extensions: amqp 1.9.3, redis 3.1.6, mongodb 1.4.0, apcu 5.1.10 Environment variable ANAXEXP_DIR_FILES replaced to FILES_DIR Vanilla Drupal updated to 8.4.5 MariaDB: Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Nginx: Updated to 1.13.9 Rebased to Alpine Linux 3.7 Redis: Updated to 4.0.8 Bugfix: redis 4 init could not disable THP on some servers OpenSMTPD: Improved health check now runs smtp command Messages queue is now persistent Varnish: The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 ) Bugfix: auth issue in Apache ( https://github.com/anaxexp/php-apache/issues/1 ) Upgrade instructions Make sure you don't use any of deprecated environment variables in PHP (listed in 4.4.0 changes ) and Varnish (listed above) otherwise update their names If you used ANAXEXP_DIR_FILES in your code replace it with FILES_DIR Make sure the default cron container 512M RAM limit is enough for your cron jobs, otherwise increase it manually from service configuration page 4.4.1 Vanilla Drupal updated to 8.4.3 Fixed missing tags for vanilla with PHP 7.0 Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 4.4.0 4.4.0 Changes since 4.3.0 PHP: PHP updated to 7.1.12, 7.0.26 PHP extensions updated: memcached 3.0.4, ast 0.1.6 Added packages: tig, nano, tmux, less, libjpeg-turbo-utils PHPunit deleted from image to avoid composer conflicts Env vars naming fixes (old names still supported), old new: PHP_APCU_ENABLE PHP_APCU_ENABLED PHP_FPM_SLOWLOG_TIMEOUT PHP_FPM_REQUEST_SLOWLOG_TIMEOUT PHP_FPM_MAX_CHILDREN PHP_FPM_PM_MAX_CHILDREN PHP_FPM_START_SERVERS PHP_FPM_PM_START_SERVERS PHP_FPM_MIN_SPARE_SERVERS PHP_FPM_PM_MIN_SPARE_SERVERS PHP_FPM_MAX_SPARE_SERVERS PHP_FPM_PM_MAX_SPARE_SERVERS PHP_FPM_MAX_REQUESTS PHP_FPM_PM_MAX_REQUESTS PHP_FPM_STATUS_PATH PHP_FPM_PM_STATUS_PATH New -dev image tags (replacing -debug ) for CI/CD (TBA) Env var ANAXEXP_HOST_PRIMARY value now contains host (instead of URL) as it should, ANAXEXP_URL_PRIMARY has been added for the URL value. See environment variables section Drush launcher added Improved validation and error reporting for drush import Git email and name now can be configured via environment variables Nginx: Nginx updated to 1.13.7, 1.12.2 Fixed broken health check New env var NGINX_NO_DEFAULT_HEADERS to hide default headers We now show request real IP in access logs New env var NGINX_DRUPAL_FILE_PROXY_URL to set up static files proxy MariaDB: New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Deployment strategy no longer can be changed Optimized default config (my.cnf) values New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice and ionice to prioritize CPU and I/O time for this process Improved error reporting during import Solr: New Solr versions 7.0.1 and 7.1.0 have been added Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 Config set source search_api_solr updated to 8.x-1.2 We now create a default solr core named default automatically if no cores found Redis: Redis updated to 3.2.11, 4.0.2 Fixed init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Varnish: Varnish updated to 4.1.9 Cache hash id now respects protocol (http/https) Global environment variables changes: $ANAXEXP_APP_NAME no longer contains instance machine name, only application machine name $ANAXEXP_ENVIRONMENT_ variables have been deprecated and replaced with $ANAXEXP_INSTANCE_ New variables $ANAXEXP_INSTANCE_UUID and $ANAXEXP_APP_UUID Apache updated to 2.4.29 Vanilla Drupal updated to 8.4.2 Health checks timeout increased to 30 seconds for all services OpenSMTPD now supports relay auth without password Files backup and mirroring actions now run with nice and ionice to prioritize CPU and I/O time for this process Update instructions from 4.3.0 If you used $ANAXEXP_APP_NAME update your code accordingly to the new value (machine name of the app) If you used $ANAXEXP_HOST_PRIMARY (now contains host instead of URL) before you should replace it to $ANAXEXP_URL_PRIMARY Upgrade downtime ~5 minutes 4.3.0 Changes since 4.2.1 User www-data is now default in php, nginx and apache containers PHP: PHP updated to 7.0.24, 7.1.10 Default vanilla Drupal updated to 8.4.0 Core extension pcntl is now enabled in PHP 7.x Libressl added Extensions update: redis 3.1.4, mongodb 1.3.0 New extension geoip Default post_max_size , upload_max_filesize set to 32m Optimized default opcache settings New env var PHP_MAX_FILE_UPLOADS to control max_file_uploads Bugfix: apcu (PHP 7.x) could cause segfaults in some cases Bugfix: path to CA certificates specified in ldap config Bugfix: files backup could fail when files changed during the process Bugfix: missing quotes in anaxexp.sites.php Nginx: Nginx updated to 1.13.5 Nginx config revamped: upstream name changed from backend to php and moved from nginx.conf to drupal.conf Default client_max_body_size set to 32m Bugfix: broken static files on Drupal's 8 update.php page Apache: Apache updated to 2.4.28 Image has been replaced to generic anaxexp/php-apache Varnish Env vars for daemon launch params now have prefix VARNISHD_ to avoid collisions New env vars VARNISH_EXCLUDE_URLS and VARNISH_STATIC_FILES for customization Default exclude URLs now consider language prefixes OpenSMTPD bugfix: health probes caused warning in logs Update instructions from 4.2.1 !!! If you forked drupal.conf , you must get the latest version from the source ( /etc/nginx/conf.d/drupal.conf ) and re-apply your changes. If you used NGINX_SERVER_EXTRA_CONF_FILEPATH , update usage of backend upstream to php Make sure that the new default value (32m) of php's post_max_size , upload_max_filesize and nginx's client_max_body_size is enough for you If you customized varnish launch params, update corresponding env vars prefix to VARNISHD_ 4.2.1 Changes since 4.2.0 Improved backward compatibility, the following environment variables are now available from PHP-FPM 4.2.0 Changes since 4.1.9 PHP updated to 7.1.9, 7.0.23 PHPUnit updated to 6.3 New service Blackfire agent for profiling via blackfire.io, see usage instructions Environment varibles now cleared in PHP-FPM by default except for ANAXEXP_APP_NAME , ANAXEXP_ENVIRONMENT_TYPE , ANAXEXP_ENVIRONMENT_NAME . You can disable it by adding environment variable PHP_FPM_CLEAR_ENV with no value to Drupal (PHP) container OpenSMTPD now supports relay without auth Bugfix: PHP-FPM health probes sometimes could fail Upgrade notes Downtime 5 minutes 4.1.9 Changes since 4.1.8 Vanilla Drupal updated to 8.3.7 MariaDB and its client updated to 10.1.26 Athenapdf versions freeze to 2.10.0 Bugfix: PHP-FPM health probes sometimes could fail Upgrade notes Downtime 5-10 minutes 4.1.8 Updated service: Redis (1.0.3) PHP containers now have health checks and can be upgraded without downtime We no longer support sites/*/files directories under version control Drush modules added: registry rebuild and patchfile PHP update: 7.1.8, 7.0.22 Vanilla Drupal update: 8.3.6 Nginx update: 1.13.4 Experimental redis 4.0 added You can now enable PHP slowlog via environment variable PHP_FPM_SLOWLOG_TIMEOUT Improved Varnish health checks, now use varnishadm instead of curl Varnish bugfix: duplicated X-Forwarded-For header Varnish bugfix: unrestricted purge/ban, now allowed only from internal network 4.1.7 New container webgrind \u2013 Xdebug profiling web frontend Additional environment variables for Xdebug extension configuration including tracing and profiling PHP extensions update: ast 0.1.5; yaml 2.0.2 Improvement: better handling of failed deployments Bugfix: some environment variable could be unavailble in SSH container 4.1.6 Solr: fixed persistent data paths configuration 4.1.5 New fast health-check endpoints for Nginx and Apache2 hidden from access logs by default Updated services: Redis (1.0.2) , MariaDB (1.0.3) , AthenaPDF (1.0.1) PHP extension redis updated to 3.1.3 Number of default PHP-FPM workers set to 8 4.1.4 PHP updates: 7.1.7 , 7.0.21 with security fixes Nginx updates: 1.13.3 , 1.12.1 with a fix in the range filter vulnerability (CVE-2017-7529). Apache2 updates: 2.4.27 Vanilla Drupal updates: 8.3.5 Solr: new versions 6.6 and 6.5 for Drupal 8 Solr: search_api_solr version updated from to 8.x-1.0 (default solr configs used from this module) Nginx: Content-Type is now set only if not empty https://github.com/anaxexp/drupal-nginx/issues/27 Bugfix: some files could be delete during drush import Bugfix: vanilla Drupal always re-synced Drupal sources https://github.com/anaxexp/drupal/issues/2 Solr versions are now frozen https://github.com/anaxexp/solr#versions Redis version is now frozen https://github.com/anaxexp/redis#versions 4.1.3 New Apache container New Adminer container Nginx updated to 1.13.2 Drupal headers no longer hidden by default, configurable via Nginx/Apache environment variables MariaDB now recovers privileges in case of an error during import Drupal node container freeze 4.1.2 Updated MariaDB with bug fixes PHP extension APCu is now configurable All PHP extensions are now frozen Runtime Alpine Linux libraries used by PHP extension are now frozen Dropped few environment variables Updated vanilla Drupal: 8.3.4 4.1.1 Updated PHP: 7.1.5 7.1.6, 7.0.19 7.0.20 OpenSMTPD can now relay emails to 3 rd party SMTP servers OpenSMTPD rebased to Alpine 3.6 with freezed version Environment variables for OpenSMTPD configuration OpenSMTPD now writes logs to container output Increased max_allowed_packet for MariaDB daemon PHP expose header now disabled by default Nginx images rebased to Alpine 3.6 Updated vanilla Drupal 4.1.0 Bug fix: git checkout in php container sometimes failed Bug fix: varnish cache flush action failed Bug fix: some environment variables missed in SSH container MariaDB: No longer lock table during backups (--single-transaction) MariaDB: Excludes cache tables data from backups. See backups section for more details Nginx: New version: 1.13.0 1.13.1 PHP: All images rebased to Alpine Linux 3.6 and now use LibreSSL instead of OpenSSL PHP: Fixed segfault caused by imagick extension PHP: MongoDB extension downgraded to 1.1.10 New AthenaPDF container \u2013 drop-in replacement for wkhtmltopdf New Rsyslog container New Node.js container 4.0.1 Bug fixes and stabilization improvements Images versions freeze PHP versions freeze 4.0.0 Changes since 3.x All-new revamped docker container images consistent with docker4drupal Improved performance of containers Revamped orchestration with better logging and performance Optional services now can be enabled/disabled on the working app Services configuration via environment variables from the dashboard Services' containers now can can be scaled (# of replicas) Detailed log output for orchestration tasks Redesigned scalability for cluster deployments There's no backward compatibility with stacks 3.x","title":"Changelog"},{"location":"stacks/drupal/changelog/#drupal-stack-changelog","text":"This is the changelog for Drupal stack deployed via AnaxExp, for docker4drupal changes see GitHub releases page .","title":"Drupal stack changelog"},{"location":"stacks/drupal/changelog/#510","text":"Drupal Vanilla Drupal core updated to 8.5.4 We now set $settings['reverse_proxy_addresses'] and $settings['reverse_proxy'] in anaxexp.settings.php file. You can also add additional proxy addresses via env var DRUPAL_REVERSE_PROXY_ADDRESSES PHP \u2b50\ufe0f Added new PHP 7.2 Added php tidy extension Added tideways xhprof extension https://github.com/anaxexp/drupal-php#49 (disabled by default) auto_prepend_file and auto_append_file are now configurable Updated PHP extensions: GRPC 1.12.0, igbinary 2.0.6, mongodb 1.4.4 MariaDB: New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2b50\ufe0f Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container. See MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application Default innodb_buffer_pool_instances set to 1 Nginx: Added new Nginx 1.15, dropped legacy Nginx 1.12 \u2b50\ufe0f Added mog_pagespeed module. Disabled by default, to enable add NGINX_PAGESPEED=on to nginx service Added new modules: http_image_filter_module http_slice_module http_xslt_module stream_geoip_module stream_realip_module stream_ssl_preread_module Varnish Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4 \u2757Static files no longer cached unless you set VARNISH_CACHE_STATIC_FILES https://github.com/anaxexp/drupal-varnish/pull/4 Added VARNISH_SECONDARY_STORAGE_CONDITION to specify the condition when to use secondary storage https://github.com/anaxexp/drupal-varnish/pull/3 Webgrind: error reporting now exludes strict and deprecated errors, rebased to latest PHP 7.1 image","title":"5.1.0"},{"location":"stacks/drupal/changelog/#upgrade-instructions","text":"\u2757Make sure the new default size of innodb_buffer_pool_instances (128M) is enough for your project, see MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application","title":"Upgrade instructions"},{"location":"stacks/drupal/changelog/#507","text":"PHP: Security update : 7.2.5, 7.1.17, 7.0.30 New php extensions added: GMP and igbinary APCu extension updated to 5.0.11 for PHP 7.x APCu serialized is now configurable with $PHP_APCU_SERIALIZER Shell prompt in PHP containers now shows current user, application name and instance name Added new helper script files_chown Bugfix: iconv implementation missing anaxexp/php#25 Vanilla Drupal: Security update : 8.5.3 Bugfix: drush cache permission issues anaxexp/drupal#261 Drupal node container rebased to anaxexp/node with freezed node version Added Nginx 1.14, patch update for 1.13 Nginx's underscores_in_headers is now configurable via $NGINX_UNDERSCORES_IN_HEADERS","title":"5.0.7"},{"location":"stacks/drupal/changelog/#506","text":"PHP log errors max length set to unlimited Bugfix: PHP errors didn't show up in the container output Bugfix: APACHE_LIMITED_ACCESS support from 5.0.4 release was missing","title":"5.0.6"},{"location":"stacks/drupal/changelog/#505","text":"PHP: Updated to 7.2.4, 7.1.16, 7.0.35 ( security update ) Added jpegoptim","title":"5.0.5"},{"location":"stacks/drupal/changelog/#504","text":"Vanilla Drupal updated to 8.5.1 ( security update ) Apache: Updated to 2.4.33 ( security update ) New environment variable APACHE_LIMITED_ACCESS to remove Require all granted when you need to limit access by IP","title":"5.0.4"},{"location":"stacks/drupal/changelog/#503","text":"PHP: PHP extension grpc updated to 1.10.0 Added environment variables for PHP session runtime configuration Improved error reporting and progress messages for public files directory init Bugfix: global drush used instead of drush launcher Solr: New 7.2 version Patch update: 6.6.3 Solr 7.x config sources updated to search_api_solr 8.x-2.0-alpha3 Nginx updated to 1.13.10 You can now override apache config with APACHE_INCLUDE_CONF","title":"5.0.3"},{"location":"stacks/drupal/changelog/#502","text":"Cron now runs from www-data user instead of anaxexp Vanilla Drupal updated to 8.5.0 Drupal console launcher updated to 1.7.0 and freezed files_chmod script now sets permissions with execution allowed only for directories","title":"5.0.2"},{"location":"stacks/drupal/changelog/#501","text":"PHP updated to 7.2.3, 7.1.15, 7.0.28 (security updates) Global drush freezed to 8.x","title":"5.0.1"},{"location":"stacks/drupal/changelog/#500","text":"","title":"5.0.0"},{"location":"stacks/drupal/changelog/#changes-since-441","text":"All containers now have resources request as listed here in Resources column , in addition, crond has CPU limit PHP: Container default user has been changed to anaxexp (uid/gid 1000), see https://github.com/anaxexp/php#users-and-permissions for more details PHP updated to 7.1.14, 7.0.27, 5.6.33 (security updates) Rebased to Alpine Linux 3.7 Now when your upgrade stack with a new version of vanilla Drupal, your source code will be updated You can monitor PHP with NewRelic APM allow_url_fopen and default_socket_timeout is now configurable New php extensions added: newrelic, grpc, ds Global drush updated to 9.x for PHP 7.x Drush launcher updated to 0.5.1 Deprecated environment variables dropped (listed in 4.4.0 changes ) Added postgresql client bins (pg_dump, pg_restore, ...) Added redis-cli Updated php extensions: amqp 1.9.3, redis 3.1.6, mongodb 1.4.0, apcu 5.1.10 Environment variable ANAXEXP_DIR_FILES replaced to FILES_DIR Vanilla Drupal updated to 8.4.5 MariaDB: Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Nginx: Updated to 1.13.9 Rebased to Alpine Linux 3.7 Redis: Updated to 4.0.8 Bugfix: redis 4 init could not disable THP on some servers OpenSMTPD: Improved health check now runs smtp command Messages queue is now persistent Varnish: The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 ) Bugfix: auth issue in Apache ( https://github.com/anaxexp/php-apache/issues/1 )","title":"Changes since 4.4.1"},{"location":"stacks/drupal/changelog/#upgrade-instructions_1","text":"Make sure you don't use any of deprecated environment variables in PHP (listed in 4.4.0 changes ) and Varnish (listed above) otherwise update their names If you used ANAXEXP_DIR_FILES in your code replace it with FILES_DIR Make sure the default cron container 512M RAM limit is enough for your cron jobs, otherwise increase it manually from service configuration page","title":"Upgrade instructions"},{"location":"stacks/drupal/changelog/#441","text":"Vanilla Drupal updated to 8.4.3 Fixed missing tags for vanilla with PHP 7.0 Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 4.4.0","title":"4.4.1"},{"location":"stacks/drupal/changelog/#440","text":"","title":"4.4.0"},{"location":"stacks/drupal/changelog/#changes-since-430","text":"PHP: PHP updated to 7.1.12, 7.0.26 PHP extensions updated: memcached 3.0.4, ast 0.1.6 Added packages: tig, nano, tmux, less, libjpeg-turbo-utils PHPunit deleted from image to avoid composer conflicts Env vars naming fixes (old names still supported), old new: PHP_APCU_ENABLE PHP_APCU_ENABLED PHP_FPM_SLOWLOG_TIMEOUT PHP_FPM_REQUEST_SLOWLOG_TIMEOUT PHP_FPM_MAX_CHILDREN PHP_FPM_PM_MAX_CHILDREN PHP_FPM_START_SERVERS PHP_FPM_PM_START_SERVERS PHP_FPM_MIN_SPARE_SERVERS PHP_FPM_PM_MIN_SPARE_SERVERS PHP_FPM_MAX_SPARE_SERVERS PHP_FPM_PM_MAX_SPARE_SERVERS PHP_FPM_MAX_REQUESTS PHP_FPM_PM_MAX_REQUESTS PHP_FPM_STATUS_PATH PHP_FPM_PM_STATUS_PATH New -dev image tags (replacing -debug ) for CI/CD (TBA) Env var ANAXEXP_HOST_PRIMARY value now contains host (instead of URL) as it should, ANAXEXP_URL_PRIMARY has been added for the URL value. See environment variables section Drush launcher added Improved validation and error reporting for drush import Git email and name now can be configured via environment variables Nginx: Nginx updated to 1.13.7, 1.12.2 Fixed broken health check New env var NGINX_NO_DEFAULT_HEADERS to hide default headers We now show request real IP in access logs New env var NGINX_DRUPAL_FILE_PROXY_URL to set up static files proxy MariaDB: New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Deployment strategy no longer can be changed Optimized default config (my.cnf) values New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice and ionice to prioritize CPU and I/O time for this process Improved error reporting during import Solr: New Solr versions 7.0.1 and 7.1.0 have been added Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 Config set source search_api_solr updated to 8.x-1.2 We now create a default solr core named default automatically if no cores found Redis: Redis updated to 3.2.11, 4.0.2 Fixed init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Varnish: Varnish updated to 4.1.9 Cache hash id now respects protocol (http/https) Global environment variables changes: $ANAXEXP_APP_NAME no longer contains instance machine name, only application machine name $ANAXEXP_ENVIRONMENT_ variables have been deprecated and replaced with $ANAXEXP_INSTANCE_ New variables $ANAXEXP_INSTANCE_UUID and $ANAXEXP_APP_UUID Apache updated to 2.4.29 Vanilla Drupal updated to 8.4.2 Health checks timeout increased to 30 seconds for all services OpenSMTPD now supports relay auth without password Files backup and mirroring actions now run with nice and ionice to prioritize CPU and I/O time for this process","title":"Changes since 4.3.0"},{"location":"stacks/drupal/changelog/#update-instructions-from-430","text":"If you used $ANAXEXP_APP_NAME update your code accordingly to the new value (machine name of the app) If you used $ANAXEXP_HOST_PRIMARY (now contains host instead of URL) before you should replace it to $ANAXEXP_URL_PRIMARY Upgrade downtime ~5 minutes","title":"Update instructions from 4.3.0"},{"location":"stacks/drupal/changelog/#430","text":"","title":"4.3.0"},{"location":"stacks/drupal/changelog/#changes-since-421","text":"User www-data is now default in php, nginx and apache containers PHP: PHP updated to 7.0.24, 7.1.10 Default vanilla Drupal updated to 8.4.0 Core extension pcntl is now enabled in PHP 7.x Libressl added Extensions update: redis 3.1.4, mongodb 1.3.0 New extension geoip Default post_max_size , upload_max_filesize set to 32m Optimized default opcache settings New env var PHP_MAX_FILE_UPLOADS to control max_file_uploads Bugfix: apcu (PHP 7.x) could cause segfaults in some cases Bugfix: path to CA certificates specified in ldap config Bugfix: files backup could fail when files changed during the process Bugfix: missing quotes in anaxexp.sites.php Nginx: Nginx updated to 1.13.5 Nginx config revamped: upstream name changed from backend to php and moved from nginx.conf to drupal.conf Default client_max_body_size set to 32m Bugfix: broken static files on Drupal's 8 update.php page Apache: Apache updated to 2.4.28 Image has been replaced to generic anaxexp/php-apache Varnish Env vars for daemon launch params now have prefix VARNISHD_ to avoid collisions New env vars VARNISH_EXCLUDE_URLS and VARNISH_STATIC_FILES for customization Default exclude URLs now consider language prefixes OpenSMTPD bugfix: health probes caused warning in logs","title":"Changes since 4.2.1"},{"location":"stacks/drupal/changelog/#update-instructions-from-421","text":"!!! If you forked drupal.conf , you must get the latest version from the source ( /etc/nginx/conf.d/drupal.conf ) and re-apply your changes. If you used NGINX_SERVER_EXTRA_CONF_FILEPATH , update usage of backend upstream to php Make sure that the new default value (32m) of php's post_max_size , upload_max_filesize and nginx's client_max_body_size is enough for you If you customized varnish launch params, update corresponding env vars prefix to VARNISHD_","title":"Update instructions from 4.2.1"},{"location":"stacks/drupal/changelog/#421","text":"","title":"4.2.1"},{"location":"stacks/drupal/changelog/#changes-since-420","text":"Improved backward compatibility, the following environment variables are now available from PHP-FPM","title":"Changes since 4.2.0"},{"location":"stacks/drupal/changelog/#420","text":"","title":"4.2.0"},{"location":"stacks/drupal/changelog/#changes-since-419","text":"PHP updated to 7.1.9, 7.0.23 PHPUnit updated to 6.3 New service Blackfire agent for profiling via blackfire.io, see usage instructions Environment varibles now cleared in PHP-FPM by default except for ANAXEXP_APP_NAME , ANAXEXP_ENVIRONMENT_TYPE , ANAXEXP_ENVIRONMENT_NAME . You can disable it by adding environment variable PHP_FPM_CLEAR_ENV with no value to Drupal (PHP) container OpenSMTPD now supports relay without auth Bugfix: PHP-FPM health probes sometimes could fail","title":"Changes since 4.1.9"},{"location":"stacks/drupal/changelog/#upgrade-notes","text":"Downtime 5 minutes","title":"Upgrade notes"},{"location":"stacks/drupal/changelog/#419","text":"","title":"4.1.9"},{"location":"stacks/drupal/changelog/#changes-since-418","text":"Vanilla Drupal updated to 8.3.7 MariaDB and its client updated to 10.1.26 Athenapdf versions freeze to 2.10.0 Bugfix: PHP-FPM health probes sometimes could fail","title":"Changes since 4.1.8"},{"location":"stacks/drupal/changelog/#upgrade-notes_1","text":"Downtime 5-10 minutes","title":"Upgrade notes"},{"location":"stacks/drupal/changelog/#418","text":"Updated service: Redis (1.0.3) PHP containers now have health checks and can be upgraded without downtime We no longer support sites/*/files directories under version control Drush modules added: registry rebuild and patchfile PHP update: 7.1.8, 7.0.22 Vanilla Drupal update: 8.3.6 Nginx update: 1.13.4 Experimental redis 4.0 added You can now enable PHP slowlog via environment variable PHP_FPM_SLOWLOG_TIMEOUT Improved Varnish health checks, now use varnishadm instead of curl Varnish bugfix: duplicated X-Forwarded-For header Varnish bugfix: unrestricted purge/ban, now allowed only from internal network","title":"4.1.8"},{"location":"stacks/drupal/changelog/#417","text":"New container webgrind \u2013 Xdebug profiling web frontend Additional environment variables for Xdebug extension configuration including tracing and profiling PHP extensions update: ast 0.1.5; yaml 2.0.2 Improvement: better handling of failed deployments Bugfix: some environment variable could be unavailble in SSH container","title":"4.1.7"},{"location":"stacks/drupal/changelog/#416","text":"Solr: fixed persistent data paths configuration","title":"4.1.6"},{"location":"stacks/drupal/changelog/#415","text":"New fast health-check endpoints for Nginx and Apache2 hidden from access logs by default Updated services: Redis (1.0.2) , MariaDB (1.0.3) , AthenaPDF (1.0.1) PHP extension redis updated to 3.1.3 Number of default PHP-FPM workers set to 8","title":"4.1.5"},{"location":"stacks/drupal/changelog/#414","text":"PHP updates: 7.1.7 , 7.0.21 with security fixes Nginx updates: 1.13.3 , 1.12.1 with a fix in the range filter vulnerability (CVE-2017-7529). Apache2 updates: 2.4.27 Vanilla Drupal updates: 8.3.5 Solr: new versions 6.6 and 6.5 for Drupal 8 Solr: search_api_solr version updated from to 8.x-1.0 (default solr configs used from this module) Nginx: Content-Type is now set only if not empty https://github.com/anaxexp/drupal-nginx/issues/27 Bugfix: some files could be delete during drush import Bugfix: vanilla Drupal always re-synced Drupal sources https://github.com/anaxexp/drupal/issues/2 Solr versions are now frozen https://github.com/anaxexp/solr#versions Redis version is now frozen https://github.com/anaxexp/redis#versions","title":"4.1.4"},{"location":"stacks/drupal/changelog/#413","text":"New Apache container New Adminer container Nginx updated to 1.13.2 Drupal headers no longer hidden by default, configurable via Nginx/Apache environment variables MariaDB now recovers privileges in case of an error during import Drupal node container freeze","title":"4.1.3"},{"location":"stacks/drupal/changelog/#412","text":"Updated MariaDB with bug fixes PHP extension APCu is now configurable All PHP extensions are now frozen Runtime Alpine Linux libraries used by PHP extension are now frozen Dropped few environment variables Updated vanilla Drupal: 8.3.4","title":"4.1.2"},{"location":"stacks/drupal/changelog/#411","text":"Updated PHP: 7.1.5 7.1.6, 7.0.19 7.0.20 OpenSMTPD can now relay emails to 3 rd party SMTP servers OpenSMTPD rebased to Alpine 3.6 with freezed version Environment variables for OpenSMTPD configuration OpenSMTPD now writes logs to container output Increased max_allowed_packet for MariaDB daemon PHP expose header now disabled by default Nginx images rebased to Alpine 3.6 Updated vanilla Drupal","title":"4.1.1"},{"location":"stacks/drupal/changelog/#410","text":"Bug fix: git checkout in php container sometimes failed Bug fix: varnish cache flush action failed Bug fix: some environment variables missed in SSH container MariaDB: No longer lock table during backups (--single-transaction) MariaDB: Excludes cache tables data from backups. See backups section for more details Nginx: New version: 1.13.0 1.13.1 PHP: All images rebased to Alpine Linux 3.6 and now use LibreSSL instead of OpenSSL PHP: Fixed segfault caused by imagick extension PHP: MongoDB extension downgraded to 1.1.10 New AthenaPDF container \u2013 drop-in replacement for wkhtmltopdf New Rsyslog container New Node.js container","title":"4.1.0"},{"location":"stacks/drupal/changelog/#401","text":"Bug fixes and stabilization improvements Images versions freeze PHP versions freeze","title":"4.0.1"},{"location":"stacks/drupal/changelog/#400","text":"","title":"4.0.0"},{"location":"stacks/drupal/changelog/#changes-since-3x","text":"All-new revamped docker container images consistent with docker4drupal Improved performance of containers Revamped orchestration with better logging and performance Optional services now can be enabled/disabled on the working app Services configuration via environment variables from the dashboard Services' containers now can can be scaled (# of replicas) Detailed log output for orchestration tasks Redesigned scalability for cluster deployments There's no backward compatibility with stacks 3.x","title":"Changes since 3.x"},{"location":"stacks/drupal/containers/","text":"Drupal stack containers SSH and Cron containers For AnaxExp environments we additionally spin up copies of PHP services with overridden commands to run cron and ssh daemons. All environment variables added to PHP-FPM service will be automatically passed to SSHd and Crond services. Nginx Nginx can be configured with the following environment variables Default Nginx virtual host config for: Drupal 8 Drupal 7 Drupal 6 Installed nginx modules Do not gzip pages in Drupal We already gzip content on Nginx side and it works faster. Having double gzip may cause issues. Restarting nginx as default user: sudo nginx -s reload PHP endpoints For security reasons, default nginx config allows executing limited php endpoints. This is how you can add additional php endpoints: Add *.conf file to your codebase with locations definition, example: location = /custom-php-endpoint.php { fastcgi_pass php; } In nginx service configuration set new environment variable NGINX_SERVER_EXTRA_CONF_FILEPATH to your *.conf file (e.g. /var/www/html/drupal.conf ). It will be included at the end of /etc/nginx/conf.d/drupal.conf Restart the service Alternatively, you can replace your HTTP server to Apache (not recommended) that has less strict rules. XML endpoints By default nginx config requests Drupal backend when rss.xml or sitemap.xml requested. If you want to add another XML endpoint generated by Drupal just set environment variable NGINX_ALLOW_XML_ENDPOINTS to any value and restart the service. Custom config If the default drupal config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/drupal.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf Files proxy You can proxy all requests to files to (similar to what drupal module stage_file_proxy does) by adding the environment variable NGINX_DRUPAL_FILE_PROXY_URL to URL of your Drupal instance with files, e.g. http://example.com Mod pagespeed Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service. Apache Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart PHP PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR Drupal-specific Additionally, variable $DRUPAL_SITE (previous deprecated name $ANAXEXP_APP_SUBSITE ) contains Drupal site directory, e.g. default . WARNING Some environment variables used by PHP may be overridden in anaxexp.settings.php file Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug Xdebug (local) Debugging web requests Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1 Debugging CLI requests Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version: Linux, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make ) macOS, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist Windows Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost IDE configuration You must additionally configure your IDE to debug CLI requests. PHPStorm Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings NewRelic You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME . Profiling You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions. PHPUnit Inside your drupal/core directory, copy the file phpunit.xml.dist and rename it to phpunit.xml Open that file and make sure that you update SIMPLETEST_BASE_URL to http://nginx In order to make sure that your DB connection is working as well, update SIMPLETEST_DB to mysql://drupal:drupal@mariadb/drupal Drush PHP container comes with pre-installed drush and drush launcher. Drush launcher will help you use drush that comes with your project without specifying the full path to it. Drupal Console PHP container comes with installed drupal console launcher (not the same as drupal console), the launcher used to be able run drupal console without specified the full path to it. Please note that starting Drupal Console ~1.0 you have to install it manually (via composer) per project. Crond A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. SSHd A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance. Mailhog If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ). OpenSMTPD See OpenSMTPD stack documentation . MariaDB See MariaDB stack documentation . Node.js Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file. Drupal Node.js Drupal node is a container with a server app for the Node.js Integration Drupal module . You can configure Node.js via environment variables that listed at https://github.com/anaxexp/drupal-node Usage: Install Node.js Integration Drupal module on your site Visit the Node.js configuration page under the Configuration menu, and enter the connection information for your Node.js server application. Set host to node (or drupal-node for local environment) and service key can be found on [Instance] Stack Node.js Solr See Solr for Drupal stack documentation . Memcached You can check the status of memcached and its hits by running the following command. watch echo stats | nc 127.0.0.1 11211 Redis Drupal 8 Install redis module and enable redis integration under [Instance] Cache Settings page of AnaxExp dashboard to use it as an internal cache storage for Drupal. All required configuration already provided in anaxexp.settings.php file . Drupal 7 Install redis module to use it as an internal cache storage for Drupal. All required configuration already provided in anaxexp.settings.php file . Redis stack documentation Varnish Varnish ignores the following GET parameters for cache id generation: utm_source utm_medium utm_campaign utm_content gclid cx ie cof siteurl For more details see Varnish stack documentation Drupal 8 Read this article to learn how to use Varnish with Drupal 8. Drupal 7 Install and enable varnish module (use the dev version). All required configuration for this module already provided in anaxexp.settings.php file Go to Home \u00bb Administration \u00bb Configuration \u00bb Development page of Drupal website and Check Cache pages for anonymous users Check Compress cached pages. Check Aggregate and compress CSS files. Check Aggregate JavaScript files. Also, we recommend to install expire module to configure auto purge of pages when some content has been updated. After installation go to Home \u00bb Administration \u00bb Configuration \u00bb System and select External expiration at the \"Module status\" tab. Rsyslog Rsyslog can be used to stream your applications logs (watchdog). It's similar to using syslog, however there's no syslog in PHP container. Rsyslog will stream all incoming logs to a container output. Here how you can use it with Monolog: Install monolog module . Make sure all dependencies being downloaded Add new handler at monolog/monolog.services.yml : monolog.handler.rsyslog: class: Monolog\\Handler\\SyslogUdpHandler arguments: [ rsyslog ] Rebuild cache ( drush cr ) Use rsyslog handler for your channels Find your logs in rsyslog container output Read Logging in Drupal 8 to learn more. Blackfire You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation Webgrind Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Containers"},{"location":"stacks/drupal/containers/#drupal-stack-containers","text":"SSH and Cron containers For AnaxExp environments we additionally spin up copies of PHP services with overridden commands to run cron and ssh daemons. All environment variables added to PHP-FPM service will be automatically passed to SSHd and Crond services.","title":"Drupal stack containers"},{"location":"stacks/drupal/containers/#nginx","text":"Nginx can be configured with the following environment variables Default Nginx virtual host config for: Drupal 8 Drupal 7 Drupal 6 Installed nginx modules Do not gzip pages in Drupal We already gzip content on Nginx side and it works faster. Having double gzip may cause issues. Restarting nginx as default user: sudo nginx -s reload","title":"Nginx"},{"location":"stacks/drupal/containers/#php-endpoints","text":"For security reasons, default nginx config allows executing limited php endpoints. This is how you can add additional php endpoints: Add *.conf file to your codebase with locations definition, example: location = /custom-php-endpoint.php { fastcgi_pass php; } In nginx service configuration set new environment variable NGINX_SERVER_EXTRA_CONF_FILEPATH to your *.conf file (e.g. /var/www/html/drupal.conf ). It will be included at the end of /etc/nginx/conf.d/drupal.conf Restart the service Alternatively, you can replace your HTTP server to Apache (not recommended) that has less strict rules.","title":"PHP endpoints"},{"location":"stacks/drupal/containers/#xml-endpoints","text":"By default nginx config requests Drupal backend when rss.xml or sitemap.xml requested. If you want to add another XML endpoint generated by Drupal just set environment variable NGINX_ALLOW_XML_ENDPOINTS to any value and restart the service.","title":"XML endpoints"},{"location":"stacks/drupal/containers/#custom-config","text":"If the default drupal config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/drupal.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf","title":"Custom config"},{"location":"stacks/drupal/containers/#files-proxy","text":"You can proxy all requests to files to (similar to what drupal module stage_file_proxy does) by adding the environment variable NGINX_DRUPAL_FILE_PROXY_URL to URL of your Drupal instance with files, e.g. http://example.com","title":"Files proxy"},{"location":"stacks/drupal/containers/#mod-pagespeed","text":"Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service.","title":"Mod pagespeed"},{"location":"stacks/drupal/containers/#apache","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Apache"},{"location":"stacks/drupal/containers/#php","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel","title":"PHP"},{"location":"stacks/drupal/containers/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/drupal/containers/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR","title":"Environment variables"},{"location":"stacks/drupal/containers/#drupal-specific","text":"Additionally, variable $DRUPAL_SITE (previous deprecated name $ANAXEXP_APP_SUBSITE ) contains Drupal site directory, e.g. default . WARNING Some environment variables used by PHP may be overridden in anaxexp.settings.php file","title":"Drupal-specific"},{"location":"stacks/drupal/containers/#xdebug-remote","text":"Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug","title":"Xdebug (remote)"},{"location":"stacks/drupal/containers/#xdebug-local","text":"","title":"Xdebug (local)"},{"location":"stacks/drupal/containers/#debugging-web-requests","text":"Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1","title":"Debugging web requests"},{"location":"stacks/drupal/containers/#debugging-cli-requests","text":"Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version:","title":"Debugging CLI requests"},{"location":"stacks/drupal/containers/#linux-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make )","title":"Linux, Docker"},{"location":"stacks/drupal/containers/#macos-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist","title":"macOS, Docker"},{"location":"stacks/drupal/containers/#windows","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost","title":"Windows"},{"location":"stacks/drupal/containers/#ide-configuration","text":"You must additionally configure your IDE to debug CLI requests.","title":"IDE configuration"},{"location":"stacks/drupal/containers/#phpstorm","text":"Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings","title":"PHPStorm"},{"location":"stacks/drupal/containers/#newrelic","text":"You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME .","title":"NewRelic"},{"location":"stacks/drupal/containers/#profiling","text":"You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions.","title":"Profiling"},{"location":"stacks/drupal/containers/#phpunit","text":"Inside your drupal/core directory, copy the file phpunit.xml.dist and rename it to phpunit.xml Open that file and make sure that you update SIMPLETEST_BASE_URL to http://nginx In order to make sure that your DB connection is working as well, update SIMPLETEST_DB to mysql://drupal:drupal@mariadb/drupal","title":"PHPUnit"},{"location":"stacks/drupal/containers/#drush","text":"PHP container comes with pre-installed drush and drush launcher. Drush launcher will help you use drush that comes with your project without specifying the full path to it.","title":"Drush"},{"location":"stacks/drupal/containers/#drupal-console","text":"PHP container comes with installed drupal console launcher (not the same as drupal console), the launcher used to be able run drupal console without specified the full path to it. Please note that starting Drupal Console ~1.0 you have to install it manually (via composer) per project.","title":"Drupal Console"},{"location":"stacks/drupal/containers/#crond","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page.","title":"Crond"},{"location":"stacks/drupal/containers/#sshd","text":"A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance.","title":"SSHd"},{"location":"stacks/drupal/containers/#mailhog","text":"If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ).","title":"Mailhog"},{"location":"stacks/drupal/containers/#opensmtpd","text":"See OpenSMTPD stack documentation .","title":"OpenSMTPD"},{"location":"stacks/drupal/containers/#mariadb","text":"See MariaDB stack documentation .","title":"MariaDB"},{"location":"stacks/drupal/containers/#nodejs","text":"Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file.","title":"Node.js"},{"location":"stacks/drupal/containers/#drupal-nodejs","text":"Drupal node is a container with a server app for the Node.js Integration Drupal module . You can configure Node.js via environment variables that listed at https://github.com/anaxexp/drupal-node Usage: Install Node.js Integration Drupal module on your site Visit the Node.js configuration page under the Configuration menu, and enter the connection information for your Node.js server application. Set host to node (or drupal-node for local environment) and service key can be found on [Instance] Stack Node.js","title":"Drupal Node.js"},{"location":"stacks/drupal/containers/#solr","text":"See Solr for Drupal stack documentation .","title":"Solr"},{"location":"stacks/drupal/containers/#memcached","text":"You can check the status of memcached and its hits by running the following command. watch echo stats | nc 127.0.0.1 11211","title":"Memcached"},{"location":"stacks/drupal/containers/#redis","text":"","title":"Redis"},{"location":"stacks/drupal/containers/#drupal-8","text":"Install redis module and enable redis integration under [Instance] Cache Settings page of AnaxExp dashboard to use it as an internal cache storage for Drupal. All required configuration already provided in anaxexp.settings.php file .","title":"Drupal 8"},{"location":"stacks/drupal/containers/#drupal-7","text":"Install redis module to use it as an internal cache storage for Drupal. All required configuration already provided in anaxexp.settings.php file . Redis stack documentation","title":"Drupal 7"},{"location":"stacks/drupal/containers/#varnish","text":"Varnish ignores the following GET parameters for cache id generation: utm_source utm_medium utm_campaign utm_content gclid cx ie cof siteurl For more details see Varnish stack documentation","title":"Varnish"},{"location":"stacks/drupal/containers/#drupal-8_1","text":"Read this article to learn how to use Varnish with Drupal 8.","title":"Drupal 8"},{"location":"stacks/drupal/containers/#drupal-7_1","text":"Install and enable varnish module (use the dev version). All required configuration for this module already provided in anaxexp.settings.php file Go to Home \u00bb Administration \u00bb Configuration \u00bb Development page of Drupal website and Check Cache pages for anonymous users Check Compress cached pages. Check Aggregate and compress CSS files. Check Aggregate JavaScript files. Also, we recommend to install expire module to configure auto purge of pages when some content has been updated. After installation go to Home \u00bb Administration \u00bb Configuration \u00bb System and select External expiration at the \"Module status\" tab.","title":"Drupal 7"},{"location":"stacks/drupal/containers/#rsyslog","text":"Rsyslog can be used to stream your applications logs (watchdog). It's similar to using syslog, however there's no syslog in PHP container. Rsyslog will stream all incoming logs to a container output. Here how you can use it with Monolog: Install monolog module . Make sure all dependencies being downloaded Add new handler at monolog/monolog.services.yml : monolog.handler.rsyslog: class: Monolog\\Handler\\SyslogUdpHandler arguments: [ rsyslog ] Rebuild cache ( drush cr ) Use rsyslog handler for your channels Find your logs in rsyslog container output Read Logging in Drupal 8 to learn more.","title":"Rsyslog"},{"location":"stacks/drupal/containers/#blackfire","text":"You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation","title":"Blackfire"},{"location":"stacks/drupal/containers/#webgrind","text":"Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Webgrind"},{"location":"stacks/drupal/local/","text":"Local environment with Docker4Drupal Docker4Drupal is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows. Requirements Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose Usage Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. There are 2 options how to use docker4drupal \u2013 you can either run vanilla Drupal from the image or mount your own Drupal codebase: Vanilla Drupal Clone docker4drupal repository and switch to the latest stable tag or download/unpack the source code from the latest release Optional: for Drupal 7 or 6 comment out corresponding DRUPAL_TAG and NGINX_TAG in .env file Configure domains From project root directory run docker-compose up -d or make up to start containers. Give it 10-20 seconds to initialize after the start That's it! Proceed with Drupal installation at http://drupal.docker.localhost:8000 . Default database user, password and database name are all drupal , database host is mariadb You can see status of your containers and their logs via portainer: http://portainer.drupal.docker.localhost:8000 Mount my codebase Download docker4drupal.tar.gz from the latest stable release and unpack to your Drupal project root. If you choose to clone the repository delete docker-compose.override.yml as it's used to deploy vanilla Drupal Ensure NGINX_SERVER_ROOT (or APACHE_SERVER_ROOT ) is correct, by default set to /var/www/html/web for composer-based projects where Drupal is in web subdirectory Ensure database access settings in your settings.php corresponds to values in .env file, e.g.: $databases[ default ][ default ] = array ( database = drupal , // same as $DB_NAME username = drupal , // same as $DB_USER password = drupal , // same as $DB_PASSWORD host = mariadb , // same as $DB_HOST driver = mysql , // same as $DB_DRIVER port = 3306 , // different for PostgreSQL namespace = Drupal\\\\Core\\\\Database\\\\Driver\\\\mysql , // different for PostgreSQL prefix = , ); Configure domains Optional: for Drupal 7 or 6 comment out corresponding PHP_TAG and NGINX_TAG in your .env file Optional: import existing database Optional: uncomment lines in the compose file to run redis, solr, varnish, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your drupal website should be up and running at http://drupal.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.drupal.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the docker4drupal.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page Domains Traefik container used for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to drupal.docker.localhost , you can change it in .env file. Add 127.0.0.1 drupal.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://drupal.docker.localhost:8000 pma http://pma.drupal.docker.localhost:8000 adminer http://adminer.drupal.docker.localhost:8000 mailhog http://mailhog.drupal.docker.localhost:8000 solr http://solr.drupal.docker.localhost:8000 nodejs http://nodejs.drupal.docker.localhost:8000 node http://front.drupal.docker.localhost:8000 varnish http://varnish.drupal.docker.localhost:8000 portainer http://portainer.drupal.docker.localhost:8000 webgrind http://webgrind.drupal.docker.localhost:8000 Database import and export MariaDB See MariaDB stack documentation PostgreSQL See PostgreSQL stack documentation Make commands Basic: We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service Drupal-specific: make drush [command] Execute drush command (runs with -r /var/www/html/web, you can override it via DRUPAL_ROOT=PATH) Docker for mac There two major problems macOS users face with when using Docker for mac: macOS permissions issues To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user. Bind mounts performance Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved. User-guided caching Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\". Docker-sync Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page . Permissions issues You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container. Linux Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions. macOS Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user. Windows Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root Different uid/gid? You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default) Running multiple Projects Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Local environment"},{"location":"stacks/drupal/local/#local-environment-with-docker4drupal","text":"Docker4Drupal is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows.","title":"Local environment with Docker4Drupal"},{"location":"stacks/drupal/local/#requirements","text":"Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose","title":"Requirements"},{"location":"stacks/drupal/local/#usage","text":"Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. There are 2 options how to use docker4drupal \u2013 you can either run vanilla Drupal from the image or mount your own Drupal codebase:","title":"Usage"},{"location":"stacks/drupal/local/#vanilla-drupal","text":"Clone docker4drupal repository and switch to the latest stable tag or download/unpack the source code from the latest release Optional: for Drupal 7 or 6 comment out corresponding DRUPAL_TAG and NGINX_TAG in .env file Configure domains From project root directory run docker-compose up -d or make up to start containers. Give it 10-20 seconds to initialize after the start That's it! Proceed with Drupal installation at http://drupal.docker.localhost:8000 . Default database user, password and database name are all drupal , database host is mariadb You can see status of your containers and their logs via portainer: http://portainer.drupal.docker.localhost:8000","title":"Vanilla Drupal"},{"location":"stacks/drupal/local/#mount-my-codebase","text":"Download docker4drupal.tar.gz from the latest stable release and unpack to your Drupal project root. If you choose to clone the repository delete docker-compose.override.yml as it's used to deploy vanilla Drupal Ensure NGINX_SERVER_ROOT (or APACHE_SERVER_ROOT ) is correct, by default set to /var/www/html/web for composer-based projects where Drupal is in web subdirectory Ensure database access settings in your settings.php corresponds to values in .env file, e.g.: $databases[ default ][ default ] = array ( database = drupal , // same as $DB_NAME username = drupal , // same as $DB_USER password = drupal , // same as $DB_PASSWORD host = mariadb , // same as $DB_HOST driver = mysql , // same as $DB_DRIVER port = 3306 , // different for PostgreSQL namespace = Drupal\\\\Core\\\\Database\\\\Driver\\\\mysql , // different for PostgreSQL prefix = , ); Configure domains Optional: for Drupal 7 or 6 comment out corresponding PHP_TAG and NGINX_TAG in your .env file Optional: import existing database Optional: uncomment lines in the compose file to run redis, solr, varnish, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your drupal website should be up and running at http://drupal.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.drupal.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the docker4drupal.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page","title":"Mount my codebase"},{"location":"stacks/drupal/local/#domains","text":"Traefik container used for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to drupal.docker.localhost , you can change it in .env file. Add 127.0.0.1 drupal.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://drupal.docker.localhost:8000 pma http://pma.drupal.docker.localhost:8000 adminer http://adminer.drupal.docker.localhost:8000 mailhog http://mailhog.drupal.docker.localhost:8000 solr http://solr.drupal.docker.localhost:8000 nodejs http://nodejs.drupal.docker.localhost:8000 node http://front.drupal.docker.localhost:8000 varnish http://varnish.drupal.docker.localhost:8000 portainer http://portainer.drupal.docker.localhost:8000 webgrind http://webgrind.drupal.docker.localhost:8000","title":"Domains"},{"location":"stacks/drupal/local/#database-import-and-export","text":"","title":"Database import and export"},{"location":"stacks/drupal/local/#mariadb","text":"See MariaDB stack documentation","title":"MariaDB"},{"location":"stacks/drupal/local/#postgresql","text":"See PostgreSQL stack documentation","title":"PostgreSQL"},{"location":"stacks/drupal/local/#make-commands","text":"Basic: We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service Drupal-specific: make drush [command] Execute drush command (runs with -r /var/www/html/web, you can override it via DRUPAL_ROOT=PATH)","title":"Make commands"},{"location":"stacks/drupal/local/#docker-for-mac","text":"There two major problems macOS users face with when using Docker for mac:","title":"Docker for mac"},{"location":"stacks/drupal/local/#macos-permissions-issues","text":"To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user.","title":"macOS permissions issues"},{"location":"stacks/drupal/local/#bind-mounts-performance","text":"Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved.","title":"Bind mounts performance"},{"location":"stacks/drupal/local/#user-guided-caching","text":"Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\".","title":"User-guided caching"},{"location":"stacks/drupal/local/#docker-sync","text":"Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page .","title":"Docker-sync"},{"location":"stacks/drupal/local/#permissions-issues","text":"You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container.","title":"Permissions issues"},{"location":"stacks/drupal/local/#linux","text":"Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions.","title":"Linux"},{"location":"stacks/drupal/local/#macos","text":"Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user.","title":"macOS"},{"location":"stacks/drupal/local/#windows","text":"Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root","title":"Windows"},{"location":"stacks/drupal/local/#different-uidgid","text":"You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default)","title":"Different uid/gid?"},{"location":"stacks/drupal/local/#running-multiple-projects","text":"Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Running multiple Projects"},{"location":"stacks/elasticsearch/","text":"Elasticsearch stack documentation Elasticsearch and Kibana can be configured via environment variables: https://github.com/anaxexp/elasticsearch#environment-variables https://github.com/anaxexp/kibana#environment-variables Changelog 2.0.1 Default memory request set to 512m 2.0.0 Elasticsearch and Kibana images rebuilt to Alpine Linux Elasticsearch and Kibana 5 updated to 5.6.6 New line 6 (6.1) for Elasticsearch and Kibana, now default Updated the list of environment variables for configuration, see http://github.com/anaxexp/elasticsearch and http://github.com/anaxexp/kibana 1.0.1 Make elasticsearch accessible from the outside. 1.0.0 Initial release","title":"Elasticsearch"},{"location":"stacks/elasticsearch/#elasticsearch-stack-documentation","text":"Elasticsearch and Kibana can be configured via environment variables: https://github.com/anaxexp/elasticsearch#environment-variables https://github.com/anaxexp/kibana#environment-variables","title":"Elasticsearch stack documentation"},{"location":"stacks/elasticsearch/#changelog","text":"","title":"Changelog"},{"location":"stacks/elasticsearch/#201","text":"Default memory request set to 512m","title":"2.0.1"},{"location":"stacks/elasticsearch/#200","text":"Elasticsearch and Kibana images rebuilt to Alpine Linux Elasticsearch and Kibana 5 updated to 5.6.6 New line 6 (6.1) for Elasticsearch and Kibana, now default Updated the list of environment variables for configuration, see http://github.com/anaxexp/elasticsearch and http://github.com/anaxexp/kibana","title":"2.0.0"},{"location":"stacks/elasticsearch/#101","text":"Make elasticsearch accessible from the outside.","title":"1.0.1"},{"location":"stacks/elasticsearch/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/gitlab/","text":"GitLab stack documentation GitLab can be configured with the following environment variables Credentials We use email from your AnaxExp account as GitLab root user email. When you deploy GitLab for the first time you will be asked to change your root password. Backups We do not yet provide orchestration for backup / restore from the dashboard. However, you still can do it manually. Access your GitLab (Unicorn) container and run bundle exec rake gitlab:backup:create to create a backup. Run bundle exec rake gitlab:backup:restore to restore your backup. See the GitLab backup and restore documentation for more details. SSL Access to the gitlab application can be secured using SSL, follow these steps: Add a custom domain and mark it as primary Acquire an SSL certificate for this domain Add an environment variable GITLAB_HTTPS=true to GitLab (Unicorn) and Nginx services Logs All services provided with the stack configured to stream their logs to a container output. It's a common practice for docker containers. Those logs are not persistent and available until the next container restart. You can either stream logs in real-time via our dashboard or via CLI. To get logs via CLI copy Show logs command from [Instance] Stack [Service] and execute it on the host server of an instance. Modify kubectl logs command flags to your needs. Container registry Follow these steps to connect docker container registry to your GitLab instance: Enable Docker registry service in your application stack Assign a custom domain to Docker registry from Domains tab Acquire Let's Encrypt certificate for this domain Add the following environment variables (replace [TOKEN]) to GitLab (Unicorn) service in your stack and redeploy it: GITLAB_REGISTRY_ENABLED: true GITLAB_REGISTRY_HOST: [YOUR REGISTRY DOMAIN (e.g. registry.example.com)] Once the deployment from the previous step has completed, add the following environment variables (replace [TOKEN]) to your Docker registry service: REGISTRY_AUTH_TOKEN_ROOTCERTBUNDLE: /certs/registry-auth.crt REGISTRY_AUTH_TOKEN_REALM: http://[YOUR GITLAB PRIMARY DOMAIN]/jwt/auth REGISTRY_AUTH_TOKEN_SERVICE: container_registry REGISTRY_AUTH_TOKEN_ISSUER: gitlab-issuer GitLab CI Deploy a new GitLab Runner application. The runner will use docker executor by default. Add the following environment variables to register your runner in your Gitlab instance: CI_SERVER_URL: [YOUR GITLAB URL] REGISTRATION_TOKEN: [YOUR GITLAB REGISTRATION TOKEN] You can find the token from /admin/runners if you want to register it as a shared runner. Or visit Settings - CI/CD page of your GitLab project to register it as a specific. GitLab runner Add CI_SERVER_URL and REGISTRATION_TOKEN environment variables to register your runner in your Gitlab instance. All environment variables available for configuration can be found at https://github.com/anaxexp/gitlab-runner Known issues GitLab is not designed to be run with a \"one process per container\" approach. As a result some functionality may not be available, such as GitLab can't detect Sidekiq background jobs (e.g. Admin Area Background Jobs ) Logs ( Admin Area Logs ) not available, but you still can get logs from containers' output via AnaxExp dashboard Mail delivery Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. GitLab Pages Prerequisites: You have a custom domain your gitlab pages will be served under (e.g. *.example.io) You have configured a wildcard DNS record for that domain You have connected at least one shared GitLab Runner Follow these steps to connect docker container registry to your GitLab instance: Enable GitLab Pages service in your application stack Add your custom wildcard domain with * (e.g. *.example.io), attach it to pages service Add the following environment variables to GitLab (Unicorn) service of your stack: GITLAB_PAGES_ENABLED: true GITLAB_PAGES_HOST: [YOUR CUSTOM DOMAIN WITHOUT * (e.g. example.io)] Redeploy your stack Deploy your GitLab pages repository (you can fork one of the official examples ) Manually run a pipeline for the first time in order to trigger the job process If the job has completed successfully, you can find your pages domain under your project's Settings Pages For more details visit https://about.gitlab.com/features/pages/ Reply by email GitLab can be set up to allow users to comment on issues and merge requests by replying to notification emails. Learn more about this feature from the official documentation . To enable this functionality you should: Enable Mailroom service in your stack Add environment variables listed below to GitLab (Unicorn) service: Variable Description GITLAB_INCOMING_EMAIL Set to true to enable GITLAB_INCOMING_EMAIL_ADDRESS The email address including the %{key} placeholder that will be replaced to reference the item being replied to. The placeholder can be omitted but if present, it must appear in the \"user\" part of the address (before the @ ). GITLAB_INCOMING_EMAIL_USER Email account username. With third party providers, this is usually the full email address. With self-hosted email servers, this is usually the user part of the email address. GITLAB_INCOMING_EMAIL_PASSWORD Email account password GITLAB_INCOMING_EMAIL_HOST IMAP server host GITLAB_INCOMING_EMAIL_PORT IMAP server port Gmail If you want to use Gmail / Google Apps with Reply by email, make sure you have IMAP access enabled. Configuration for Gmail / Google Apps, assumes mailbox gitlab-incoming@gmail.com : GITLAB_INCOMING_EMAIL: true GITLAB_INCOMING_EMAIL_ADDRESS: gitlab-incoming+%{key}@gmail.com GITLAB_INCOMING_EMAIL_USER: gitlab-incoming@gmail.com GITLAB_INCOMING_EMAIL_PASSWORD: my-password GITLAB_INCOMING_EMAIL_HOST: imap.gmail.com GITLAB_INCOMING_EMAIL_PORT: 993 Microsoft Exchange Configuration for Microsoft Exchange mail server with IMAP enabled, assumes mailbox incoming@exchange.example.com : GITLAB_INCOMING_EMAIL: true GITLAB_INCOMING_EMAIL_ADDRESS: incoming@exchange.example.com GITLAB_INCOMING_EMAIL_USER: incoming@ad-domain.example.com GITLAB_INCOMING_EMAIL_PASSWORD: my-password GITLAB_INCOMING_EMAIL_HOST: exchange.example.com GITLAB_INCOMING_EMAIL_PORT: 993","title":"Overview"},{"location":"stacks/gitlab/#gitlab-stack-documentation","text":"GitLab can be configured with the following environment variables","title":"GitLab stack documentation"},{"location":"stacks/gitlab/#credentials","text":"We use email from your AnaxExp account as GitLab root user email. When you deploy GitLab for the first time you will be asked to change your root password.","title":"Credentials"},{"location":"stacks/gitlab/#backups","text":"We do not yet provide orchestration for backup / restore from the dashboard. However, you still can do it manually. Access your GitLab (Unicorn) container and run bundle exec rake gitlab:backup:create to create a backup. Run bundle exec rake gitlab:backup:restore to restore your backup. See the GitLab backup and restore documentation for more details.","title":"Backups"},{"location":"stacks/gitlab/#ssl","text":"Access to the gitlab application can be secured using SSL, follow these steps: Add a custom domain and mark it as primary Acquire an SSL certificate for this domain Add an environment variable GITLAB_HTTPS=true to GitLab (Unicorn) and Nginx services","title":"SSL"},{"location":"stacks/gitlab/#logs","text":"All services provided with the stack configured to stream their logs to a container output. It's a common practice for docker containers. Those logs are not persistent and available until the next container restart. You can either stream logs in real-time via our dashboard or via CLI. To get logs via CLI copy Show logs command from [Instance] Stack [Service] and execute it on the host server of an instance. Modify kubectl logs command flags to your needs.","title":"Logs"},{"location":"stacks/gitlab/#container-registry","text":"Follow these steps to connect docker container registry to your GitLab instance: Enable Docker registry service in your application stack Assign a custom domain to Docker registry from Domains tab Acquire Let's Encrypt certificate for this domain Add the following environment variables (replace [TOKEN]) to GitLab (Unicorn) service in your stack and redeploy it: GITLAB_REGISTRY_ENABLED: true GITLAB_REGISTRY_HOST: [YOUR REGISTRY DOMAIN (e.g. registry.example.com)] Once the deployment from the previous step has completed, add the following environment variables (replace [TOKEN]) to your Docker registry service: REGISTRY_AUTH_TOKEN_ROOTCERTBUNDLE: /certs/registry-auth.crt REGISTRY_AUTH_TOKEN_REALM: http://[YOUR GITLAB PRIMARY DOMAIN]/jwt/auth REGISTRY_AUTH_TOKEN_SERVICE: container_registry REGISTRY_AUTH_TOKEN_ISSUER: gitlab-issuer","title":"Container registry"},{"location":"stacks/gitlab/#gitlab-ci","text":"Deploy a new GitLab Runner application. The runner will use docker executor by default. Add the following environment variables to register your runner in your Gitlab instance: CI_SERVER_URL: [YOUR GITLAB URL] REGISTRATION_TOKEN: [YOUR GITLAB REGISTRATION TOKEN] You can find the token from /admin/runners if you want to register it as a shared runner. Or visit Settings - CI/CD page of your GitLab project to register it as a specific.","title":"GitLab CI"},{"location":"stacks/gitlab/#gitlab-runner","text":"Add CI_SERVER_URL and REGISTRATION_TOKEN environment variables to register your runner in your Gitlab instance. All environment variables available for configuration can be found at https://github.com/anaxexp/gitlab-runner","title":"GitLab runner"},{"location":"stacks/gitlab/#known-issues","text":"GitLab is not designed to be run with a \"one process per container\" approach. As a result some functionality may not be available, such as GitLab can't detect Sidekiq background jobs (e.g. Admin Area Background Jobs ) Logs ( Admin Area Logs ) not available, but you still can get logs from containers' output via AnaxExp dashboard","title":"Known issues"},{"location":"stacks/gitlab/#mail-delivery","text":"Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application.","title":"Mail delivery"},{"location":"stacks/gitlab/#gitlab-pages","text":"Prerequisites: You have a custom domain your gitlab pages will be served under (e.g. *.example.io) You have configured a wildcard DNS record for that domain You have connected at least one shared GitLab Runner Follow these steps to connect docker container registry to your GitLab instance: Enable GitLab Pages service in your application stack Add your custom wildcard domain with * (e.g. *.example.io), attach it to pages service Add the following environment variables to GitLab (Unicorn) service of your stack: GITLAB_PAGES_ENABLED: true GITLAB_PAGES_HOST: [YOUR CUSTOM DOMAIN WITHOUT * (e.g. example.io)] Redeploy your stack Deploy your GitLab pages repository (you can fork one of the official examples ) Manually run a pipeline for the first time in order to trigger the job process If the job has completed successfully, you can find your pages domain under your project's Settings Pages For more details visit https://about.gitlab.com/features/pages/","title":"GitLab Pages"},{"location":"stacks/gitlab/#reply-by-email","text":"GitLab can be set up to allow users to comment on issues and merge requests by replying to notification emails. Learn more about this feature from the official documentation . To enable this functionality you should: Enable Mailroom service in your stack Add environment variables listed below to GitLab (Unicorn) service: Variable Description GITLAB_INCOMING_EMAIL Set to true to enable GITLAB_INCOMING_EMAIL_ADDRESS The email address including the %{key} placeholder that will be replaced to reference the item being replied to. The placeholder can be omitted but if present, it must appear in the \"user\" part of the address (before the @ ). GITLAB_INCOMING_EMAIL_USER Email account username. With third party providers, this is usually the full email address. With self-hosted email servers, this is usually the user part of the email address. GITLAB_INCOMING_EMAIL_PASSWORD Email account password GITLAB_INCOMING_EMAIL_HOST IMAP server host GITLAB_INCOMING_EMAIL_PORT IMAP server port","title":"Reply by email"},{"location":"stacks/gitlab/#gmail","text":"If you want to use Gmail / Google Apps with Reply by email, make sure you have IMAP access enabled. Configuration for Gmail / Google Apps, assumes mailbox gitlab-incoming@gmail.com : GITLAB_INCOMING_EMAIL: true GITLAB_INCOMING_EMAIL_ADDRESS: gitlab-incoming+%{key}@gmail.com GITLAB_INCOMING_EMAIL_USER: gitlab-incoming@gmail.com GITLAB_INCOMING_EMAIL_PASSWORD: my-password GITLAB_INCOMING_EMAIL_HOST: imap.gmail.com GITLAB_INCOMING_EMAIL_PORT: 993","title":"Gmail"},{"location":"stacks/gitlab/#microsoft-exchange","text":"Configuration for Microsoft Exchange mail server with IMAP enabled, assumes mailbox incoming@exchange.example.com : GITLAB_INCOMING_EMAIL: true GITLAB_INCOMING_EMAIL_ADDRESS: incoming@exchange.example.com GITLAB_INCOMING_EMAIL_USER: incoming@ad-domain.example.com GITLAB_INCOMING_EMAIL_PASSWORD: my-password GITLAB_INCOMING_EMAIL_HOST: exchange.example.com GITLAB_INCOMING_EMAIL_PORT: 993","title":"Microsoft Exchange"},{"location":"stacks/gitlab/changelog/","text":"Changelog GitLab stack changelog 0.4.0 GitLab CE updated to 10.5.2 Default memory request set to ~1.5G 0.3.0 GitLab updated to 10.3.5 0.2.0 GitLab updated to 10.2.5 GitLab rebased to ruby 2.3.6 and alpine 3.7 Fixed bug with backup creation (pgclient updated to 10.1) 0.1.1 GitLab CE updated to 10.2.4 (patch release) 0.1.0 Initial release GitLab runner stack changelog 0.2.0 GitLab Runner updated to 10.5.0 Default memory request set to 8m 0.1.0 Initial release","title":"Changelog"},{"location":"stacks/gitlab/changelog/#changelog","text":"","title":"Changelog"},{"location":"stacks/gitlab/changelog/#gitlab-stack-changelog","text":"","title":"GitLab stack changelog"},{"location":"stacks/gitlab/changelog/#040","text":"GitLab CE updated to 10.5.2 Default memory request set to ~1.5G","title":"0.4.0"},{"location":"stacks/gitlab/changelog/#030","text":"GitLab updated to 10.3.5","title":"0.3.0"},{"location":"stacks/gitlab/changelog/#020","text":"GitLab updated to 10.2.5 GitLab rebased to ruby 2.3.6 and alpine 3.7 Fixed bug with backup creation (pgclient updated to 10.1)","title":"0.2.0"},{"location":"stacks/gitlab/changelog/#011","text":"GitLab CE updated to 10.2.4 (patch release)","title":"0.1.1"},{"location":"stacks/gitlab/changelog/#010","text":"Initial release","title":"0.1.0"},{"location":"stacks/gitlab/changelog/#gitlab-runner-stack-changelog","text":"","title":"GitLab runner stack changelog"},{"location":"stacks/gitlab/changelog/#020_1","text":"GitLab Runner updated to 10.5.0 Default memory request set to 8m","title":"0.2.0"},{"location":"stacks/gitlab/changelog/#010_1","text":"Initial release","title":"0.1.0"},{"location":"stacks/html/","text":"HTML stack documentation Deployment CI/CD CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: HTTP server: nginx or apache Containers Nginx Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Restarting nginx as default user: sudo nginx -s reload Apache Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart Changelog 0.1.0 Initial release","title":"HTML"},{"location":"stacks/html/#html-stack-documentation","text":"","title":"HTML stack documentation"},{"location":"stacks/html/#deployment","text":"","title":"Deployment"},{"location":"stacks/html/#cicd","text":"CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: HTTP server: nginx or apache","title":"CI/CD"},{"location":"stacks/html/#containers","text":"","title":"Containers"},{"location":"stacks/html/#nginx","text":"Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Restarting nginx as default user: sudo nginx -s reload","title":"Nginx"},{"location":"stacks/html/#apache","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Apache"},{"location":"stacks/html/#changelog","text":"","title":"Changelog"},{"location":"stacks/html/#010","text":"Initial release","title":"0.1.0"},{"location":"stacks/jenkins/","text":"Jenkins stack documentation Default jenkins user is admin . You can find automatically generated Jenkins password from [Instance] Stack Jenkins Jenkins can be configured with the following environment variables Pre-installed plugins Changelog 2.0.0 1.0.1 Default memory request set to 128m 1.0.0 Initial release","title":"Jenkins"},{"location":"stacks/jenkins/#jenkins-stack-documentation","text":"Default jenkins user is admin . You can find automatically generated Jenkins password from [Instance] Stack Jenkins Jenkins can be configured with the following environment variables Pre-installed plugins","title":"Jenkins stack documentation"},{"location":"stacks/jenkins/#changelog","text":"","title":"Changelog"},{"location":"stacks/jenkins/#200","text":"","title":"2.0.0"},{"location":"stacks/jenkins/#101","text":"Default memory request set to 128m","title":"1.0.1"},{"location":"stacks/jenkins/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/mariadb/","text":"MariaDB stack documentation MariaDB can be configured with the following environment variables Calculating the optimal size of innodb_buffer_pool_size Run the following query to get the recommend innodb buffer pool size: SELECT CONCAT ( CEILING ( RIBPS / POWER ( 1024 , pw )), SUBSTR ( KMGT , pw + 1 , 1 )) Recommended_InnoDB_Buffer_Pool_Size FROM ( SELECT RIBPS , FLOOR ( LOG ( RIBPS ) / LOG ( 1024 )) pw FROM ( SELECT SUM ( data_length + index_length ) * 1 . 1 * growth RIBPS FROM information_schema . tables AAA , ( SELECT 1 . 25 growth ) BBB WHERE ENGINE = InnoDB ) AA ) A ; Source: by RolandoMySQLDBA from the answer on dba stackexchange . External access If you want to access the database outside of the AnaxExp infrastructure you will have to use SSH tunnel via the main container: Set up SSH tunnel on port 53306 (you can change it). You can find [SSH Port] on Instance Stack SSH page. For MariaDB (port 3306 by default) use the following command: $ ssh -L 53306 :mariadb:3306 -p [ SSH Port ] anaxexp@ [ Server IP ] -N Connect to the database via the tunnel on port 53306 (replace [tokens] ): $ mysql --protocol = TCP -P53306 -u [ USER ] -p [ PASSWORD ] [ DATABASE ] Local environment Import existing database if you want to import your database, uncomment the line for mariadb-init volume in your compose file. Create the volume directory ./mariadb-init in the same directory as the compose file and put there your .sql .sql.gz .sh file(s). All SQL files will be automatically imported once MariaDB container has started. Export Exporting all databases: docker-compose exec mariadb sh -c exec mysqldump --all-databases -uroot -p root-password databases.sql Exporting a specific database: docker-compose exec mariadb sh -c exec mysqldump -uroot -p root-password my-db my-db.sql Changelog 2.2.0 New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2757Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container. See stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application Default innodb_buffer_pool_instances set to 1 2.1.0 Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Default memory request set to 64m 2.0.1 Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 2.0.0 2.0.0 New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Health check timeout increased to 30 seconds Deployment strategy no longer can be changed Optimized default values in my.cnf New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice (10) and ionice (7) Improved error handling in import action 1.0.4 MariaDB updated to 10.1.26 1.0.3 Directory __MACOSX now excluded from import archive 1.0.2 MariaDB now recovers privileges in case of an error during import 1.0.1 Priveleges are now revoked for a regular user during import Bug fix: sometimes tables weren't ignored during backup 1.0.0 Initial release","title":"MariaDB"},{"location":"stacks/mariadb/#mariadb-stack-documentation","text":"MariaDB can be configured with the following environment variables","title":"MariaDB stack documentation"},{"location":"stacks/mariadb/#calculating-the-optimal-size-of-innodb_buffer_pool_size","text":"Run the following query to get the recommend innodb buffer pool size: SELECT CONCAT ( CEILING ( RIBPS / POWER ( 1024 , pw )), SUBSTR ( KMGT , pw + 1 , 1 )) Recommended_InnoDB_Buffer_Pool_Size FROM ( SELECT RIBPS , FLOOR ( LOG ( RIBPS ) / LOG ( 1024 )) pw FROM ( SELECT SUM ( data_length + index_length ) * 1 . 1 * growth RIBPS FROM information_schema . tables AAA , ( SELECT 1 . 25 growth ) BBB WHERE ENGINE = InnoDB ) AA ) A ; Source: by RolandoMySQLDBA from the answer on dba stackexchange .","title":"Calculating the optimal size of innodb_buffer_pool_size"},{"location":"stacks/mariadb/#external-access","text":"If you want to access the database outside of the AnaxExp infrastructure you will have to use SSH tunnel via the main container: Set up SSH tunnel on port 53306 (you can change it). You can find [SSH Port] on Instance Stack SSH page. For MariaDB (port 3306 by default) use the following command: $ ssh -L 53306 :mariadb:3306 -p [ SSH Port ] anaxexp@ [ Server IP ] -N Connect to the database via the tunnel on port 53306 (replace [tokens] ): $ mysql --protocol = TCP -P53306 -u [ USER ] -p [ PASSWORD ] [ DATABASE ]","title":"External access"},{"location":"stacks/mariadb/#local-environment","text":"","title":"Local environment"},{"location":"stacks/mariadb/#import-existing-database","text":"if you want to import your database, uncomment the line for mariadb-init volume in your compose file. Create the volume directory ./mariadb-init in the same directory as the compose file and put there your .sql .sql.gz .sh file(s). All SQL files will be automatically imported once MariaDB container has started.","title":"Import existing database"},{"location":"stacks/mariadb/#export","text":"Exporting all databases: docker-compose exec mariadb sh -c exec mysqldump --all-databases -uroot -p root-password databases.sql Exporting a specific database: docker-compose exec mariadb sh -c exec mysqldump -uroot -p root-password my-db my-db.sql","title":"Export"},{"location":"stacks/mariadb/#changelog","text":"","title":"Changelog"},{"location":"stacks/mariadb/#220","text":"New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2757Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container. See stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application Default innodb_buffer_pool_instances set to 1","title":"2.2.0"},{"location":"stacks/mariadb/#210","text":"Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Default memory request set to 64m","title":"2.1.0"},{"location":"stacks/mariadb/#201","text":"Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 2.0.0","title":"2.0.1"},{"location":"stacks/mariadb/#200","text":"New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Health check timeout increased to 30 seconds Deployment strategy no longer can be changed Optimized default values in my.cnf New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice (10) and ionice (7) Improved error handling in import action","title":"2.0.0"},{"location":"stacks/mariadb/#104","text":"MariaDB updated to 10.1.26","title":"1.0.4"},{"location":"stacks/mariadb/#103","text":"Directory __MACOSX now excluded from import archive","title":"1.0.3"},{"location":"stacks/mariadb/#102","text":"MariaDB now recovers privileges in case of an error during import","title":"1.0.2"},{"location":"stacks/mariadb/#101","text":"Priveleges are now revoked for a regular user during import Bug fix: sometimes tables weren't ignored during backup","title":"1.0.1"},{"location":"stacks/mariadb/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/matomo/","text":"Matomo stack documentation Setup Use the following credentials during the initial setup on the Database Setup step: Database Server: matomo Login: matomo Password: Copy the value of MYSQL_PASSWORD from [Instance] Stack Database page Database Name: matomo Redis integration You can use redis to store Matomo cachet (by default stored in the local filesystem): Enable redis service in your application stack from [Instance] Stack page Copy your instance UUID from [Instance] Settings page Access the server hosting your matomo instance as root and append /srv/anaxexp/instances/[INSTANCE UUID]/app/config/config.ini.php file with the following values: [Cache] backend = chained [ChainedCache] backends[] = array backends[] = redis [RedisCache] host = redis port = 6379 timeout = 0.0 password = Copy the value of `REDIS_PASSWORD` from `[Instance] Stack Redis page` database = 14 Mail delivery Go to Settings System General settings Email server settings in your Matomo instance. Specify opensmtpd as server address and 25 as port. For more details how to configure guaranteed email delivery see OpenSMTPD stack documentation No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Cron By default we run the following cron command from crond container every hour: /usr/local/bin/php /var/www/html/console core:archive --url=${ANAXEXP_URL_PRIMARY} Containers PHP PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR Crond A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. $ANAXEXP_HOST_PRIMARY is a domain marked as primary. SSHd A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance. OpenSMTPD MariaDB Redis Changelog 0.3.1 Matomo updated to 3.5.1 Added Nginx 1.14, 1.15 MariaBD: Added new version 10.3 Version 10.2 updated to 10.2.15 Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage PHP error reporting now exludes strict and deprecated errors 0.3.0 Matomo updated to 3.5.0 Added SSHD container Matomo image rebased to latest stable PHP 7.1 image ( anaxexp/php:7.1-4.4.2 ) 0.2.1 Bugfix: cron task failed 0.2.0 PHP updated to 7.1.15 (security updates) Redis service added Docs: added instructions for redis and email configuration Bugfix: insufficient permissions for plugins install 0.1.0 Initial release","title":"Matomo"},{"location":"stacks/matomo/#matomo-stack-documentation","text":"","title":"Matomo stack documentation"},{"location":"stacks/matomo/#setup","text":"Use the following credentials during the initial setup on the Database Setup step: Database Server: matomo Login: matomo Password: Copy the value of MYSQL_PASSWORD from [Instance] Stack Database page Database Name: matomo","title":"Setup"},{"location":"stacks/matomo/#redis-integration","text":"You can use redis to store Matomo cachet (by default stored in the local filesystem): Enable redis service in your application stack from [Instance] Stack page Copy your instance UUID from [Instance] Settings page Access the server hosting your matomo instance as root and append /srv/anaxexp/instances/[INSTANCE UUID]/app/config/config.ini.php file with the following values: [Cache] backend = chained [ChainedCache] backends[] = array backends[] = redis [RedisCache] host = redis port = 6379 timeout = 0.0 password = Copy the value of `REDIS_PASSWORD` from `[Instance] Stack Redis page` database = 14","title":"Redis integration"},{"location":"stacks/matomo/#mail-delivery","text":"Go to Settings System General settings Email server settings in your Matomo instance. Specify opensmtpd as server address and 25 as port. For more details how to configure guaranteed email delivery see OpenSMTPD stack documentation No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services .","title":"Mail delivery"},{"location":"stacks/matomo/#cron","text":"By default we run the following cron command from crond container every hour: /usr/local/bin/php /var/www/html/console core:archive --url=${ANAXEXP_URL_PRIMARY}","title":"Cron"},{"location":"stacks/matomo/#containers","text":"","title":"Containers"},{"location":"stacks/matomo/#php","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel","title":"PHP"},{"location":"stacks/matomo/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/matomo/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR","title":"Environment variables"},{"location":"stacks/matomo/#crond","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. $ANAXEXP_HOST_PRIMARY is a domain marked as primary.","title":"Crond"},{"location":"stacks/matomo/#sshd","text":"A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance.","title":"SSHd"},{"location":"stacks/matomo/#opensmtpd","text":"","title":"OpenSMTPD"},{"location":"stacks/matomo/#mariadb","text":"","title":"MariaDB"},{"location":"stacks/matomo/#redis","text":"","title":"Redis"},{"location":"stacks/matomo/#changelog","text":"","title":"Changelog"},{"location":"stacks/matomo/#031","text":"Matomo updated to 3.5.1 Added Nginx 1.14, 1.15 MariaBD: Added new version 10.3 Version 10.2 updated to 10.2.15 Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage PHP error reporting now exludes strict and deprecated errors","title":"0.3.1"},{"location":"stacks/matomo/#030","text":"Matomo updated to 3.5.0 Added SSHD container Matomo image rebased to latest stable PHP 7.1 image ( anaxexp/php:7.1-4.4.2 )","title":"0.3.0"},{"location":"stacks/matomo/#021","text":"Bugfix: cron task failed","title":"0.2.1"},{"location":"stacks/matomo/#020","text":"PHP updated to 7.1.15 (security updates) Redis service added Docs: added instructions for redis and email configuration Bugfix: insufficient permissions for plugins install","title":"0.2.0"},{"location":"stacks/matomo/#010","text":"Initial release","title":"0.1.0"},{"location":"stacks/opensmtpd/","text":"OpenSMTPD stack documentation No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . OpenSMTPD can be configured with the following environment variables Sending test emails from CLI Access OpenSMTPD container Run sendmail -v -f verified-sender@verified-domain.com to@example.com Enter email subject and body in the input: Subject: test subject test body Enter Ctrl + D Integration with third-party SMTP services SendGrid AWS SES OpenSMTPD can be configured with any SMTP server. Just provide the following environment variables: RELAY_HOST RELAY_USER RELAY_PASSWORD By default it utilizes TLS schema with port 587 that can be changed via RELAY_PORT . Changelog 1.1.0 Improved health check now runs smtp command Messages queue is now persistent Default memory request set to 4m 1.0.3 Allow relay auth without password Use netcat instead of telnet in health checks Health check timeout increased to 30 seconds 1.0.2 Bugfix: health probes caused warning in logs 1.0.1 Support for relay without authentication 1.0.0 Initial release","title":"OpenSMTPD"},{"location":"stacks/opensmtpd/#opensmtpd-stack-documentation","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . OpenSMTPD can be configured with the following environment variables","title":"OpenSMTPD stack documentation"},{"location":"stacks/opensmtpd/#sending-test-emails-from-cli","text":"Access OpenSMTPD container Run sendmail -v -f verified-sender@verified-domain.com to@example.com Enter email subject and body in the input: Subject: test subject test body Enter Ctrl + D","title":"Sending test emails from CLI"},{"location":"stacks/opensmtpd/#integration-with-third-party-smtp-services","text":"SendGrid AWS SES OpenSMTPD can be configured with any SMTP server. Just provide the following environment variables: RELAY_HOST RELAY_USER RELAY_PASSWORD By default it utilizes TLS schema with port 587 that can be changed via RELAY_PORT .","title":"Integration with third-party SMTP services"},{"location":"stacks/opensmtpd/#changelog","text":"","title":"Changelog"},{"location":"stacks/opensmtpd/#110","text":"Improved health check now runs smtp command Messages queue is now persistent Default memory request set to 4m","title":"1.1.0"},{"location":"stacks/opensmtpd/#103","text":"Allow relay auth without password Use netcat instead of telnet in health checks Health check timeout increased to 30 seconds","title":"1.0.3"},{"location":"stacks/opensmtpd/#102","text":"Bugfix: health probes caused warning in logs","title":"1.0.2"},{"location":"stacks/opensmtpd/#101","text":"Support for relay without authentication","title":"1.0.1"},{"location":"stacks/opensmtpd/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/php/","text":"PHP stack documentation IMPORTANT PHP stack is not yet available for deployment via AnaxExp. But you already can use it for local development with Docker4PHP . You can subscribe to our newsletter to get the announcement email when we release it.","title":"Overview"},{"location":"stacks/php/#php-stack-documentation","text":"IMPORTANT PHP stack is not yet available for deployment via AnaxExp. But you already can use it for local development with Docker4PHP . You can subscribe to our newsletter to get the announcement email when we release it.","title":"PHP stack documentation"},{"location":"stacks/php/containers/","text":"PHP stack containers Nginx Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Do not gzip pages in your PHP application We already gzip content on Nginx side and it works faster. Having double gzip may cause issues. Restarting nginx as default user: sudo nginx -s reload Custom config If the default config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/php.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf ) Mod pagespeed Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service. Apache Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart PHP PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR ### Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug Xdebug (local) Debugging web requests Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1 Debugging CLI requests Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version: Linux, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make ) macOS, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist Windows Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost IDE configuration You must additionally configure your IDE to debug CLI requests. PHPStorm Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings NewRelic You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME . Profiling You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions. Crond A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. SSHd A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance. Mailhog If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ). OpenSMTPD See OpenSMTPD stack documentation . MariaDB See MariaDB stack documentation . Node.js Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file. Rsyslog Rsyslog can be used to stream your applications logs. It's similar to using syslog, however there's no syslog in PHP container (one process per container). Rsyslog will stream all incoming logs to a container output. You can use Monolog with SyslogUdpHandler to stream logs to rsyslog Blackfire You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation Webgrind Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Containers"},{"location":"stacks/php/containers/#php-stack-containers","text":"","title":"PHP stack containers"},{"location":"stacks/php/containers/#nginx","text":"Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Do not gzip pages in your PHP application We already gzip content on Nginx side and it works faster. Having double gzip may cause issues. Restarting nginx as default user: sudo nginx -s reload","title":"Nginx"},{"location":"stacks/php/containers/#custom-config","text":"If the default config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/php.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf )","title":"Custom config"},{"location":"stacks/php/containers/#mod-pagespeed","text":"Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service.","title":"Mod pagespeed"},{"location":"stacks/php/containers/#apache","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Apache"},{"location":"stacks/php/containers/#php","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel","title":"PHP"},{"location":"stacks/php/containers/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/php/containers/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR ### Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug","title":"Environment variables"},{"location":"stacks/php/containers/#xdebug-local","text":"","title":"Xdebug (local)"},{"location":"stacks/php/containers/#debugging-web-requests","text":"Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1","title":"Debugging web requests"},{"location":"stacks/php/containers/#debugging-cli-requests","text":"Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version:","title":"Debugging CLI requests"},{"location":"stacks/php/containers/#linux-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make )","title":"Linux, Docker"},{"location":"stacks/php/containers/#macos-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist","title":"macOS, Docker"},{"location":"stacks/php/containers/#windows","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost","title":"Windows"},{"location":"stacks/php/containers/#ide-configuration","text":"You must additionally configure your IDE to debug CLI requests.","title":"IDE configuration"},{"location":"stacks/php/containers/#phpstorm","text":"Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings","title":"PHPStorm"},{"location":"stacks/php/containers/#newrelic","text":"You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME .","title":"NewRelic"},{"location":"stacks/php/containers/#profiling","text":"You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions.","title":"Profiling"},{"location":"stacks/php/containers/#crond","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page.","title":"Crond"},{"location":"stacks/php/containers/#sshd","text":"A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance.","title":"SSHd"},{"location":"stacks/php/containers/#mailhog","text":"If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ).","title":"Mailhog"},{"location":"stacks/php/containers/#opensmtpd","text":"See OpenSMTPD stack documentation .","title":"OpenSMTPD"},{"location":"stacks/php/containers/#mariadb","text":"See MariaDB stack documentation .","title":"MariaDB"},{"location":"stacks/php/containers/#nodejs","text":"Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file.","title":"Node.js"},{"location":"stacks/php/containers/#rsyslog","text":"Rsyslog can be used to stream your applications logs. It's similar to using syslog, however there's no syslog in PHP container (one process per container). Rsyslog will stream all incoming logs to a container output. You can use Monolog with SyslogUdpHandler to stream logs to rsyslog","title":"Rsyslog"},{"location":"stacks/php/containers/#blackfire","text":"You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation","title":"Blackfire"},{"location":"stacks/php/containers/#webgrind","text":"Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Webgrind"},{"location":"stacks/php/local/","text":"Local environment with Docker4PHP Docker4PHP is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows. Requirements Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose Usage Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. Download php4docker.tar.gz from the latest stable release and unpack to your PHP project root Make sure NGINX_SERVER_ROOT (or APACHE_SERVER_ROOT ) is set to your project public directory with index.php (by default /var/www/html/public ) Ensure database credentials match in your database config and .env files Configure domains Optional: import existing database Optional: uncomment lines in the compose file to run redis, elasticsearch, kibana, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your php application should be up and running at http://php.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.php.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the php4docker.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page Domains Docker4PHP uses traefik container for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to php.docker.localhost , you can change it in .env file. Add 127.0.0.1 php.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://php.docker.localhost:8000 pma http://pma.php.docker.localhost:8000 adminer http://adminer.php.docker.localhost:8000 mailhog http://mailhog.php.docker.localhost:8000 solr http://solr.php.docker.localhost:8000 kibana http://kibana.php.docker.localhost:8000 node http://front.php.docker.localhost:8000 varnish http://varnish.php.docker.localhost:8000 portainer http://portainer.php.docker.localhost:8000 webgrind http://webgrind.php.docker.localhost:8000 Database import and export MariaDB See MariaDB stack documentation PostgreSQL See PostgreSQL stack documentation Make commands We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service Docker for mac There two major problems macOS users face with when using Docker for mac: macOS permissions issues To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user. Bind mounts performance Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved. User-guided caching Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\". Docker-sync Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page . Permissions issues You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container. Linux Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions. macOS Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user. Windows Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root Different uid/gid? You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default) Running multiple Projects Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Local environment"},{"location":"stacks/php/local/#local-environment-with-docker4php","text":"Docker4PHP is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows.","title":"Local environment with Docker4PHP"},{"location":"stacks/php/local/#requirements","text":"Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose","title":"Requirements"},{"location":"stacks/php/local/#usage","text":"Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. Download php4docker.tar.gz from the latest stable release and unpack to your PHP project root Make sure NGINX_SERVER_ROOT (or APACHE_SERVER_ROOT ) is set to your project public directory with index.php (by default /var/www/html/public ) Ensure database credentials match in your database config and .env files Configure domains Optional: import existing database Optional: uncomment lines in the compose file to run redis, elasticsearch, kibana, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your php application should be up and running at http://php.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.php.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the php4docker.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page","title":"Usage"},{"location":"stacks/php/local/#domains","text":"Docker4PHP uses traefik container for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to php.docker.localhost , you can change it in .env file. Add 127.0.0.1 php.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://php.docker.localhost:8000 pma http://pma.php.docker.localhost:8000 adminer http://adminer.php.docker.localhost:8000 mailhog http://mailhog.php.docker.localhost:8000 solr http://solr.php.docker.localhost:8000 kibana http://kibana.php.docker.localhost:8000 node http://front.php.docker.localhost:8000 varnish http://varnish.php.docker.localhost:8000 portainer http://portainer.php.docker.localhost:8000 webgrind http://webgrind.php.docker.localhost:8000","title":"Domains"},{"location":"stacks/php/local/#database-import-and-export","text":"","title":"Database import and export"},{"location":"stacks/php/local/#mariadb","text":"See MariaDB stack documentation","title":"MariaDB"},{"location":"stacks/php/local/#postgresql","text":"See PostgreSQL stack documentation","title":"PostgreSQL"},{"location":"stacks/php/local/#make-commands","text":"We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service","title":"Make commands"},{"location":"stacks/php/local/#docker-for-mac","text":"There two major problems macOS users face with when using Docker for mac:","title":"Docker for mac"},{"location":"stacks/php/local/#macos-permissions-issues","text":"To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user.","title":"macOS permissions issues"},{"location":"stacks/php/local/#bind-mounts-performance","text":"Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved.","title":"Bind mounts performance"},{"location":"stacks/php/local/#user-guided-caching","text":"Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\".","title":"User-guided caching"},{"location":"stacks/php/local/#docker-sync","text":"Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page .","title":"Docker-sync"},{"location":"stacks/php/local/#permissions-issues","text":"You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container.","title":"Permissions issues"},{"location":"stacks/php/local/#linux","text":"Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions.","title":"Linux"},{"location":"stacks/php/local/#macos","text":"Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user.","title":"macOS"},{"location":"stacks/php/local/#windows","text":"Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root","title":"Windows"},{"location":"stacks/php/local/#different-uidgid","text":"You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default)","title":"Different uid/gid?"},{"location":"stacks/php/local/#running-multiple-projects","text":"Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Running multiple Projects"},{"location":"stacks/postgres/","text":"PostgreSQL stack documentation PostgreSQL can be configured with the following environment variables Local environment if you want to import your database, uncomment the line for postgres-init volume in your compose file. Create the volume directory ./postgres-init in the same directory as the compose file and put there your .sql .sql.gz .sh file(s). All SQL files will be automatically imported once Postgres container has started. Changelog 1.2.1 PostgreSQL updated to 10.4, 9.6.9, 9.5.13, 9.4.18, 9.3.23 1.2.0 PostgreSQL updated to 10.2, 9.6.7, 9.5.11, 9.4.16, 9.3.21 Default memory request set to 64m 1.1.0 New PostgreSQL 10.1 PostgreSQL versions updated and freezed: 9.6.6, 9.5.10, 9.4.15, 9.3.20 PostgreSQL 9.2 has reached end of life and dropped Shutdown grace period increased to 5 minutes Health check timeout increased to 30 seconds Backup action now runs with nice (10) and ionice (7) Improved error handling in import action 1.0.1 Versions freeze: 9.6.3, 9.5.7, 9.4.12, 9.3.17, 9.2.21 Add new environment variable POSTGRES_DB_EXTENSIONS to create extensions 1.0.0 Initial release","title":"PostgreSQL"},{"location":"stacks/postgres/#postgresql-stack-documentation","text":"PostgreSQL can be configured with the following environment variables","title":"PostgreSQL stack documentation"},{"location":"stacks/postgres/#local-environment","text":"if you want to import your database, uncomment the line for postgres-init volume in your compose file. Create the volume directory ./postgres-init in the same directory as the compose file and put there your .sql .sql.gz .sh file(s). All SQL files will be automatically imported once Postgres container has started.","title":"Local environment"},{"location":"stacks/postgres/#changelog","text":"","title":"Changelog"},{"location":"stacks/postgres/#121","text":"PostgreSQL updated to 10.4, 9.6.9, 9.5.13, 9.4.18, 9.3.23","title":"1.2.1"},{"location":"stacks/postgres/#120","text":"PostgreSQL updated to 10.2, 9.6.7, 9.5.11, 9.4.16, 9.3.21 Default memory request set to 64m","title":"1.2.0"},{"location":"stacks/postgres/#110","text":"New PostgreSQL 10.1 PostgreSQL versions updated and freezed: 9.6.6, 9.5.10, 9.4.15, 9.3.20 PostgreSQL 9.2 has reached end of life and dropped Shutdown grace period increased to 5 minutes Health check timeout increased to 30 seconds Backup action now runs with nice (10) and ionice (7) Improved error handling in import action","title":"1.1.0"},{"location":"stacks/postgres/#101","text":"Versions freeze: 9.6.3, 9.5.7, 9.4.12, 9.3.17, 9.2.21 Add new environment variable POSTGRES_DB_EXTENSIONS to create extensions","title":"1.0.1"},{"location":"stacks/postgres/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/redis/","text":"Redis stack documentation Redis can be configured with the following environment variables Changelog 1.1.0 Redis updated to 4.0.8 Default memory request set to 256m Bugfix: redis 4 init could not disable THP on some servers 1.0.4 Redis updated to 3.2.11, 4.0.2 Fix init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Health check timeout increased to 30 seconds 1.0.3 New experimental 4.0 version Update to 3.2.10 Bugfix: incorrect data volume permissions Changed data volume location from /var/lib/redis to /data 1.0.2 Set net.core.somaxconn to 65535 during initial start Transparent Huge Pages are now disabled to improve performance 1.0.1 Version freeze https://github.com/anaxexp/redis#versions Redis now always has persistent volume 1.0.0 Initial release","title":"Redis"},{"location":"stacks/redis/#redis-stack-documentation","text":"Redis can be configured with the following environment variables","title":"Redis stack documentation"},{"location":"stacks/redis/#changelog","text":"","title":"Changelog"},{"location":"stacks/redis/#110","text":"Redis updated to 4.0.8 Default memory request set to 256m Bugfix: redis 4 init could not disable THP on some servers","title":"1.1.0"},{"location":"stacks/redis/#104","text":"Redis updated to 3.2.11, 4.0.2 Fix init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Health check timeout increased to 30 seconds","title":"1.0.4"},{"location":"stacks/redis/#103","text":"New experimental 4.0 version Update to 3.2.10 Bugfix: incorrect data volume permissions Changed data volume location from /var/lib/redis to /data","title":"1.0.3"},{"location":"stacks/redis/#102","text":"Set net.core.somaxconn to 65535 during initial start Transparent Huge Pages are now disabled to improve performance","title":"1.0.2"},{"location":"stacks/redis/#101","text":"Version freeze https://github.com/anaxexp/redis#versions Redis now always has persistent volume","title":"1.0.1"},{"location":"stacks/redis/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/redmine/","text":"Redmine stack documentation Changelog 1.1.0 Redmine updated to 3.4 Default memory request set to: Redmine: 128m PostgreSQL: 64m OpenSMTPD: 64m 1.0.1 Updated Postgres (1.0.1) service 1.0.0 Initial release","title":"Redmine stack documentation"},{"location":"stacks/redmine/#redmine-stack-documentation","text":"","title":"Redmine stack documentation"},{"location":"stacks/redmine/#changelog","text":"","title":"Changelog"},{"location":"stacks/redmine/#110","text":"Redmine updated to 3.4 Default memory request set to: Redmine: 128m PostgreSQL: 64m OpenSMTPD: 64m","title":"1.1.0"},{"location":"stacks/redmine/#101","text":"Updated Postgres (1.0.1) service","title":"1.0.1"},{"location":"stacks/redmine/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/slackin/","text":"Slackin stack documentation After deployment add the following environment variables: SLACK_TEAM \u2013 your Slack team name SLACK_TOKEN - generated token here: https://api.slack.com/custom-integrations/legacy-tokens Changelog 1.1.0 Default memory request set to 16m 1.0.0 Initial release","title":"Slackin"},{"location":"stacks/slackin/#slackin-stack-documentation","text":"After deployment add the following environment variables: SLACK_TEAM \u2013 your Slack team name SLACK_TOKEN - generated token here: https://api.slack.com/custom-integrations/legacy-tokens","title":"Slackin stack documentation"},{"location":"stacks/slackin/#changelog","text":"","title":"Changelog"},{"location":"stacks/slackin/#110","text":"Default memory request set to 16m","title":"1.1.0"},{"location":"stacks/slackin/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/solr/","text":"Solr stack documentation Default core Since 1.1.0 we automatically create a default solr core named default when no cores found. All environment variables available for Solr configuration can be found at https://github.com/anaxexp/solr Creating solr core You can create new cores via Solr admin UI or create it manually from CLI with additional parameters: Go to Instance Stack Search Engine page Copy and execute Access container on your host server to access the container with Solr Copy Create Solr core command (edit to change core name), execute it inside of the container. Response 200 means that the core has been successfully created. Changelog 1.2.1 New 7.2 version added Patch update: 6.6.3 1.2.0 Default memory request set to 256m 1.1.0 New Solr versions 7.0.1 and 7.1.0 have been added Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 We now create a default solr core named default automatically if no cores found Health check timeouts increased to 30 seconds 1.0.3 Solr: fixed persistent data paths configuration 1.0.1 New 6.6 and 6.5 versions Solr versions are now frozen https://github.com/anaxexp/solr#versions 1.0.0 Initial release","title":"Solr"},{"location":"stacks/solr/#solr-stack-documentation","text":"Default core Since 1.1.0 we automatically create a default solr core named default when no cores found. All environment variables available for Solr configuration can be found at https://github.com/anaxexp/solr","title":"Solr stack documentation"},{"location":"stacks/solr/#creating-solr-core","text":"You can create new cores via Solr admin UI or create it manually from CLI with additional parameters: Go to Instance Stack Search Engine page Copy and execute Access container on your host server to access the container with Solr Copy Create Solr core command (edit to change core name), execute it inside of the container. Response 200 means that the core has been successfully created.","title":"Creating solr core"},{"location":"stacks/solr/#changelog","text":"","title":"Changelog"},{"location":"stacks/solr/#121","text":"New 7.2 version added Patch update: 6.6.3","title":"1.2.1"},{"location":"stacks/solr/#120","text":"Default memory request set to 256m","title":"1.2.0"},{"location":"stacks/solr/#110","text":"New Solr versions 7.0.1 and 7.1.0 have been added Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 We now create a default solr core named default automatically if no cores found Health check timeouts increased to 30 seconds","title":"1.1.0"},{"location":"stacks/solr/#103","text":"Solr: fixed persistent data paths configuration","title":"1.0.3"},{"location":"stacks/solr/#101","text":"New 6.6 and 6.5 versions Solr versions are now frozen https://github.com/anaxexp/solr#versions","title":"1.0.1"},{"location":"stacks/solr/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/solr-drupal/","text":"Solr for Drupal stack documentation The only difference from Solr stack is that core created with a config set from Search API Solr module by default, for more details on the version see https://github.com/anaxexp/drupal-solr . Default core We automatically create a default solr core named default when no cores found. Drupal 8 Install Search API Solr module . Go to Home \u00bb Administration \u00bb Configuration \u00bb Search and metadata \u00bb Search API , create a new core or edit the default one. In expanded CONFIGURE SOLR BACKEND field set specify: HTTP protocol: http Solr host: solr Solr port: 8983 Solr path: /solr Solr core: [NAME OF YOUR CORE] Drupal 7 Install Search API Solr module . Go to Configuration \u00bb Search and metadata \u00bb Search API and select Service class to \"Solr service\". In expanded settings field set specify: HTTP protocol: http Solr host: solr Solr port: 8983 Solr path: /solr/[NAME OF YOUR CORE] Different hostname for stand-alone stacks If you use a stand-alone Solr for Drupal stack for Solr host use Internal hostname from [Instance] Stack Search Engine page in case Drupal and Solr stacks deployed on the same server. Changelog 1.2.1 New 7.2 version added Patch update: 6.6.3 Solr 7.x config sources updated to search_api_solr 8.x-2.0-alpha3 1.2.0 Default memory request set to 256m 1.1.0 New Solr versions 7.0.1 and 7.1.0 have been added with a config set from search_api_solr 8.x-2.0-alpha2 Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 Config set source search_api_solr updated to 8.x-1.2 We now create a default solr core named default automatically if no cores found Health check timeouts increased to 30 seconds 1.0.4 Solr: fixed persistent data paths configuration 1.0.1 Solr: new versions 6.6 and 6.5 for Drupal 8 Solr: search_api_solr version updated from to 8.x-1.0 (default solr configs used from this module) Solr versions are now frozen https://github.com/anaxexp/solr#versions 1.0.0 Initial release","title":"Solr for Drupal"},{"location":"stacks/solr-drupal/#solr-for-drupal-stack-documentation","text":"The only difference from Solr stack is that core created with a config set from Search API Solr module by default, for more details on the version see https://github.com/anaxexp/drupal-solr . Default core We automatically create a default solr core named default when no cores found.","title":"Solr for Drupal stack documentation"},{"location":"stacks/solr-drupal/#drupal-8","text":"Install Search API Solr module . Go to Home \u00bb Administration \u00bb Configuration \u00bb Search and metadata \u00bb Search API , create a new core or edit the default one. In expanded CONFIGURE SOLR BACKEND field set specify: HTTP protocol: http Solr host: solr Solr port: 8983 Solr path: /solr Solr core: [NAME OF YOUR CORE]","title":"Drupal 8"},{"location":"stacks/solr-drupal/#drupal-7","text":"Install Search API Solr module . Go to Configuration \u00bb Search and metadata \u00bb Search API and select Service class to \"Solr service\". In expanded settings field set specify: HTTP protocol: http Solr host: solr Solr port: 8983 Solr path: /solr/[NAME OF YOUR CORE] Different hostname for stand-alone stacks If you use a stand-alone Solr for Drupal stack for Solr host use Internal hostname from [Instance] Stack Search Engine page in case Drupal and Solr stacks deployed on the same server.","title":"Drupal 7"},{"location":"stacks/solr-drupal/#changelog","text":"","title":"Changelog"},{"location":"stacks/solr-drupal/#121","text":"New 7.2 version added Patch update: 6.6.3 Solr 7.x config sources updated to search_api_solr 8.x-2.0-alpha3","title":"1.2.1"},{"location":"stacks/solr-drupal/#120","text":"Default memory request set to 256m","title":"1.2.0"},{"location":"stacks/solr-drupal/#110","text":"New Solr versions 7.0.1 and 7.1.0 have been added with a config set from search_api_solr 8.x-2.0-alpha2 Solr versions updated and freezed: 6.6.2, 6.5.1, 6.4.2, 6.3.0, 5.5.5, 5.4.1 Config set source search_api_solr updated to 8.x-1.2 We now create a default solr core named default automatically if no cores found Health check timeouts increased to 30 seconds","title":"1.1.0"},{"location":"stacks/solr-drupal/#104","text":"Solr: fixed persistent data paths configuration","title":"1.0.4"},{"location":"stacks/solr-drupal/#101","text":"Solr: new versions 6.6 and 6.5 for Drupal 8 Solr: search_api_solr version updated from to 8.x-1.0 (default solr configs used from this module) Solr versions are now frozen https://github.com/anaxexp/solr#versions","title":"1.0.1"},{"location":"stacks/solr-drupal/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/varnish/","text":"Varnish stack documentation Varnish can be configured with the following environment variables Headers X-Varnish-Cache : HIT or MISS, corresponds to when the cache was found or not Age: 34 : age of the cache in seconds X-Varnish: 65658 65623 - the first number is the ID of a request, the second is the ID of cache inside of Varnish. When operating normally the first number changes with every request of the same page and the second stays the same. Set header Cache-Control:no-cache on backend to tell Varnish to not cache this page. CLI Grouped list with the most usual entries from different logs: varnishtop A histogram that shows the time taken for the requests processing: varnishhist Varnish stats, shows how many contents on cache hits, resource consumption, etc..: varnishstat Log showing requests made to the web backend server: varnishlog Troubleshooting 503 (guru meditation) errors You can get more details on 503 responses by filtering the logs: varnishlog -q RespStatus == 503 -g request A few reasons why you may get 503: A problem on backend, see backend container's logs Varnish may cache non-broken page from backend when backend gives 5xx, in this cases you will sometimes get 503 (fetch from backend) and sometimes 200 OK (from cache) Broken backend headers that prevent from parsing backend response's body, e.g. gzip encoding header when the body in fact is not gzipped (you should not gzip pages on your backend, we already do that on Nginx) Timeouts on varnish are too low (unlikely, the defaults are high enough for 95% cases), you can increase them via environment variables Changelog 1.2.0 Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4 1.1.0 Default memory request set to 16m The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 ) 1.0.3 Varnish updated to 4.1.9 Health check timeout increased to 30 seconds 1.0.2 Varnish daemon env vars now start from VARNISHD_ to avoid collisions 1.0.1 Health check improvement, now uses varnishadm instead of curl 1.0.0 Initial release","title":"Varnish"},{"location":"stacks/varnish/#varnish-stack-documentation","text":"Varnish can be configured with the following environment variables","title":"Varnish stack documentation"},{"location":"stacks/varnish/#headers","text":"X-Varnish-Cache : HIT or MISS, corresponds to when the cache was found or not Age: 34 : age of the cache in seconds X-Varnish: 65658 65623 - the first number is the ID of a request, the second is the ID of cache inside of Varnish. When operating normally the first number changes with every request of the same page and the second stays the same. Set header Cache-Control:no-cache on backend to tell Varnish to not cache this page.","title":"Headers"},{"location":"stacks/varnish/#cli","text":"Grouped list with the most usual entries from different logs: varnishtop A histogram that shows the time taken for the requests processing: varnishhist Varnish stats, shows how many contents on cache hits, resource consumption, etc..: varnishstat Log showing requests made to the web backend server: varnishlog","title":"CLI"},{"location":"stacks/varnish/#troubleshooting-503-guru-meditation-errors","text":"You can get more details on 503 responses by filtering the logs: varnishlog -q RespStatus == 503 -g request A few reasons why you may get 503: A problem on backend, see backend container's logs Varnish may cache non-broken page from backend when backend gives 5xx, in this cases you will sometimes get 503 (fetch from backend) and sometimes 200 OK (from cache) Broken backend headers that prevent from parsing backend response's body, e.g. gzip encoding header when the body in fact is not gzipped (you should not gzip pages on your backend, we already do that on Nginx) Timeouts on varnish are too low (unlikely, the defaults are high enough for 95% cases), you can increase them via environment variables","title":"Troubleshooting 503 (guru meditation) errors"},{"location":"stacks/varnish/#changelog","text":"","title":"Changelog"},{"location":"stacks/varnish/#120","text":"Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4","title":"1.2.0"},{"location":"stacks/varnish/#110","text":"Default memory request set to 16m The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 )","title":"1.1.0"},{"location":"stacks/varnish/#103","text":"Varnish updated to 4.1.9 Health check timeout increased to 30 seconds","title":"1.0.3"},{"location":"stacks/varnish/#102","text":"Varnish daemon env vars now start from VARNISHD_ to avoid collisions","title":"1.0.2"},{"location":"stacks/varnish/#101","text":"Health check improvement, now uses varnishadm instead of curl","title":"1.0.1"},{"location":"stacks/varnish/#100","text":"Initial release","title":"1.0.0"},{"location":"stacks/wordpress/","text":"WordPress stack documentation WordPress stack wordpress4docker Deployment See main code deployment article to learn about code deployment options on AnaxExp. Vanilla WordPress For demo purposes and simple WordPress installations you can use Vanilla WordPress deployment option. In this case WordPress code that comes with the Docker image will be used. In case of changes all data made to your codebase will persist but there will be no versions control. Direct git integration We recommend using Composer to manage dependencies in your repository. Dependencies will be installed via post-deployment scripts : Fork our boilerplate Create anaxexp.yml in repository root (our boilerplate already has it) with the following content: pipeline: - name: Install dependencies type: command command: composer install --prefer-dist -n --no-dev directory: $APP_ROOT Enter web (it's a directory name with WordPress root in our boilerplate) in Codebase dir input on the 3 rd step of new application deployment form CI/CD CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: php crond sshd HTTP server: nginx or apache Import There are different way to import existing WordPress website. From duplicator archive Install duplicator plugin on your existing website. Go to admin part of your WordPress website and create a new package via duplicator. Now navigate to Apps Deploy and choose duplicator archive as data source on the 3 rd step. From separate archives Import WordPress via separate archives for database and files. We support .zip , .gz , .tar.gz , .tgz and .tar archives. This option is available on the 3 rd step of a new instance deployment form and also on [Instance] Import page of existing instance. Manual import In case your import data is huge it makes sense to import it manually from the server. Follow these steps: Deploy your WordPress website from a git repository without importing data Once the app is deployed, go to Stack SSH and copy SSH command Connect to the container by SSH Copy your database archive here via wget or scp , make sure it's gzipped Import unpacked database dump using wp db import my-db-dump.sql Now let's import your files, cd to /mnt/files/public Copy your files archive here via wget or scp and unpack the archive That's it! Clear WordPress cache and remove import artifacts Import between instances You can import database and files from one instance to another regardless of whether instances are on the same server or not. Go to [Instance] Import tab and select an instance where you'd like to import database/files from. Upgrading WordPress Use composer We recommend managing WordPress core and plugins dependencies with composer, you can find a boilerplate at https://github.com/anaxexp/wordpress-composer Upgrading core Upgrading WordPress core requires a full writing permissions on the entire codebase, we do not provide such wide permissions for security reasons. So you'll have to upgrade your core either manually via your git or by upgrading your stack if you deployed a vanilla WordPress. Upgrading themes and plugins If your theme or plugin introduces a new directory or a file under a non-standard path, you'll have to grant writing permissions manually by changing the owner's group to :www-data and adding writing permissions to the group. Connect to your app instance by SSH and run: chown :www-data YOUR_FILE chmod 664 YOUR_FILE Or if you want to set writing permission on a directory recursively: chown :www-data -R YOUR_DIR chmod 664 -R YOUR_DIR WordPress config wp-config.php AnaxExp automatically adds include of anaxexp.wp-config.php to wp-config.php file in WP root. If the file does not exist AnaxExp will create it automatically. Do not edit anaxexp.wp-config.php , all changes to this file will be reset. The anaxexp.wp-config.php file contains configuration settings for integration with AnaxExp services such as Database, Cache storage and Reverse Caching Proxy. You can override settings specified in anaxexp.wp-config.php in your wp-config.php file after the include. Files Files for WordPress located in /mnt/files/public and symlinked to wp-content/uploads . Base URL The domain marked with primary flag will be used as a WP_HOME and WP_SITEURL in anaxexp.wp-config.php file. Mail delivery No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from Application Stack Settings page. Cron By default we run the following cron command from crond container every hour: wp cron event run --due-now --path= ${HTTP_ROOT} --url= ${BASE_URL} You can customize crontab from [Instance] Stack Settings page. Cache control You can clear caches and control cache settings from [Instance] Cache page. The following actions are available: Clear application cache Clear redis cache (if enabled) Clear varnish cache (if enabled) Clear all caches Enable/disable opcache Multi-site WordPress multi-site supported, both subdomains and subdirectories based.","title":"Overview"},{"location":"stacks/wordpress/#wordpress-stack-documentation","text":"WordPress stack wordpress4docker","title":"WordPress stack documentation"},{"location":"stacks/wordpress/#deployment","text":"See main code deployment article to learn about code deployment options on AnaxExp.","title":"Deployment"},{"location":"stacks/wordpress/#vanilla-wordpress","text":"For demo purposes and simple WordPress installations you can use Vanilla WordPress deployment option. In this case WordPress code that comes with the Docker image will be used. In case of changes all data made to your codebase will persist but there will be no versions control.","title":"Vanilla WordPress"},{"location":"stacks/wordpress/#direct-git-integration","text":"We recommend using Composer to manage dependencies in your repository. Dependencies will be installed via post-deployment scripts : Fork our boilerplate Create anaxexp.yml in repository root (our boilerplate already has it) with the following content: pipeline: - name: Install dependencies type: command command: composer install --prefer-dist -n --no-dev directory: $APP_ROOT Enter web (it's a directory name with WordPress root in our boilerplate) in Codebase dir input on the 3 rd step of new application deployment form","title":"Direct git integration"},{"location":"stacks/wordpress/#cicd","text":"CI/CD tutorial For a detailed instructions of setting up CI/CD workflow see the main deployment article The following services are CI services that will be built by default: php crond sshd HTTP server: nginx or apache","title":"CI/CD"},{"location":"stacks/wordpress/#import","text":"There are different way to import existing WordPress website.","title":"Import"},{"location":"stacks/wordpress/#from-duplicator-archive","text":"Install duplicator plugin on your existing website. Go to admin part of your WordPress website and create a new package via duplicator. Now navigate to Apps Deploy and choose duplicator archive as data source on the 3 rd step.","title":"From duplicator archive"},{"location":"stacks/wordpress/#from-separate-archives","text":"Import WordPress via separate archives for database and files. We support .zip , .gz , .tar.gz , .tgz and .tar archives. This option is available on the 3 rd step of a new instance deployment form and also on [Instance] Import page of existing instance.","title":"From separate archives"},{"location":"stacks/wordpress/#manual-import","text":"In case your import data is huge it makes sense to import it manually from the server. Follow these steps: Deploy your WordPress website from a git repository without importing data Once the app is deployed, go to Stack SSH and copy SSH command Connect to the container by SSH Copy your database archive here via wget or scp , make sure it's gzipped Import unpacked database dump using wp db import my-db-dump.sql Now let's import your files, cd to /mnt/files/public Copy your files archive here via wget or scp and unpack the archive That's it! Clear WordPress cache and remove import artifacts","title":"Manual import"},{"location":"stacks/wordpress/#import-between-instances","text":"You can import database and files from one instance to another regardless of whether instances are on the same server or not. Go to [Instance] Import tab and select an instance where you'd like to import database/files from.","title":"Import between instances"},{"location":"stacks/wordpress/#upgrading-wordpress","text":"Use composer We recommend managing WordPress core and plugins dependencies with composer, you can find a boilerplate at https://github.com/anaxexp/wordpress-composer","title":"Upgrading WordPress"},{"location":"stacks/wordpress/#upgrading-core","text":"Upgrading WordPress core requires a full writing permissions on the entire codebase, we do not provide such wide permissions for security reasons. So you'll have to upgrade your core either manually via your git or by upgrading your stack if you deployed a vanilla WordPress.","title":"Upgrading core"},{"location":"stacks/wordpress/#upgrading-themes-and-plugins","text":"If your theme or plugin introduces a new directory or a file under a non-standard path, you'll have to grant writing permissions manually by changing the owner's group to :www-data and adding writing permissions to the group. Connect to your app instance by SSH and run: chown :www-data YOUR_FILE chmod 664 YOUR_FILE Or if you want to set writing permission on a directory recursively: chown :www-data -R YOUR_DIR chmod 664 -R YOUR_DIR","title":"Upgrading themes and plugins"},{"location":"stacks/wordpress/#wordpress-config","text":"","title":"WordPress config"},{"location":"stacks/wordpress/#wp-configphp","text":"AnaxExp automatically adds include of anaxexp.wp-config.php to wp-config.php file in WP root. If the file does not exist AnaxExp will create it automatically. Do not edit anaxexp.wp-config.php , all changes to this file will be reset. The anaxexp.wp-config.php file contains configuration settings for integration with AnaxExp services such as Database, Cache storage and Reverse Caching Proxy. You can override settings specified in anaxexp.wp-config.php in your wp-config.php file after the include.","title":"wp-config.php"},{"location":"stacks/wordpress/#files","text":"Files for WordPress located in /mnt/files/public and symlinked to wp-content/uploads .","title":"Files"},{"location":"stacks/wordpress/#base-url","text":"The domain marked with primary flag will be used as a WP_HOME and WP_SITEURL in anaxexp.wp-config.php file.","title":"Base URL"},{"location":"stacks/wordpress/#mail-delivery","text":"No delivery guarantee If you're using a server from a public cloud there's a good chance that its IP is already compromised and blacklisted by major mail services, hence your emails will not be delivered or will land in the spam folder. We strongly recommend using OpenSMTPD in pair with a third-party SMTP services . Mail transfer agent OpenSMTPD included in the stack and used as a default mail delivery service. Emails will be sent from the server hosting your application. Additionally, you can enable mail catcher service Mailhog to catch all outbound emails and release them manually from UI to an SMTP server. You can switch an active mail delivery service from Application Stack Settings page.","title":"Mail delivery"},{"location":"stacks/wordpress/#cron","text":"By default we run the following cron command from crond container every hour: wp cron event run --due-now --path= ${HTTP_ROOT} --url= ${BASE_URL} You can customize crontab from [Instance] Stack Settings page.","title":"Cron"},{"location":"stacks/wordpress/#cache-control","text":"You can clear caches and control cache settings from [Instance] Cache page. The following actions are available: Clear application cache Clear redis cache (if enabled) Clear varnish cache (if enabled) Clear all caches Enable/disable opcache","title":"Cache control"},{"location":"stacks/wordpress/#multi-site","text":"WordPress multi-site supported, both subdomains and subdirectories based.","title":"Multi-site"},{"location":"stacks/wordpress/changelog/","text":"WordPress stack changelog This is the changelog for WordPress stack deployed via AnaxExp, for wordpress4docker changes see GitHub releases page . 5.1.0 Vanilla WordPress core updated to 4.9.6 PHP Added php tidy extension Added tideways xhprof extension https://github.com/anaxexp/drupal-php#49 (disabled by default) auto_prepend_file and auto_append_file are now configurable Updated PHP extensions: GRPC 1.12.0, igbinary 2.0.6, mongodb 1.4.4 MariaDB: New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2b50\ufe0f Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container Default innodb_buffer_pool_instances set to 1 Nginx: Added new Nginx 1.15 \u2b50\ufe0f Added mog_pagespeed module. Disabled by default, to enable add NGINX_PAGESPEED=on to nginx service Added new modules: http_image_filter_module http_slice_module http_xslt_module stream_geoip_module stream_realip_module stream_ssl_preread_module Varnish Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4 Unrestricted purge is now allowed in internal network (from containers within the same instance) Webgrind: error reporting now exludes strict and deprecated errors, rebased to latest PHP 7.1 image Upgrade instructions \u2757Make sure the new default size of innodb_buffer_pool_instances (128M) is enough for your project, see MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application 5.0.11 PHP: Security update : 7.2.5, 7.1.17, 7.0.30, 5.6.36 New php extensions added: GMP and igbinary APCu extension updated to 5.0.11 for PHP 7.x APCu serialized is now configurable with $PHP_APCU_SERIALIZER Shell prompt in PHP containers now shows current user, application name and instance name Added new helper script files_chown Bugfix: iconv implementation missing anaxexp/php#25 Varnish purge via HTTP is now unrestricted with a purge key, see updated docs Added Nginx 1.14, patch update for 1.13 Nginx's underscores_in_headers is now configurable via $NGINX_UNDERSCORES_IN_HEADERS 5.0.10 PHP log errors max length set to unlimited Bugfix: PHP errors didn't show up in the container output Bugfix: APACHE_LIMITED_ACCESS support from 5.0.6 release was missing 5.0.9 Vanilla WordPress updated to 4.9.5 ( security and maintenance release ) 5.0.8 PHP 5.6 version returned 5.0.7 PHP: Updated to 7.2.4, 7.1.16, 7.0.35 ( security update ) Added jpegoptim Added writable permission to FPM for 'wp-content/wp-rocket-config/' 5.0.6 Apache: Updated to 2.4.33 ( security update ) New environment variable APACHE_LIMITED_ACCESS to remove Require all granted when you need to limit access by IP 5.0.5 Nginx updated to 1.13.10 PHP extension grpc updated to 1.10.0 Added environment variables for PHP session runtime configuration Improved error reporting and progress messages for public files directory init Bugfix: apache settings file didn't include when virtual host config overridden with APACHE_INCLUDE_CONF 5.0.4 Bugfix: vanilla WordPress didn't work with PHP 7.2 Bugfix: wp-admin/ with apache redirected to homepage Bugfix: some caching plugins didn't work because of unsufficient permissions Added APACHE_INCLUDE_CONF to override apache config Database service is now optional in case you want to have a stand-alone DB server Redis service is not included by default 5.0.3 Cron now runs from www-data user instead of anaxexp files_chmod script now sets permissions with execution allowed only for directories 5.0.2 Bugfix: translation update fail due to insufficient permissions 5.0.1 PHP updated to 7.2.3, 7.1.15, 7.0.28 (security updates) Bugfix: insufficient permissions for plugins update Bugfix: missing ~/.ssh directory for www-data user required by some plugins 5.0.0 Changes since 4.4.1 All containers now have resources request as listed here in Resources column , in addition, crond has CPU limit PHP: Container default user has been changed to anaxexp (uid/gid 1000), see https://github.com/anaxexp/php#users-and-permissions for more details PHP updated to 7.2.2, 7.1.14, 7.0.27 (security updates) Rebased to Alpine Linux 3.7 Now when your upgrade stack with a new version of vanilla WordPress, your source code will be updated You can monitor PHP with NewRelic APM allow_url_fopen and default_socket_timeout is now configurable New php extensions added: newrelic, grpc, ds Deprecated environment variables dropped (listed in 4.4.0 changes ) Added postgresql client bins (pg_dump, pg_restore, ...) Added redis-cli Updated php extensions: amqp 1.9.3, redis 3.1.6, mongodb 1.4.0, apcu 5.1.10 Environment variable ANAXEXP_DIR_FILES replaced to FILES_DIR Bugfix: cache clearing from dashboard didn't work for subdirectories MariaDB: Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Nginx: Updated to 1.13.9 Rebased to Alpine Linux 3.7 Redis: Updated to 4.0.8 Bugfix: redis 4 init could not disable THP on some servers OpenSMTPD: Improved health check now runs smtp command Messages queue is now persistent Varnish: The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 ) Bugfix: auth issue in Apache ( https://github.com/anaxexp/php-apache/issues/1 ) Upgrade instructions Make sure you don't use any of deprecated environment variables in PHP (listed in 4.4.0 changes ) and Varnish (listed above) otherwise update their names If you used ANAXEXP_DIR_FILES in your code replace it with FILES_DIR Make sure the default cron container 512M RAM limit is enough for your cron jobs, otherwise increase it manually from service configuration page 4.1.1 Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 4.4.0 4.1.0 Changes since 4.0.0 PHP: New PHP 7.2 PHP updated to 7.1.12, 7.0.26 PHP extensions updated: memcached 3.0.4, ast 0.1.6 Added packages: tig, nano, tmux, less, libjpeg-turbo-utils PHPunit deleted from image to avoid composer conflicts Env vars naming fixes (old names still supported), old new: PHP_APCU_ENABLE PHP_APCU_ENABLED PHP_FPM_SLOWLOG_TIMEOUT PHP_FPM_REQUEST_SLOWLOG_TIMEOUT PHP_FPM_MAX_CHILDREN PHP_FPM_PM_MAX_CHILDREN PHP_FPM_START_SERVERS PHP_FPM_PM_START_SERVERS PHP_FPM_MIN_SPARE_SERVERS PHP_FPM_PM_MIN_SPARE_SERVERS PHP_FPM_MAX_SPARE_SERVERS PHP_FPM_PM_MAX_SPARE_SERVERS PHP_FPM_MAX_REQUESTS PHP_FPM_PM_MAX_REQUESTS PHP_FPM_STATUS_PATH PHP_FPM_PM_STATUS_PATH New -dev image tags (replacing -debug ) for CI/CD (TBA) Env var ANAXEXP_HOST_PRIMARY value now contains host (instead of URL) as it should, ANAXEXP_URL_PRIMARY has been added for the URL value. See environment variables section Improved validation and error reporting for duplicator import Git email and name now can be configured via environment variables Nginx: Nginx updated to 1.13.7, 1.12.2 Fixed broken health check New env var NGINX_NO_DEFAULT_HEADERS to hide default headers We now show request real IP in access logs MariaDB: New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Deployment strategy no longer can be changed Optimized default config (my.cnf) values New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice and ionice to prioritize CPU and I/O time for this process Improved error reporting during import Redis: Redis updated to 3.2.11, 4.0.2 Fixed init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Global environment variables changes: $ANAXEXP_APP_NAME no longer contains instance machine name, only application machine name $ANAXEXP_ENVIRONMENT_ variables have been deprecated and replaced with $ANAXEXP_INSTANCE_ New variables $ANAXEXP_INSTANCE_UUID and $ANAXEXP_APP_UUID Varnish updated to 4.1.9 Apache updated to 2.4.29 Vanilla WordPress updated to 4.9.1 Health checks timeout increased to 30 seconds for all services OpenSMTPD now supports relay auth without password Files backup and mirroring actions now run with nice and ionice to prioritize CPU and I/O time for this process Update instructions from 4.0.0 If you used $ANAXEXP_APP_NAME update your code accordingly to the new value (machine name of the app) If you used $ANAXEXP_HOST_PRIMARY (now contains host instead of URL) before you should replace it to $ANAXEXP_URL_PRIMARY Upgrade downtime ~5 minutes 4.0.0 Changes since 3.x All-new revamped containers consistent with wordpress4docker Improved performance of containers (especially I/O) Revamped orchestration with better logging and performance Optional services now can be enabled/disabled on the working app Services configuration via environment variables from the dashboard This stack is now suited for container-based cluster Log streaming is now available Detailed log output for orchestration tasks New services: apache, webgrind, blackfire, rsyslog, athenapdf There's no backward compatibility with stacks 3.x","title":"Changelog"},{"location":"stacks/wordpress/changelog/#wordpress-stack-changelog","text":"This is the changelog for WordPress stack deployed via AnaxExp, for wordpress4docker changes see GitHub releases page .","title":"WordPress stack changelog"},{"location":"stacks/wordpress/changelog/#510","text":"Vanilla WordPress core updated to 4.9.6 PHP Added php tidy extension Added tideways xhprof extension https://github.com/anaxexp/drupal-php#49 (disabled by default) auto_prepend_file and auto_append_file are now configurable Updated PHP extensions: GRPC 1.12.0, igbinary 2.0.6, mongodb 1.4.4 MariaDB: New version 10.3 added (10.3.7) MariaDB updates: 10.2.15, 10.1.34 optimizer_prune_level and optimizer_search_depth are now configurable https://github.com/anaxexp/mariadb/issues/4 \u2b50\ufe0f Default innodb_buffer_pool_size set to 128M that should significantly decrease memory usage by MariaDB container Default innodb_buffer_pool_instances set to 1 Nginx: Added new Nginx 1.15 \u2b50\ufe0f Added mog_pagespeed module. Disabled by default, to enable add NGINX_PAGESPEED=on to nginx service Added new modules: http_image_filter_module http_slice_module http_xslt_module stream_geoip_module stream_realip_module stream_ssl_preread_module Varnish Environment variable VARNISHD_STORAGE_SIZE has been dropped, we no longer add a predefined secondary storage. You can now add your custom secondary storage via VARNISHD_SECONDARY_STORAGE https://github.com/anaxexp/varnish/pull/4 Unrestricted purge is now allowed in internal network (from containers within the same instance) Webgrind: error reporting now exludes strict and deprecated errors, rebased to latest PHP 7.1 image","title":"5.1.0"},{"location":"stacks/wordpress/changelog/#upgrade-instructions","text":"\u2757Make sure the new default size of innodb_buffer_pool_instances (128M) is enough for your project, see MariaDB stack documentation to learn how to calculate the optimal size of innodb_buffer_pool_size for your application","title":"Upgrade instructions"},{"location":"stacks/wordpress/changelog/#5011","text":"PHP: Security update : 7.2.5, 7.1.17, 7.0.30, 5.6.36 New php extensions added: GMP and igbinary APCu extension updated to 5.0.11 for PHP 7.x APCu serialized is now configurable with $PHP_APCU_SERIALIZER Shell prompt in PHP containers now shows current user, application name and instance name Added new helper script files_chown Bugfix: iconv implementation missing anaxexp/php#25 Varnish purge via HTTP is now unrestricted with a purge key, see updated docs Added Nginx 1.14, patch update for 1.13 Nginx's underscores_in_headers is now configurable via $NGINX_UNDERSCORES_IN_HEADERS","title":"5.0.11"},{"location":"stacks/wordpress/changelog/#5010","text":"PHP log errors max length set to unlimited Bugfix: PHP errors didn't show up in the container output Bugfix: APACHE_LIMITED_ACCESS support from 5.0.6 release was missing","title":"5.0.10"},{"location":"stacks/wordpress/changelog/#509","text":"Vanilla WordPress updated to 4.9.5 ( security and maintenance release )","title":"5.0.9"},{"location":"stacks/wordpress/changelog/#508","text":"PHP 5.6 version returned","title":"5.0.8"},{"location":"stacks/wordpress/changelog/#507","text":"PHP: Updated to 7.2.4, 7.1.16, 7.0.35 ( security update ) Added jpegoptim Added writable permission to FPM for 'wp-content/wp-rocket-config/'","title":"5.0.7"},{"location":"stacks/wordpress/changelog/#506","text":"Apache: Updated to 2.4.33 ( security update ) New environment variable APACHE_LIMITED_ACCESS to remove Require all granted when you need to limit access by IP","title":"5.0.6"},{"location":"stacks/wordpress/changelog/#505","text":"Nginx updated to 1.13.10 PHP extension grpc updated to 1.10.0 Added environment variables for PHP session runtime configuration Improved error reporting and progress messages for public files directory init Bugfix: apache settings file didn't include when virtual host config overridden with APACHE_INCLUDE_CONF","title":"5.0.5"},{"location":"stacks/wordpress/changelog/#504","text":"Bugfix: vanilla WordPress didn't work with PHP 7.2 Bugfix: wp-admin/ with apache redirected to homepage Bugfix: some caching plugins didn't work because of unsufficient permissions Added APACHE_INCLUDE_CONF to override apache config Database service is now optional in case you want to have a stand-alone DB server Redis service is not included by default","title":"5.0.4"},{"location":"stacks/wordpress/changelog/#503","text":"Cron now runs from www-data user instead of anaxexp files_chmod script now sets permissions with execution allowed only for directories","title":"5.0.3"},{"location":"stacks/wordpress/changelog/#502","text":"Bugfix: translation update fail due to insufficient permissions","title":"5.0.2"},{"location":"stacks/wordpress/changelog/#501","text":"PHP updated to 7.2.3, 7.1.15, 7.0.28 (security updates) Bugfix: insufficient permissions for plugins update Bugfix: missing ~/.ssh directory for www-data user required by some plugins","title":"5.0.1"},{"location":"stacks/wordpress/changelog/#500","text":"","title":"5.0.0"},{"location":"stacks/wordpress/changelog/#changes-since-441","text":"All containers now have resources request as listed here in Resources column , in addition, crond has CPU limit PHP: Container default user has been changed to anaxexp (uid/gid 1000), see https://github.com/anaxexp/php#users-and-permissions for more details PHP updated to 7.2.2, 7.1.14, 7.0.27 (security updates) Rebased to Alpine Linux 3.7 Now when your upgrade stack with a new version of vanilla WordPress, your source code will be updated You can monitor PHP with NewRelic APM allow_url_fopen and default_socket_timeout is now configurable New php extensions added: newrelic, grpc, ds Deprecated environment variables dropped (listed in 4.4.0 changes ) Added postgresql client bins (pg_dump, pg_restore, ...) Added redis-cli Updated php extensions: amqp 1.9.3, redis 3.1.6, mongodb 1.4.0, apcu 5.1.10 Environment variable ANAXEXP_DIR_FILES replaced to FILES_DIR Bugfix: cache clearing from dashboard didn't work for subdirectories MariaDB: Updated to 10.1.31, 10.2.12 Rebased to Alpine Linux 3.7 Nginx: Updated to 1.13.9 Rebased to Alpine Linux 3.7 Redis: Updated to 4.0.8 Bugfix: redis 4 init could not disable THP on some servers OpenSMTPD: Improved health check now runs smtp command Messages queue is now persistent Varnish: The following environment variables changed names (old version no longer supported), DEPRECATED NEW: VARNISHD_THREAD_POOLS VARNISHD_PARAM_THREAD_POOLS VARNISHD_THREAD_POOL_ADD_DELAY VARNISHD_PARAM_THREAD_POOL_ADD_DELAY VARNISHD_THREAD_POOL_MIN VARNISHD_PARAM_THREAD_POOL_MIN VARNISHD_THREAD_POOL_MAX VARNISHD_PARAM_THREAD_POOL_MAX Changed default values: VARNISHD_PARAM_THREAD_POOL_ADD_DELAY from 2 to 0.000 VARNISHD_PARAM_THREAD_POOLS from 1 to 2 VARNISHD_PARAM_THREAD_POOL_MAX from 1000 to 5000 Added additional env vars that control varnishd params ( https://github.com/anaxexp/varnish/issues/1 ) Bugfix: auth issue in Apache ( https://github.com/anaxexp/php-apache/issues/1 )","title":"Changes since 4.4.1"},{"location":"stacks/wordpress/changelog/#upgrade-instructions_1","text":"Make sure you don't use any of deprecated environment variables in PHP (listed in 4.4.0 changes ) and Varnish (listed above) otherwise update their names If you used ANAXEXP_DIR_FILES in your code replace it with FILES_DIR Make sure the default cron container 512M RAM limit is enough for your cron jobs, otherwise increase it manually from service configuration page","title":"Upgrade instructions"},{"location":"stacks/wordpress/changelog/#411","text":"Restored MariaDB 10.1 innodb_large_prefix setting (enabled by default) removed in 4.4.0","title":"4.1.1"},{"location":"stacks/wordpress/changelog/#410","text":"","title":"4.1.0"},{"location":"stacks/wordpress/changelog/#changes-since-400","text":"PHP: New PHP 7.2 PHP updated to 7.1.12, 7.0.26 PHP extensions updated: memcached 3.0.4, ast 0.1.6 Added packages: tig, nano, tmux, less, libjpeg-turbo-utils PHPunit deleted from image to avoid composer conflicts Env vars naming fixes (old names still supported), old new: PHP_APCU_ENABLE PHP_APCU_ENABLED PHP_FPM_SLOWLOG_TIMEOUT PHP_FPM_REQUEST_SLOWLOG_TIMEOUT PHP_FPM_MAX_CHILDREN PHP_FPM_PM_MAX_CHILDREN PHP_FPM_START_SERVERS PHP_FPM_PM_START_SERVERS PHP_FPM_MIN_SPARE_SERVERS PHP_FPM_PM_MIN_SPARE_SERVERS PHP_FPM_MAX_SPARE_SERVERS PHP_FPM_PM_MAX_SPARE_SERVERS PHP_FPM_MAX_REQUESTS PHP_FPM_PM_MAX_REQUESTS PHP_FPM_STATUS_PATH PHP_FPM_PM_STATUS_PATH New -dev image tags (replacing -debug ) for CI/CD (TBA) Env var ANAXEXP_HOST_PRIMARY value now contains host (instead of URL) as it should, ANAXEXP_URL_PRIMARY has been added for the URL value. See environment variables section Improved validation and error reporting for duplicator import Git email and name now can be configured via environment variables Nginx: Nginx updated to 1.13.7, 1.12.2 Fixed broken health check New env var NGINX_NO_DEFAULT_HEADERS to hide default headers We now show request real IP in access logs MariaDB: New MariaDB 10.2.11 MariaDB updated to 10.1.29 Shutdown grace period increased to 5 minutes Deployment strategy no longer can be changed Optimized default config (my.cnf) values New environment variables to configure recovery options Default user/group in a container now mysql Backup action now runs with nice and ionice to prioritize CPU and I/O time for this process Improved error reporting during import Redis: Redis updated to 3.2.11, 4.0.2 Fixed init failure when there's no /sys/kernel/mm/transparent_hugepage/enabled Global environment variables changes: $ANAXEXP_APP_NAME no longer contains instance machine name, only application machine name $ANAXEXP_ENVIRONMENT_ variables have been deprecated and replaced with $ANAXEXP_INSTANCE_ New variables $ANAXEXP_INSTANCE_UUID and $ANAXEXP_APP_UUID Varnish updated to 4.1.9 Apache updated to 2.4.29 Vanilla WordPress updated to 4.9.1 Health checks timeout increased to 30 seconds for all services OpenSMTPD now supports relay auth without password Files backup and mirroring actions now run with nice and ionice to prioritize CPU and I/O time for this process","title":"Changes since 4.0.0"},{"location":"stacks/wordpress/changelog/#update-instructions-from-400","text":"If you used $ANAXEXP_APP_NAME update your code accordingly to the new value (machine name of the app) If you used $ANAXEXP_HOST_PRIMARY (now contains host instead of URL) before you should replace it to $ANAXEXP_URL_PRIMARY Upgrade downtime ~5 minutes","title":"Update instructions from 4.0.0"},{"location":"stacks/wordpress/changelog/#400","text":"","title":"4.0.0"},{"location":"stacks/wordpress/changelog/#changes-since-3x","text":"All-new revamped containers consistent with wordpress4docker Improved performance of containers (especially I/O) Revamped orchestration with better logging and performance Optional services now can be enabled/disabled on the working app Services configuration via environment variables from the dashboard This stack is now suited for container-based cluster Log streaming is now available Detailed log output for orchestration tasks New services: apache, webgrind, blackfire, rsyslog, athenapdf There's no backward compatibility with stacks 3.x","title":"Changes since 3.x"},{"location":"stacks/wordpress/containers/","text":"WordPress stack containers Nginx Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Restarting nginx as default user: sudo nginx -s reload Do not gzip pages in WordPress We already gzip content on Nginx side and it works faster. Having double gzip may cause issues. Custom config If the default config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/php.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf ) Custom config If the default wordpress config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/wordpress.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf Mod pagespeed Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service. Apache Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart PHP PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel Files directory permissions Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions Environment variables Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR ### Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug Xdebug (local) Debugging web requests Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1 Debugging CLI requests Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version: Linux, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make ) macOS, Docker Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist Windows Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost IDE configuration You must additionally configure your IDE to debug CLI requests. PHPStorm Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings NewRelic You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME . Profiling You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions. WP CLI PHP container comes with pre-installed WP CLI. Redirects If you need to make a redirect from one domain to another you can do it by customizing configuration files of nginx or by adding the snippets below to your wp-config.php file. Redirect from one domain to another: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { if ($_SERVER[ HTTP_HOST ] == redirect-from-domain.com ) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from multiple domains: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { $redirect_from = array( redirect-from-domain-1.com , redirect-from-domain-2.com , ); if (in_array($_SERVER[ HTTP_HOST ], $redirect_from)) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Crond A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page. SSHd A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance. Mailhog If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ). OpenSMTPD See OpenSMTPD stack documentation . MariaDB See MariaDB stack documentation . Node.js Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file. Redis You can configure Redis via environment variables that listed at https://github.com/anaxexp/redis . See Redis stack for more details. Integration: Install and activate redis plugin Go to redis plugin settings page and click \"enable object cache\" button Varnish Integration: Go to App instance Stack Varnish in AnaxExp dashboard and copy automatically generated value of $VARNISH_PURGE_KEY Install and activate Varnish Caching plugin in your WordPress website On the plugin cache settings configure as shown below: Copy/paste the key to Purge key in plugin setting Save all plugin settings changes For more details see Varnish stack documentation Rsyslog Rsyslog can be used to stream your applications logs. It's similar to using syslog, however there's no syslog in PHP container (one process per container). Rsyslog will stream all incoming logs to a container output. You can use Monolog with SyslogUdpHandler to stream logs to rsyslog Blackfire You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation Webgrind Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Containers"},{"location":"stacks/wordpress/containers/#wordpress-stack-containers","text":"","title":"WordPress stack containers"},{"location":"stacks/wordpress/containers/#nginx","text":"Nginx can be configured with the following environment variables Default Nginx virtual host config Installed nginx modules Restarting nginx as default user: sudo nginx -s reload Do not gzip pages in WordPress We already gzip content on Nginx side and it works faster. Having double gzip may cause issues.","title":"Nginx"},{"location":"stacks/wordpress/containers/#custom-config","text":"If the default config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/php.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf )","title":"Custom config"},{"location":"stacks/wordpress/containers/#custom-config_1","text":"If the default wordpress config and available environment variables are not enough for your customizations you can replace the config with your own: Copy /etc/nginx/conf.d/wordpress.conf to your codebase, adjust to your needs Deploy code with your config file Add new environment variable NGINX_CONF_INCLUDE for nginx service, the value should the path to your *.conf file (e.g. /var/www/html/nginx.conf","title":"Custom config"},{"location":"stacks/wordpress/containers/#mod-pagespeed","text":"Nginx comes with mod_pagespeed which is disabled by default. To enable it add NGINX_PAGESPEED=on environment variable to Nginx service.","title":"Mod pagespeed"},{"location":"stacks/wordpress/containers/#apache","text":"Apache can be configured with the following environment variables Default Apache virtual host config Installed apache modules Restarting apache as default user: sudo httpd -k restart","title":"Apache"},{"location":"stacks/wordpress/containers/#php","text":"PHP can be configured with the following environment variables Available php extensions Composer pre-installed with a default global package hirak/prestissimo:^0.3 to download dependencies in parallel","title":"PHP"},{"location":"stacks/wordpress/containers/#files-directory-permissions","text":"Public files directory (symlink to /mnt/files/public ) that used for uploads owned by www-data user (PHP-FPM user) by default and the default container user ( anaxexp ) has no writing permissions. So if you run a command that creates files in a public directory you will get insufficient permissions error. You can fix this problem by giving writing permissions for files directory to the owner's group (user anaxexp is a member of www-data group) by using one of the helper scripts : sudo files_chmod /mnt/files/public For mode details about users and permissions in PHP container see https://github.com/anaxexp/php#users-and-permissions","title":"Files directory permissions"},{"location":"stacks/wordpress/containers/#environment-variables","text":"Variables availability Environment variables provided by AnaxExp are always available in PHP even if PHP_FPM_CLEAR_ENV set to no . In addition to global environment variables , we provide the following variables in PHP container that you can use in your post-deployment scripts or settings files: Variable Description $APP_ROOT /var/www/html by default $HTTP_ROOT e.g. /var/www/html/web $CONF_DIR /var/www/conf by default $ANAXEXP_APP_NAME My app $ANAXEXP_HOST_PRIMARY example.com $ANAXEXP_URL_PRIMARY http://example.com $ANAXEXP_HOSTS [ example.com , dev.example.org.wod.by ] Deprecated variables: Variable Instead use $ANAXEXP_APP_ROOT $APP_ROOT $ANAXEXP_APP_DOCROOT $HTTP_ROOT $ANAXEXP_CONF $CONF_DIR $ANAXEXP_DIR_CONF $CONF_DIR ### Xdebug (remote) Follow these steps to debug your application instance remotely with xdebug : Enable xdebug for your instance from [Instance] Stack Settings Set up forwarding for xdebug: copy Xdebug SSH tunnel command from [Instance] Stack PHP and run on your local machine Make sure you have your IDE xdebug listener running on port 9000 Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug","title":"Environment variables"},{"location":"stacks/wordpress/containers/#xdebug-local","text":"","title":"Xdebug (local)"},{"location":"stacks/wordpress/containers/#debugging-web-requests","text":"Uncomment these lines for PHP service in your docker-compose file PHP_XDEBUG: 1 PHP_XDEBUG_DEFAULT_ENABLE: 1 Restart containers ( make ) Start debugging in IDE Start your browser debug helper plugin ( Chrome or Firefox ) and open the page you want to debug. Alternatively, enable auto start by adding PHP_XDEBUG_REMOTE_AUTOSTART=1","title":"Debugging web requests"},{"location":"stacks/wordpress/containers/#debugging-cli-requests","text":"Enable Xdebug as described in the previous section Uncomment the following environment variables for PHP service in your composer file PHP_XDEBUG_REMOTE_CONNECT_BACK: 0 PHP_IDE_CONFIG: serverName=my-ide Configure your IDE Perform configuration as described below depending on your OS and Docker version:","title":"Debugging CLI requests"},{"location":"stacks/wordpress/containers/#linux-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 172.17.0.1 for PHP service (if you have docker 18.03+ you can specify host.docker.internal instead of the IP address) Restart containers ( make )","title":"Linux, Docker"},{"location":"stacks/wordpress/containers/#macos-docker","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.254.254.254 for PHP service (just a random IP that very likely won't be used by anything else). Restart containers ( make ) You also need to have loopback alias with IP from above. You need this only once and that settings stays active until logout or restart: sudo ifconfig lo0 alias 10 .254.254.254 To add the loopback alias after a reboot, add the following contents to /Library/LaunchDaemons/docker4drupal.loopback.plist : plist version= 1.0 dict key Label /key string Default Loopback alias /string key ProgramArguments /key array string /sbin/ifconfig /string string lo0 /string string alias /string string 10.254.254.254 /string string netmask /string string 255.255.255.0 /string /array key RunAtLoad /key true/ /dict /plist","title":"macOS, Docker"},{"location":"stacks/wordpress/containers/#windows","text":"Uncomment PHP_XDEBUG_REMOTE_HOST: 10.0.75.1 for PHP service (default IP of Docker NAT). Restart containers ( make ) Allow listen connection for your IDE in Windows Firewall Allow an app .. Also, you might need to add the following lines to your hosts file (see related github issue ): 0.0.0.0 localhost 10.0.75.1 localhost","title":"Windows"},{"location":"stacks/wordpress/containers/#ide-configuration","text":"You must additionally configure your IDE to debug CLI requests.","title":"IDE configuration"},{"location":"stacks/wordpress/containers/#phpstorm","text":"Open Run Edit Configurations from the main menu, choose Defaults PHP Web Page in the left sidebar Click to [...] to the right of Server and add a new server Enter name my-ide (as specified in PHP_IDE_CONFIG ) Enter any host, it does not matter Check Use path mappings , select path to your project and enter /var/www/html in the right column (Absolute path on the server) Choose newly created server in \"Server\" for PHP Web Page Save settings","title":"PHPStorm"},{"location":"stacks/wordpress/containers/#newrelic","text":"You can add NewRelic APM monitoring for PHP by adding environment variables PHP_NEWRELIC_ENABLED=1 and PHP_NEWRELIC_LICENSE with your license number to PHP-FPM container. Application name will be automatically set to [AnaxExp Application Name] - [AnaxExp Instance Name] , if you want to change it, use PHP_NEWRELIC_APPNAME .","title":"NewRelic"},{"location":"stacks/wordpress/containers/#profiling","text":"You can profile your PHP application either via Xdebug traces (+ Webgrind ) or Tideways XHProf extensions.","title":"Profiling"},{"location":"stacks/wordpress/containers/#wp-cli","text":"PHP container comes with pre-installed WP CLI.","title":"WP CLI"},{"location":"stacks/wordpress/containers/#redirects","text":"If you need to make a redirect from one domain to another you can do it by customizing configuration files of nginx or by adding the snippets below to your wp-config.php file. Redirect from one domain to another: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { if ($_SERVER[ HTTP_HOST ] == redirect-from-domain.com ) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } } Redirect from multiple domains: if (isset($_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ]) $_SERVER[ ANAXEXP_ENVIRONMENT_TYPE ] == prod php_sapi_name() != cli ) { $redirect_from = array( redirect-from-domain-1.com , redirect-from-domain-2.com , ); if (in_array($_SERVER[ HTTP_HOST ], $redirect_from)) { header( HTTP/1.0 301 Moved Permanently ); header( Location: http://redirect-to-domain.com . $_SERVER[ REQUEST_URI ]); exit(); } }","title":"Redirects"},{"location":"stacks/wordpress/containers/#crond","text":"A duplicate of the main PHP container runs with crond (instead of FPM). You can customize crontab from [Instance] Stack Settings page.","title":"Crond"},{"location":"stacks/wordpress/containers/#sshd","text":"A duplicate of PHP container runs with SSH daemon (instead of FPM). You can find access information on [Instance] Stack SSH Public SSH keys from your AnaxExp profile will be added automatically for all users that have access to an instance.","title":"SSHd"},{"location":"stacks/wordpress/containers/#mailhog","text":"If Mailhog service enabled and chosen as Mail delivery service at [Instance] Stack Settings all outbound email will be caught by the Mailhog. You can view and release these emails from Mailhog UI, the URL can be found from Domains tab. When release specify opensmtpd in SMTP server field if you want to release emails to the default Mail transfer agent ( OpenSMTPD ).","title":"Mailhog"},{"location":"stacks/wordpress/containers/#opensmtpd","text":"See OpenSMTPD stack documentation .","title":"OpenSMTPD"},{"location":"stacks/wordpress/containers/#mariadb","text":"See MariaDB stack documentation .","title":"MariaDB"},{"location":"stacks/wordpress/containers/#nodejs","text":"Light-weight node.js container to help you build your application's frontend. The containers comes without any global pre-installed packages, you can add them by running yarn global add PACKAGE or by running yarn in a directory with your package.json file.","title":"Node.js"},{"location":"stacks/wordpress/containers/#redis","text":"You can configure Redis via environment variables that listed at https://github.com/anaxexp/redis . See Redis stack for more details. Integration: Install and activate redis plugin Go to redis plugin settings page and click \"enable object cache\" button","title":"Redis"},{"location":"stacks/wordpress/containers/#varnish","text":"Integration: Go to App instance Stack Varnish in AnaxExp dashboard and copy automatically generated value of $VARNISH_PURGE_KEY Install and activate Varnish Caching plugin in your WordPress website On the plugin cache settings configure as shown below: Copy/paste the key to Purge key in plugin setting Save all plugin settings changes For more details see Varnish stack documentation","title":"Varnish"},{"location":"stacks/wordpress/containers/#rsyslog","text":"Rsyslog can be used to stream your applications logs. It's similar to using syslog, however there's no syslog in PHP container (one process per container). Rsyslog will stream all incoming logs to a container output. You can use Monolog with SyslogUdpHandler to stream logs to rsyslog","title":"Rsyslog"},{"location":"stacks/wordpress/containers/#blackfire","text":"You can profile your application via blackfire.io by following the next steps: Enable blackfire probe extension by adding the environment variable PHP_BLACKFIRE=1 to PHP container Enable blackfire agent service in your stack Add environment variables BLACKFIRE_SERVER_ID and BLACKFIRE_SERVER_TOKEN to blackfire agent service with appropriate values from your blackfire.io profile Install blackfire companion extension for Chrome or Firefox Start profiling your app via the extension and see data from blackfire.io dashboard Fore more details please refer to the blackfire official documentation","title":"Blackfire"},{"location":"stacks/wordpress/containers/#webgrind","text":"Webgrind allows you view and analyze Xdebug profiler output and generate call graphs for visualisation. To use Webgrind first enable Xdebug profiler by adding the following environment variables to your PHP container: PHP_XDEBUG: 1 PHP_XDEBUG_PROFILER_ENABLE: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER: 1 PHP_XDEBUG_PROFILER_ENABLE_TRIGGER_VALUE: 1 Add XDEBUG_PROFILE=1 param to GET or POST request (or set a cookie) you want to profile. Xdebug will generate profile files in /mnt/files/xdebug/profiler . Click Update in Webgrind to access the new information. See https://xdebug.org/docs/profiler to learn more about xdebug profiling. IMPORTANT Xdebug profiling significantly decreases performance and increases resources usage. DO NOT USE it on Production servers.","title":"Webgrind"},{"location":"stacks/wordpress/local/","text":"Local environment with WordPress4Docker WordPress4Docker is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows. Requirements Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose Usage Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. There are 2 options how to use wordpress4docker \u2013 you can either run vanilla WordPress from the image or mount your own WordPress codebase: Vanilla WordPress Clone wordpress4docker repository and switch to the latest stable tag or download/unpack the source code from the latest release Configure domains From project root directory run docker-compose up -d or make up to start containers. Give it 10-20 seconds to initialize after the start That's it! Proceed with WordPress installation at http://wp.docker.localhost:8000 . Default database user, password and database name are all wordpress , database host is mariadb You can see status of your containers and their logs via portainer: http://portainer.wp.docker.localhost:8000 Mount my codebase Download wordpress4docker.tar.gz from the latest stable release and unpack to your WordPress project root. If you choose to clone the repository delete docker-compose.override.yml as it's used to deploy vanilla WordPress Ensure database credentials match in your wp-config.php and .env files Configure domains Optional: import existing database Optional: uncomment lines in the compose file to run redis, varnish, phpmyadmin, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your WordPress website should be up and running at http://wp.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.wp.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the wordpress4docker.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page Domains Traefik container used for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to wp.docker.localhost , you can change it in .env file. Add 127.0.0.1 wp.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://wp.docker.localhost:8000 pma http://pma.wp.docker.localhost:8000 adminer http://adminer.wp.docker.localhost:8000 mailhog http://mailhog.wp.docker.localhost:8000 varnish http://varnish.wp.docker.localhost:8000 portainer http://portainer.wp.docker.localhost:8000 webgrind http://webgrind.wp.docker.localhost:8000 Database import and export MariaDB See MariaDB stack documentation PostgreSQL See PostgreSQL stack documentation Make commands We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service Docker for mac There two major problems macOS users face with when using Docker for mac: macOS permissions issues To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user. Bind mounts performance Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved. User-guided caching Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\". Docker-sync Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page . Permissions issues You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container. Linux Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions. macOS Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user. Windows Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root Different uid/gid? You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default) Running multiple Projects Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Local environment"},{"location":"stacks/wordpress/local/#local-environment-with-wordpress4docker","text":"WordPress4Docker is an open-source project ( GitHub page ) that provides pre-configured docker-compose.yml file with images to spin up local environment on Linux, Mac OS X and Windows.","title":"Local environment with WordPress4Docker"},{"location":"stacks/wordpress/local/#requirements","text":"Install Docker ( Linux , Docker for Mac or Docker for Windows (10+ Pro) ) For Linux additionally install docker compose","title":"Requirements"},{"location":"stacks/wordpress/local/#usage","text":"Database data persistence By default Docker will create a persistent volume for your DB data and unless you explicitly remove volumes the files will not be deleted. However, if you run docker-compose down (it's ok to use stop though) these volumes will not be reattached when you run docker-compose up . If you want to have your DB data all-time persistent and attached, we recommend using a bind mount . To use a bind mount uncomment to corresponding line under db server's volumes: in your docker-compose.yml and update the host path to your data directory. There are 2 options how to use wordpress4docker \u2013 you can either run vanilla WordPress from the image or mount your own WordPress codebase:","title":"Usage"},{"location":"stacks/wordpress/local/#vanilla-wordpress","text":"Clone wordpress4docker repository and switch to the latest stable tag or download/unpack the source code from the latest release Configure domains From project root directory run docker-compose up -d or make up to start containers. Give it 10-20 seconds to initialize after the start That's it! Proceed with WordPress installation at http://wp.docker.localhost:8000 . Default database user, password and database name are all wordpress , database host is mariadb You can see status of your containers and their logs via portainer: http://portainer.wp.docker.localhost:8000","title":"Vanilla WordPress"},{"location":"stacks/wordpress/local/#mount-my-codebase","text":"Download wordpress4docker.tar.gz from the latest stable release and unpack to your WordPress project root. If you choose to clone the repository delete docker-compose.override.yml as it's used to deploy vanilla WordPress Ensure database credentials match in your wp-config.php and .env files Configure domains Optional: import existing database Optional: uncomment lines in the compose file to run redis, varnish, phpmyadmin, etc Optional: macOS users please read this Optional: Windows users please read this Run containers: make up or docker-compose up -d Your WordPress website should be up and running at http://wp.docker.localhost:8000 You can see status of your containers and their logs via portainer: http://portainer.wp.docker.localhost:8000 You can stop containers by executing make stop or docker-compose stop . Optional files If you don't need to run multiple projects and don't use docker-sync to improve volumes performance on macOS feel free to delete traefik.yml and docker-sync.yml that come with the wordpress4docker.tar.gz Get updates We release updates to images from time to time, you can find detailed changelog and update instructions on GitHub under releases page","title":"Mount my codebase"},{"location":"stacks/wordpress/local/#domains","text":"Traefik container used for routing. By default, we use port 8000 to avoid potential conflicts but if port 80 is free on your host machine just replace traefik's ports definition in the compose file. By default BASE_URL set to wp.docker.localhost , you can change it in .env file. Add 127.0.0.1 wp.docker.localhost to your /etc/hosts file (some browsers like Chrome may work without it). Do the same for other default domains you might need from listed below: Service Domain nginx/apache http://wp.docker.localhost:8000 pma http://pma.wp.docker.localhost:8000 adminer http://adminer.wp.docker.localhost:8000 mailhog http://mailhog.wp.docker.localhost:8000 varnish http://varnish.wp.docker.localhost:8000 portainer http://portainer.wp.docker.localhost:8000 webgrind http://webgrind.wp.docker.localhost:8000","title":"Domains"},{"location":"stacks/wordpress/local/#database-import-and-export","text":"","title":"Database import and export"},{"location":"stacks/wordpress/local/#mariadb","text":"See MariaDB stack documentation","title":"MariaDB"},{"location":"stacks/wordpress/local/#postgresql","text":"See PostgreSQL stack documentation","title":"PostgreSQL"},{"location":"stacks/wordpress/local/#make-commands","text":"We provide Makefile that contains commands to simplify the work with your local environment. You can run make [COMMAND] to execute the following commands: Usage: make COMMAND Commands: up Start up all container from the current docker-compose.yml stop Stop all containers for the current docker-compose.yml (docker-compose stop) down Same as stop prune Stop and remove containers, networks, images, and volumes (docker-compose down) ps List container for the current project (docker ps with filter by name) shell Access PHP container via shell as a default user (docker exec -ti $CID sh) logs [service] Show containers logs, use [service] to show logs of specific service","title":"Make commands"},{"location":"stacks/wordpress/local/#docker-for-mac","text":"There two major problems macOS users face with when using Docker for mac:","title":"Docker for mac"},{"location":"stacks/wordpress/local/#macos-permissions-issues","text":"To avoid any permissions issues caused by different user id (uid), group id (gid) between your host and a container use -dev-macos version of php image (uncomment the environment variables in .env files) where the default user anaxexp has 501:20 uid/gid that matches default macOS user.","title":"macOS permissions issues"},{"location":"stacks/wordpress/local/#bind-mounts-performance","text":"Out of the box Docker for mac bind mounts (volumes from host) have poor performance on sync. There are 2 ways how it can be improved.","title":"Bind mounts performance"},{"location":"stacks/wordpress/local/#user-guided-caching","text":"Since Docker for Mac 17.06 there's a new native :cached option available for bind mounts. You can find more information about this in docker blog . Replace codebase volume definition of php and nginx / apache services with the option below marked as \"User-guided caching\".","title":"User-guided caching"},{"location":"stacks/wordpress/local/#docker-sync","text":"Performance tests 2017 Docker-sync vs Native . The core idea of this project is to use an external volume that will sync your files with a file synchronizer tool. $ gem install docker-sync Download docker-sync.yml file (inside of docker4x.tar.gz archive) from the latest stable release Uncomment docker-sync volume definition in your compose file Replace volumes definition of php and nginx / apache services with the option below marked as \"Docker-sync\". Start docker-sync: docker-sync start In a new shell run after you started docker-sync docker-compose up -d Now when you change your code on the host machine docker-sync will sync your data to php and nginx/apache containers. For more information visit docker-sync project page .","title":"Docker-sync"},{"location":"stacks/wordpress/local/#permissions-issues","text":"You might have permissions issues caused by non-matching uid/gid on your host machine and the default user in php container.","title":"Permissions issues"},{"location":"stacks/wordpress/local/#linux","text":"Since version 5.0 the default php container user anaxexp has uid/gid 1000 that matches the default uid/gid for most popular Linux distributions.","title":"Linux"},{"location":"stacks/wordpress/local/#macos","text":"Use -dev-macos version of php image where default anaxexp user has 501:20 uid/gid that matches default macOS user.","title":"macOS"},{"location":"stacks/wordpress/local/#windows","text":"Since you can't change owner of mounted volumes in Docker for Win, the only solution is to run everything as root, add the following options to php service in your docker-compose file: php: user: root command: php-fpm -R environment: PHP_FPM_USER: root PHP_FPM_GROUP: root","title":"Windows"},{"location":"stacks/wordpress/local/#different-uidgid","text":"You can rebuild the base image anaxexp/php with custom user/group ids by using docker build arguments ANAXEXP_USER_ID , ANAXEXP_USER_ID (both 1000 by default)","title":"Different uid/gid?"},{"location":"stacks/wordpress/local/#running-multiple-projects","text":"Tr\u00e6fik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. To understand the basics of Traefik it is suggested to check Tr\u00e6fik's documentation page: https://docs.traefik.io/ Image: Multi-domain set-up example (Source: traefik.io) Steps to set up two projects on one host: Create two dirs where you will host two projects. Let's name them site1 and site2 Copy docker-compose.yml file to both dirs ( site1 and site2 ) Download traefik.yml file (inside of tar.gz archive) from the latest stable release to the parent dir where site1 and site2 dirs are Edit traefik.yml and change project1-dir_default to site1_default and project2-dir_default to site2_default . Those are docker networks names that are created automatically from the dir name where docker-compose.yml is located Edit site1's docker-compose.yml file. There are 3 main things that need to be done there: In nginx service, under labels, change traefik.backend=nginx to traefik.backend=site1_nginx_1 . This is the name of the container. You can see that under NAMES when your have the containers running by executing docker ps Change traefik.frontend.rule from Host:php.docker.localhost to Host:site1.docker.localhost Comment out all lines of traefik service at the bottom of the file Make similar 3 changes in site2's docker-compose.yml file: traefik.backend=nginx to traefik.backend=site2_nginx_1 Host:php.docker.localhost to Host:site2.docker.localhost Comment out all lines of traefik service at the bottom of the file Run docker-compose up -d in site1 and site2 dirs to spin up containers for both projects Run stand-alone traefik docker-compose -f traefik.yml up -d to spin up traefik reverse proxy Visit http://site1.docker.localhost and http://site2.docker.localhost in your browser This set up also works for any Docker projects. You can replace nginx-proxy config with Traefik and get other projects all routed with on traefik container. For macOS users with docker-sync Make sure names of syncs in docker-sync.yml are unique per project. The recommended way is to run a stand-alone docker-sync with syncs definition for all projects. Do not forget to update src paths for projects In case of issues: Check docker ps to see which containers are running and check if you have set up all names correctly. Check docker network ls to check if the network names are matching. Run docker-compose logs -f in site1 or site2 to see the log of each project.","title":"Running multiple Projects"}]}